# Chapter 11: Server Dev

Chapter contents:

* [Introduction](11.md#introduction)
  * [Why build a GraphQL server?](11.md#why-build-a-graphql-server)
  * [What kind of GraphQL server should I build?](11.md#what-kind-of-graphql-server-should-i-build)
* [Building](11.md#building)
  * [Project setup](11.md#project-setup)
  * [Types and resolvers](11.md#types-and-resolvers)
  * [Authenticating](11.md#authenticating)
  * [Data sources](11.md#data-sources)
    * [Setting up](11.md#setting-up)
    * [File structure](11.md#file-structure)
    * [Creating reviews](11.md#creating-reviews)
  * [Custom scalars](11.md#custom-scalars)
  * [Creating users](11.md#creating-users)
    * [Protecting with secret key](11.md#protecting-with-secret-key)
    * [Setting user context](11.md#setting-user-context)
    * [Linking users to reviews](11.md#linking-users-to-reviews)
  * [Authorizing](11.md#authorizing)
  * [Errors](11.md#errors)
    * [Nullability](11.md#nullability)
    * [Union errors](11.md#union-errors)
    * [formatError](11.md#formaterror)
      * [Logging errors](11.md#logging-errors)
      * [Masking errors](11.md#masking-errors)
    * [Error checking](11.md#error-checking)
    * [Custom errors](11.md#custom-errors)
  * [Subscriptions](11.md#subscriptions)
    * [githubStars](11.md#githubstars)
    * [reviewCreated](11.md#reviewcreated)
* [Testing](11.md#testing)
  * [Static testing](11.md#static-tests)
  * [Review integration tests](11.md#review-integration-tests)
  * [Code coverage](11.md#code-coverage)
  * [User integration tests](11.md#user-integration-tests)
  * [Unit tests](11.md#unit-tests)
  * [End-to-end tests](11.md#end-to-end-tests)
* [Production](11.md#production)
  * [Deployment](11.md#deployment)
    * [Options](11.md#options)
    * [Deploying](11.md#deploying)
    * [Environment variables](11.md#environment-variables)
  * [Database hosting](11.md#database-hosting)
    * [MongoDB hosting](11.md#mongodb-hosting)
    * [Redis hosting](11.md#redis-hosting)
      * [Redis PubSub](11.md#redis-pubsub)
      * [Redis caching](11.md#redis-caching)
  * [Querying in production](11.md#querying-in-production)
  * [Analytics](11.md#analytics)
  * [Error reporting](11.md#error-reporting)
* [More data sources](11.md#more-data-sources)
  * [SQL](11.md#sql)
    * [SQL setup](11.md#sql-setup)
    * [SQL data source](11.md#sql-data-source)
    * [SQL testing](11.md#sql-testing)
  * [REST](11.md#rest)
  * [GraphQL](11.md#graphql)
  * [Custom data source](11.md#custom-data-source)
  * [Prisma](11.md#prisma)
* [Extended topics](11.md#extended-topics)
  * [Mocking](11.md#mocking)
  * [Pagination](11.md#pagination)
    * [Offset-based](11.md#offset-based)
    * [Cursors](11.md#cursors)
      * [after an ID](11.md#after-an-id)
      * [Relay cursor connections](11.md#relay-cursor-connections)
  * [File uploads](11.md#file-uploads)
    * [Client-side](11.md#client-side)
    * [Server-side](11.md#server-side)
  * [Schema validation](11.md#schema-validation)
  * [Apollo federation](11.md#apollo-federation)
    * [Federated service](11.md#federated-service)
    * [Federated gateway](11.md#federated-gateway)
    * [Extending entities](11.md#extending-entities)
    * [Managed federation](11.md#managed-federation)
    * [Deploying federation](11.md#deploying-federation)
  * [Hasura](11.md#hasura)
  * [Schema design](11.md#schema-design)
    * [One schema](11.md#one-schema)
    * [User-centric](11.md#user-centric)
    * [Easy to understand](11.md#easy-to-understand)
    * [Easy to use](11.md#easy-to-use)
    * [Mutations](11.md#mutations)
      * [Arguments](11.md#arguments)
      * [Payloads](11.md#payloads)
    * [Versioning](11.md#versioning)
  * [Custom schema directives](11.md#custom-schema-directives)
    * [@tshirt](#@tshirt)
    * [@upper](#@upper)
    * [@auth](#@auth)
  * [Subscriptions in depth](11.md#subscriptions-in-depth)
    * [Server architecture](11.md#server-architecture)
    * [Subscription design](11.md#subscription-design)
  * [Security](11.md#security)
    * [Auth options](11.md#auth-options)
      * [Authentication](11.md#authentication)
      * [Authorization](11.md#authorization)
    * [Denial of service](11.md#denial-of-service)
  * [Performance](11.md#performance)
    * [Data fetching](11.md#data-fetching)
    * [Caching](11.md#caching)
  * [Future](11.md#future)

---

# Introduction

Background: [HTTP](bg.md#http), [Server](bg.md#server)

Welcome to the server chapter! This is the lastâ€”and longestâ€”chapter. Weâ€™ll learn most of the concepts through building the Guide API server, which backs the apps we built in the client chapters. The server will primarily store data in MongoDB, but weâ€™ll also connect to several other data sources, including SQL and REST. Weâ€™ll write it in JavaScript, but all server-side GraphQL libraries use [the same execution method](4.md), and most of the concepts in this chapter will apply to writing GraphQL servers in other languages. To see the differences, check out these backend tutorials:

- [Java](https://www.howtographql.com/graphql-java/0-introduction/)
- [Python](https://www.howtographql.com/graphql-python/0-introduction/)
- [Ruby](https://www.howtographql.com/graphql-ruby/0-introduction/)
- [Scala](https://www.howtographql.com/graphql-scala/0-introduction)
- [Elixir](https://www.howtographql.com/graphql-elixir/0-introduction/)

There are also GraphQL libraries in these languages:

- [.NET](https://github.com/graphql-dotnet/graphql-dotnet)
- [Clojure](https://github.com/walmartlabs/lacinia)
- [Go](https://github.com/graphql-go/graphql)
- [PHP](https://github.com/webonyx/graphql-php)

This chapter is split into five parts:

* **Introduction**
* **[Building](11.md#building)**
* **[Production](11.md#production)**
* **[More data sources](11.md#more-data-sources)**
* **[Extended topics](11.md#extended-topics)**

In **Building**, we build a GraphQL server from scratch, including authentication and authorization, query and mutation resolvers that talk to a database, error handling, subscriptions, and testing. In **Production**, we deploy our server and update it with things that are helpful to have in production, like error reporting, analytics, and security against attack. In **More data sources**, we connect our server to other databases and a REST API. In **Extended topics**, we learn about various new server-side topics and go into more depth on previous topics like the schema, subscriptions, and auth.

## Why build a GraphQL server?

There are three main reasons why we might decide our server should be a GraphQL server:

1. So we can use GraphQL on the client and gain all the client-side benefits of GraphQL.
2. To simplify our server code: instead of setting up many endpoints and implementing fetching and formatting logic for each, we set up one endpoint and write a single resolver for each data type.
3. To avoid having to create new endpoints or new APIs in the future.

For coders, #1 and #2 are often the most compelling, because it improves our quality of life ðŸ˜„. For companies, #3 is often the most compelling, since they save time and money: they get a single, flexible API that covers all their business data, which means that instead of having to create new endpoints or entire APIs for new features or apps, they can just use their existing GraphQL API (and in some cases add fields and resolvers).

## What kind of GraphQL server should I build?

Actually, the first choice we have is whether to build it or generate it ðŸ˜„. There are services that can save us a lot of time by generating a production-ready GraphQL backend for us. We'll go over the pros/cons and how to set one up in the [Hasura section](#hasura).

If we do decide to build our own server, there are two situations we might be in:

1. **Existing project**, in which case weâ€™ll either be adding a GraphQL layer in front of our existing servers, or adding a GraphQL endpoint to existing servers.
2. **New project** (a.k.a. *greenfield*), in which case we have a choice of which architecture to use.

There are two main architectures:

1. **Microservices** (a collection of servers that each cover a different business capability). GraphQL as the API gateway: the client talks to the GraphQL server API gateway, which talks to services (via GraphQL, REST, gRPC, Thrift, etc), which talk to databases.
2. **Monolith** (a single server that covers all business logic). GraphQL as the application layer: the client talks to the GraphQL server, which talks directly to databases.

Microservices are in vogue and the word â€œmonolithâ€ is often used with a scornful tone, but in most cases, itâ€™s better to have a monolith. Martin Fowler, one of the leaders in software design, [wrote](https://martinfowler.com/bliki/MicroservicePremium.html):

> So my primary guideline would be donâ€™t even consider microservices unless you have a system thatâ€™s too complex to manage as a monolith. The majority of software systems should be built as a single monolithic application. Do pay attention to good modularity within that monolith, but donâ€™t try to separate it into separate services.

While there are a lot of huge tech companies that use microservices and are better off for it, theyâ€™re better off because theyâ€™re hugeâ€”not because microservices are a general good practice.

If we have an existing monolith, it often makes sense to add a GraphQL endpoint to that server instead of putting a GraphQL server in front of the monolith. For example, if we have an Express monolith that has a lot of thin REST routes that call model functions that contain the business logic and data fetching, then it would be easy to add a `/graphql` route with [`apollo-server-express`](https://www.apollographql.com/docs/apollo-server/essentials/server#middleware) and implement resolvers that call the same model functions as the REST routes. Or if all of our logic was in the routes themselves, and we didn't need to continue supporting the REST API, we could move the code we needed over to resolvers and [Apollo data sources](#data-sources).

When weâ€™re adding a GraphQL layer in front of an existing backend, whether itâ€™s a microservices or monolith backend, we can make the choice between continuing to develop the existing backend or gradually moving logic to the GraphQL layer. If weâ€™re doing microservices and want to keep that architecture, then itâ€™s easy to keep implementing services (in whatever language(s) we implement services) and either extend the GraphQL schema and resolvers or use [schema federation](#apollo-federation).

Another question is what language to write our GraphQL server in. In the case of adding to an existing monolith, weâ€™ll use the GraphQL server library for the same language. In all other cases (new projects or a GraphQL layer in front of existing microservices or monoliths), we generally recommend JavaScript. Itâ€™s by far the most popular type of GraphQL server, and has thus developed the best ecosystem of libraries and services.

The server weâ€™ll be creating in this chapter is a greenfield monolith, so it will talk directly to the database. However, most of the concepts will carry over to the microservice model. The largest difference will be either:

- using schema federation to combine multiple GraphQL services
- fetching data and resolving mutations by talking to the services (e.g. with REST) instead of the database

Weâ€™ll go over both of these options later in the chapter.

# Building

Background: [Node & npm & nvm](bg.md#node-&-npm-&-nvm), [git](bg.md#git), [JavaScript](bg#javascript)

* [Project setup](11.md#project-setup)
* [Types and resolvers](11.md#types-and-resolvers)
* [Authenticating](11.md#authenticating)
* [Data sources](11.md#data-sources)
  * [Setting up](11.md#setting-up)
  * [File structure](11.md#file-structure)
  * [Creating reviews](11.md#creating-reviews)
* [Custom scalars](11.md#custom-scalars)
* [Creating users](11.md#creating-users)
  * [Protecting with secret key](11.md#protecting-with-secret-key)
  * [Setting user context](11.md#setting-user-context)
  * [Linking users to reviews](11.md#linking-users-to-reviews)
* [Authorizing](11.md#authorizing)
* [Errors](11.md#errors)
  * [Nullability](11.md#nullability)
  * [Union errors](11.md#union-errors)
  * [formatError](11.md#formaterror)
    * [Logging errors](11.md#logging-errors)
    * [Masking errors](11.md#masking-errors)
  * [Error checking](11.md#error-checking)
  * [Custom errors](11.md#custom-errors)
* [Subscriptions](11.md#subscriptions)
  * [githubStars](11.md#githubstars)
  * [reviewCreated](11.md#reviewcreated)

Weâ€™re using Node because itâ€™s the most popular platform for GraphQL Servers and has the best ecosystem. JavaScript is also the most used programming language in the world! â’¿â“ˆâ¬†ï¸

## Project setup

There are a few different things to set up when starting a new Node project. Weâ€™ve set them up in branch `0` of our server repo, [github.com/GraphQLGuide/guide-api](https://github.com/GraphQLGuide/guide-api): 

```sh
$ git clone https://github.com/GraphQLGuide/guide-api.git
$ cd guide-api/
$ git checkout 0_0.2.0
$ npm install
```

We now have these files:

```
.babelrc
.git/
.gitignore
.nvmrc
.prettierrc
node_modules/
package-lock.json
package.json
```

Letâ€™s look at each to see what theyâ€™re for. 

- [`.babelrc`](https://github.com/GraphQLGuide/guide-api/blob/0_0.2.0/.babelrc):

```json
{
  "presets": [
    [
      "@babel/preset-env",
      {
        "targets": {
          "node": "12.0.0"
        }
      }
    ]
  ],
  "plugins": ["import-graphql"]
}
```

`@babel/preset-env` transpiles JavaScript to work in the target environmentâ€”where weâ€™ll be running the code. In chapter 6, that was the browser. For this chapter, the target environment is Node. Weâ€™ll target version `12.0.0` so that the transpiled code will work in that or higher versions.

- `.git/`: directory where git stores its data.
- `.gitignore`:

```
node_modules/
dist/
```

A list of which files and folders we donâ€™t want committed to git. We donâ€™t want `node_modules/` as theyâ€™re added when we `npm install`. And `dist/` will be generated by the build script in our `package.json`.

- `.nvmrc`: 

```
12
```

The file that tells nvm which version of Node to use. `12` means the latest stable `12.*` version.

- `.prettierrc`:

```
singleQuote: true
semi: false
trailingComma: none
arrowParens: avoid
```

Because single quotes and no semicolons is the One True Way to style JavaScript. 

> Just kiddingâ€”there isnâ€™t one right way to style code. This is just author Lorenâ€™s preference ðŸ˜„.

And the last two settings were the Prettier default when this chapter was written.

- `node_modules/`: directory to which npm downloads all of the packages our code depends on.
- `package-lock.json`: precise current versions of all the packages.
- [`package.json`](https://github.com/GraphQLGuide/guide-api/blob/0_0.2.0/package.json):

```json
{
  "name": "guide-api",
  "version": "0.1.0",
  "description": "api.graphql.guide",
  "scripts": {
    "dev": "nodemon -e js,graphql --exec 'npm run update-graphql-imports && babel-node src/index.js'",
    "start": "node dist/index.js",
    "build": "babel src -d dist --ignore **/*.test.js",
    "update-graphql-imports": "rm -rf ./node_modules/.cache/@babel"
  },
  "engines": {
    "node": ">=12"
  },
  "dependencies": {
    "@sentry/node": "5.15.5",
    "apollo-datasource-mongodb": "0.2.6",
    "apollo-datasource-rest": "0.8.1",
    "apollo-server": "2.12.0",
    "apollo-server-cache-redis": "1.1.6",
    "apollo-server-testing": "2.12.0",
    "aws-sdk": "2.666.0",
    "casual": "1.6.2",
    "datasource-sql": "1.3.0",
    "date-fns": "2.12.0",
    "dotenv": "8.2.0",
    "graphql": "14.6.0",
    "graphql-redis-subscriptions": "2.2.1",
    "graphql-request": "1.8.2",
    "graphql-tools": "4.0.8",
    "ioredis": "4.16.3",
    "join-monster": "2.1.1",
    "join-monster-graphql-tools-adapter": "0.1.0",
    "jsonwebtoken": "8.5.1",
    "jwks-rsa": "1.8.0",
    "knex": "0.21.1",
    "lodash": "4.17.15",
    "mongodb": "3.5.7",
    "sqlite3": "4.2.0"
  },
  "devDependencies": {
    "@babel/cli": "7.8.4",
    "@babel/core": "7.9.6",
    "@babel/node": "7.8.7",
    "@babel/preset-env": "7.9.6",
    "apollo-link": "1.2.14",
    "apollo-link-http": "1.5.17",
    "babel-plugin-import-graphql": "2.7.0",
    "eslint": "6.8.0",
    "eslint-plugin-node": "11.1.0",
    "husky": "4.2.5",
    "jest": "25.5.2",
    "node-fetch": "2.6.0",
    "nodemon": "2.0.3"
  },
  "homepage": "https://github.com/GraphQLGuide/guide-api",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/GraphQLGuide/guide-api"
  },
  "bugs": {
    "url": "https://github.com/GraphQLGuide/guide-api/issues"
  },
  "private": true,
  "author": "The GraphQL Guide <hi@graphql.guide> (https://graphql.guide)"
}
```

Letâ€™s look at the scripts first:

```json
  "scripts": {
    "dev": "nodemon -e js,graphql --exec 'npm run update-graphql-imports && babel-node src/index.js'",
    "start": "node dist/index.js",
    "build": "babel src -d dist --ignore **/*.test.js",
    "update-graphql-imports": "rm -rf ./node_modules/.cache/@babel"
  },
```

- `npm run dev` will watch our JS and GraphQL files, and whenever one of them changes, it will transpile them with Babel and run them with Node.
- `npm start` will start our server in production using the transpiled version of our code located in `dist/`.
- `npm run build` will transpile our code from `src/` to `dist/` (ignoring test files).
- `npm update-graphql-imports` is used by `npm run dev` to clear the babel GraphQL plugin cache.

The [`engines`](https://docs.npmjs.com/files/package.json#engines) attribute, similar to `preset-env`â€™s `target`, describes where the code is meant to be run. For us, itâ€™s meant to be run in any version 8 or higher of Node.

```
  "engines": {
    "node": ">=12"
  },
```

All together, what weâ€™ve got configured is:

- Git
- npm
- nvm
- Babel
- Prettier

## Apollo Server

> If youâ€™re jumping in here, `git checkout 0_0.2.0` (tag [0_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/0_0.2.0), or compare [0...1](https://github.com/GraphQLGuide/guide-api/compare/0_0.2.0...1_0.2.0))

It takes less than 20 lines of JavaScript to get a working GraphQL server up and running! ðŸ˜ƒ The best GraphQL server library is [`apollo-server`](https://www.apollographql.com/docs/apollo-server/), and hereâ€™s the basic setup:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/blob/1_0.2.0/src/index.js)

```js
import { ApolloServer, gql } from 'apollo-server'

const server = new ApolloServer({
  typeDefs: gql`
    type Query {
      hello: String!
    }
  `,
  resolvers: {
    Query: {
      hello: () => 'ðŸŒðŸŒðŸŒŽ'
    }
  }
})

server
  .listen({ port: 4000 })
  .then(({ url }) => console.log(`GraphQL server running at ${url}`))
```

The main export of `apollo-server` is [`ApolloServer`](https://www.apollographql.com/docs/apollo-server/api/apollo-server#apolloserver), and its two required parameters are:
- `typeDefs`: our schema, created with the [`gql`](https://www.apollographql.com/docs/apollo-server/api/apollo-server#gql) template literal tag.
- `resolvers`: an object of [resolver](4.md#resolvers) functions that match our schema in structure. Each typeâ€”`Query`, `Mutation`, `User`, `Chapter`, etc.â€”is a top-level attribute, and the next level is that typeâ€™s field names.

We start the server by calling [`.listen()`](https://www.apollographql.com/docs/apollo-server/api/apollo-server#apolloserverlistenoptions-promise) on a port. When itâ€™s done starting up, the Promise is resolved with the URLâ€”in our case, `http://localhost:4000`.

We can test it out with our `dev` script:

```sh
$ npm run dev

> guide-api@0.1.0 dev /guide-api
> babel-watch src/index.js

GraphQL server running at http://localhost:4000/
```

After a moment, the program gets to the last line of our code, which logs `Server running at http://localhost:4000/`. When we edit `src/index.js`â€”for instance by changing `console.log`â€”the server gets restarted:

```sh
$ npm run dev

> guide-api@0.1.0 dev /guide-api
> babel-watch src/index.js

GraphQL server running at http://localhost:4000/
>>> RESTARTING <<<
GraphQL server running with new console.log statement at http://localhost:4000/
```

To stop the server, we can press the `control-c` key combination.

So weâ€™ve got the server running, but does it work? Letâ€™s open up the URL in a browser:

[http://localhost:4000](http://localhost:4000)

By default Apollo Server loads GraphQL Playground, an IDE for writing GraphQL queries. We can see our tiny schema by clicking on â€œDOCSâ€ on the right:

![Schema tab](img/hello-schema.png)

And we can test out our one query:

![hello query](img/hello-world.png)

And it works!

## Types and resolvers

> If youâ€™re jumping in here, `git checkout 1_0.2.0` (tag [1_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/1_0.2.0), or compare [1...2](https://github.com/GraphQLGuide/guide-api/compare/1_0.2.0...2_0.2.0))

The heart of a GraphQL server is the types and resolvers. The schema has the types and each typeâ€™s fields, and the resolvers *resolve* each field. We generally resolve fields by fetching data from a data source, formatting fetched data, or enacting mutations. 

Letâ€™s add some more types and fields to get a better sense of how they match up with resolvers. We want people to be able to submit reviews for the book, so we need a mutation:

```gql
type Mutation {
  createReview(text: String!, stars: Int): Review
}
```

The convention for naming a creation mutation is `create<Type>`, and it usually resolves to that type (hence `: Review` at the end). However, itâ€™s best practice to use a single input type as an argument instead of listing out all the scalars needed. So letâ€™s change it to:

```gql
type Mutation {
  createReview(input: CreateReviewInput!): Review
}
input CreateReviewInput {
  text: String!
  stars: Int
}
```

We also want people to be able to read past reviews, so we add a Query field:

```gql
type Query {
  hello: String!
  reviews: [Review!]!
}
```

We donâ€™t have a `Review` type yet, so we need to add that:

```
type Review {
  text: String!
  stars: Int
  fullReview: String!
}
```

All together, our new schema looks like this:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/1_0.2.0...2_0.2.0)

```js
const server = new ApolloServer({
  typeDefs: gql`
    type Query {
      hello: String!
      reviews: [Review!]!
    }
    type Review {
      text: String!
      stars: Int
      fullReview: String!
    }
    type Mutation {
      createReview(review: CreateReviewInput!): Review
    }
    input CreateReviewInput {
      text: String!
      stars: Int
    }
  `,
  resolvers: { ... }
})
```

We need a resolver for each field except for the `input` type. (Input types are only used for mutation arguments: fields canâ€™t resolve to input types, so input types donâ€™t need resolvers.) The structure of our `resolvers` object matches the schema, so it should look like:

```js
const server = new ApolloServer({
  typeDefs: ...
  resolvers: {
    Query: {
      hello: () => 
      reviews: () => 
    },
    Review: {
      text: () => 
      stars: () => 
      fullReview: () => 
    }
    Mutation: {
      createReview: () => 
    },
  }
})
```

Now letâ€™s fill them in! Weâ€™ll start with `createReview`:

```js
const reviews = [
  {
    text: 'Super-duper book.',
    stars: 5
  }
]

const server = new ApolloServer({
  typeDefs: ...
  resolvers: {
    ...
    Mutation: {
      createReview: (_, { review }) => {
        reviews.push(review)
        return review
      }
    }
  }
})
```

We donâ€™t need the first resolver parameter, just the second, which contains the mutation argumentâ€”the review. We add it to our array of reviews and return it (since our schema says that `createReview` resolves to an object of type `Review`).

Next we can implement the `reviews` Query field:

```js
const reviews = [
  {
    text: 'Super-duper book.',
    stars: 5
  }
]

const server = new ApolloServer({
  typeDefs: ...
  resolvers: {
    Query: {
      hello: ...
      reviews: () => reviews
    }
  }
})
```

For `Query.reviews` we just return our array of reviews. But a GraphQL server doesnâ€™t just return the `reviews` array to the client: it looks at the schema, sees that `Query.reviews` resolves to `[Review!]!`, checks to make sure the `reviews` array is non-null, and then resolves each object in the array as a `Review`. The way it does that is by calling `Review` field resolvers, which we also have to define:

```js
const reviews = [
  {
    text: 'Super-duper book.',
    stars: 5
  }
]

const server = new ApolloServer({
  typeDefs: ...
  resolvers: {
    Query: {
      hello: ...
      reviews: () => reviews
    },
    Review: {
      text: review => review.text
      stars: review => review.stars
      fullReview: review =>
        `Someone on the internet gave ${review.stars} stars, saying: "${
          review.text
        }"`
    }
  }
})
```

When the GraphQL server calls a `Review` field resolver, it provides the object as the first parameter, for example: 

```js
{
  text: 'Super-duper book.',
  stars: 5
}
```

The `text` and `stars` type fields we can just resolve to the corresponding object properties (for example, `text: review => review.text`). And we can actually take the `text` and `stars` resolvers out, because Apollo Server will do that by default. The `fullReview` field isnâ€™t a property on the object, so the default resolver won't work. So we define our own resolver, returning a string constructed from the reviewâ€™s properties.

All together, without the extraneous object property resolvers, we have:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/1_0.2.0...2_0.2.0)

```js
import { ApolloServer, gql } from 'apollo-server'

const reviews = [
  {
    text: 'Super-duper book.',
    stars: 5
  }
]

const server = new ApolloServer({
  typeDefs: gql`
    type Query {
      hello: String!
      reviews: [Review!]!
    }
    type Review {
      text: String!
      stars: Int
      fullReview: String!
    }
    type Mutation {
      createReview(review: CreateReviewInput!): Review
    }
    input CreateReviewInput {
      text: String!
      stars: Int
    }
  `,
  resolvers: {
    Query: {
      hello: () => 'ðŸŒðŸŒðŸŒŽ',
      reviews: () => reviews
    },
    Review: {
      fullReview: review =>
        `Someone on the internet gave ${review.stars} stars, saying: "${
          review.text
        }"`
    },
    Mutation: {
      createReview: (_, { review }) => {
        reviews.push(review)
        return review
      }
    }
  }
})

server
  .listen({ port: 4000 })
  .then(({ url }) => console.log(`GraphQL server running at ${url}`))
```

We can try it out with `npm run dev`, see that Playground loads, and try out the new query:

```gql
{
  reviews {
    text
    fullReview
    stars
  }
}
```

![reviews query](img/reviews-playground.png)

[localhost:4000: `{ reviews { text fullReview stars } }`](http://localhost:4000/)

We see our one hard-coded review. Now if we do our mutation followed by the `reviews` query, weâ€™ll see both that and the new review:

![createReview mutation](img/createReview-mutation.png)

[localhost:4000: `mutation { createReview(review: { text: "Passing", stars: 3 }) { text } }`](http://localhost:4000/)

![reviews query with two results](img/reviews-playground-two-results.png)

[localhost:4000: `{ reviews { text fullReview stars } }`](http://localhost:4000/)

Notice how the only things we changed in our server were our types (in the Apollo Server `typeDefs` parameter) and our resolvers. These two things (including code called by our resolver functions) will be the bulk of the coding we do for our GraphQL server.

## Authenticating

Background: [Authentication](bg.md#authentication)

> If youâ€™re jumping in here, `git checkout 2_0.2.0` (tag [2_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/2_0.2.0), or compare [2...3](https://github.com/GraphQLGuide/guide-api/compare/2_0.2.0...3_0.2.0))

One thing thatâ€™s done outside of types and resolvers is creating *context*, which is an object provided to resolvers. We set context using the [`context`](https://www.apollographql.com/docs/apollo-server/api/apollo-server#constructoroptions-apolloserver) of `ApolloServer()`. The `context` param is either an object or, more commonly, a function that returns an object. The function is called at the beginning of every request. The most common use of the `context` function is authenticating the user making the request and adding their info to the context. Hereâ€™s an example with a hard-coded user:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/2_0.2.0...3_0.2.0)

```js
const server = new ApolloServer({
  typeDefs: gql`
    type Query {
      me: User
      ...
    }
    type User {
      firstName: String
      lastName: String
    }
    ...
  `,
  resolvers: {
    Query: {
      me: (_, __, context) => context.user,
      ...
    },
    ...
  },
  context: () => {
    const user = {
      firstName: 'John',
      lastName: 'Resig'
    }

    return { user }
  }
})
```

Context is resolversâ€™ third parameter. For the `me` resolver, we just return the `user` property. We can try it out:

![me query](img/me-query.png)

[localhost:4000: `{ me { firstName lastName } }`](http://localhost:4000/)

Now letâ€™s figure out the real user. The Guide uses JWTs stored in LocalStorage, so authentication is done by cryptographically verifying the token provided in the requestâ€™s authorization header. We get the request as an argument to the [context function](https://www.apollographql.com/docs/apollo-server/api/apollo-server#parameters):

```js
import { getAuthIdFromJWT } from './util/auth'

const server = new ApolloServer({
  ...
  context: async ({ req }) => {
    const context = {}

    const jwt = req.headers.authorization
    const authId = await getAuthIdFromJWT(jwt)
    console.log(authId)

    return context
  }  
})
```

`getAuthIdFromJWT()` verifies the given JWT and returns what weâ€™re calling the userâ€™s *authId*â€”a unique string identifying the user that we get as the OpenID subject (`verifiedToken.sub` below). Hereâ€™s the functionâ€™s implementation:

[`src/util/auth.js`](https://github.com/GraphQLGuide/guide-api/blob/3_0.2.0/src/util/auth.js)

```js
import jwt from 'jsonwebtoken'
import jwks from 'jwks-rsa'
import { promisify } from 'util'

const verify = promisify(jwt.verify)

const jwksClient = jwks({
  cache: true,
  rateLimit: true,
  jwksUri: 'https://graphql.auth0.com/.well-known/jwks.json'
})

const getPublicKey = (header, callback) => {
  jwksClient.getSigningKey(header.kid, (e, key) => {
    callback(e, key.publicKey || key.rsaPublicKey)
  })
}

export const getAuthIdFromJWT = async token => {
  if (!token) {
    return
  }

  const verifiedToken = await verify(token, getPublicKey, {
    algorithms: ['RS256'],
    audience: 'https://api.graphql.guide',
    issuer: 'https://graphql.auth0.com/'
  })

  return verifiedToken.sub
}
```

> It calls `verify()` from the [`jsonwebtoken` package](https://github.com/auth0/node-jsonwebtoken/). In order to verify, it needs the Guideâ€™s public signing key. To get that, we use the [`jwks-rsa` package](https://github.com/auth0/node-jwks-rsa).

Now if we send a `{ hello }` query in Playground, we see `undefined` in the server logs. `authId` is undefined because `req.headers.authorization` is undefined. Which means that Playground isnâ€™t sending an authorization header with our query. We can set it by clicking â€œHTTP HEADERSâ€ in the bottom-left to open the JSON headers section. We want to set the authorization header to our JWT, but how do we get that? Itâ€™s produced by Auth0 during the login process and saved to localStorage, so we can get it by logging in at [graphql.guide/me](https://graphql.guide/me), opening the console, and entering:

```js
localStorage.getItem('auth.accessToken')
```

And it prints our JWT! Itâ€™s a long, random-looking, mostly alphanumeric string with some periods, dashes, and underscores. We can copy it to the Playground headers section:

```json
{ 
  "authorization": "your JWT here"
}
```

![Playground with authorization header filled in](img/authorization-header-playground.png)

> If you get a `jwt malformed` error, you likely didn't copy the whole token. Try opening the Application tab in Chrome dev tools, selecting `auth.accessToken`, and copying from the value panel at the bottom of the window.

Make note of your authorization headerâ€”youâ€™ll need it for making queries in other sections of this chapter.

Now when we run the query, we see our `authId` loggedâ€”something like this:

```sh
$ npm run dev

> guide-api@0.1.0 dev /guide-api
> babel-watch src/index.js

GraphQL server running at http://localhost:4000/
undefined
github|1615
```

The format is `github|N`, where `N` is our primary key in the users table of GitHubâ€™s database. (Itâ€™s an incrementing integer, which means that author John was GitHubâ€™s 1,615th user! ðŸ˜„)

The next thing that should happen in the code is looking up the user in our databaseâ€”something like:

```js
  context: async ({ req }) => {
    const context = {}

    const jwt = req.headers.authorization
    const authId = await getAuthIdFromJWT(jwt)
    context.user = await db.collection('users').findOne({ authId })

    return context
  }
```

But we donâ€™t have a database set up yet (weâ€™ll set it up in the next section and add users in [Setting user context](#setting-user-context)), so letâ€™s just test whether the `authId` is ours (replacing the strings with your own):

```js
  context: async ({ req }) => {
    const context = {}

    const jwt = req.headers.authorization
    const authId = await getAuthIdFromJWT(jwt)
    if (authId === 'github|1615') {
      context.user = {
        firstName: 'John',
        lastName: 'Resig'
      }
    }

    return context
  }
```

Now if we do a `me` query with our authorization header, we get our name:

![me query with results](img/me-query-with-auth-header.png)

But if we remove the header, we get null:

![me query with null](img/me-query-with-null-results.png)

This is because the `Query.me` resolver returns `context.user`, which is not defined.

In this section we learned how to put our JWT in the authorization header, verify it on the server, add the user to context, and access the context in resolvers. In the next section weâ€™ll look at connecting to a database and creating users.

## Data sources

* [Setting up](11.md#setting-up)
* [File structure](11.md#file-structure)
* [Creating reviews](11.md#creating-reviews)

### Setting up

Background: [MongoDB](bg.md#mongodb), [JavaScript classes](bg.md#javascript-classes)

> If youâ€™re jumping in here, `git checkout 3_0.2.0` (tag [3_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/3_0.2.0), or compare [3...4](https://github.com/GraphQLGuide/guide-api/compare/3_0.2.0...4_0.2.0))

Our reviews are currently stored in a JavaScript array variable. There are a few problems with this storage method. JavaScript variables are part of the Node server process, which means that:

- When the server process restarts (for instance when we deploy), our reviews get erased.
- When the server machine loses power (itâ€™s unlikely but possible for our data center to have a power outage), the data kept in RAM (which requires electricity to remember things) is lost. Since each processâ€™s variables are stored in RAM, our reviews get erased. 
- When we have multiple server processes (common in the age of Heroku, when itâ€™s easy to scale up small containers), the user will see different reviews based on which container each request is routed to.
- When we weâ€™re using serverless and donâ€™t have a long-running server process (widely introduced by AWS Lambda in 2014 and now, with Now 2.0 and Netlify Functions, becoming the standard way to host â€œserversâ€ ðŸ˜„), the process is started up for each request, so every `reviews` query would return just the single item we started out with.

The solution to all of these problems is to have a database that all of the server processes can talk toâ€”one that stores data on a drive that doesnâ€™t require power to remember things (either a disk drive that stores data on magnetic disks or a solid-state drive that stores data in flash memory). 

Weâ€™ll be using MongoDB because itâ€™s the most popular database among Node developers and because itâ€™s simple to use. The object-based API is easy to understand, and we donâ€™t need to create a schema or do migrations. (Of course, just as a schema is useful in GraphQL, itâ€™s useful for databases, and we could enforce a schema for our MongoDB database, for example with the [Mongoose ORM](https://mongoosejs.com), but weâ€™ll be using the simplest model layer possible.) For an introduction to MongoDB, check out the [MongoDB section](bg.md#mongodb) of the Background chapter.

There are two main ways to talk to a database from our GraphQL resolvers: [data sources](https://www.apollographql.com/docs/apollo-server/features/data-sources) and [Prisma](https://www.prisma.io/). We generally recommend Prisma (a next-generation ORM) for ease of use, and weâ€™ll learn how to use it in a [later section](#prisma). For now, weâ€™ll use a MongoDB data source, for the same reasons we used Create React App instead of Next.js or Gatsby in the React chapterâ€”data sources are more basic and familiar.

Data sources are classes that interact with a source of data (a database or a service). They often take care of some amount of batching queries and caching responses. Weâ€™ll go into them more deeply in the [More data sources](#more-data-sources) section. 

Usually there are two classes: a superclass that we import from a library that matches our type of database, and a subclass that we implement. There are superclass libraries for MongoDB, [SQL](#sql), and [REST](#rest), and weâ€™ll also learn how to [create our own](#custom-data-source). The MongoDB library is [`apollo-datasource-mongodb`](https://github.com/GraphQLGuide/apollo-datasource-mongodb), and its superclass is called `MongoDataSource`. Letâ€™s use it to create a data source for a `'reviews'` MongoDB collection:

[`src/data-sources/Reviews.js`](https://github.com/GraphQLGuide/guide-api/blob/4_0.2.0/src/data-sources/Reviews.js)

```js
import { MongoDataSource } from 'apollo-datasource-mongodb'

export default class Reviews extends MongoDataSource {
  all() {
    return this.collection.find().toArray()
  }
}
```
 
We start with a single method `all()` that fetches all reviews from the collection. Where does `this.collection` come from, you might ask? Itâ€™s set in the constructor (defined in `MongoDataSource`), which gets the collection as an argument:

```js
const reviews = new Reviews(db.collection('reviews'))
```

But in order to do that, we need to set up the database! We can install and start MongoDB on Windows with [these steps](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/#install-mdb-edition) or with [Homebrew](https://brew.sh/) on a Mac:

```sh

$ brew tap mongodb/brew
$ brew install mongodb-community
$ brew services start mongodb-community
```

The database is now running on our computer. We connect to it with the [`mongodb`](http://mongodb.github.io/node-mongodb-native/) package:

[`src/db.js`](https://github.com/GraphQLGuide/guide-api/blob/4_0.2.0/src/db.js)

```js
import { MongoClient } from 'mongodb'

export let db

const URL = 'mongodb://localhost:27017/guide'

const client = new MongoClient(URL, { useNewUrlParser: true })
client.connect(e => {
  if (e) {
    console.error(`Failed to connect to MongoDB at ${URL}`, e)
    return
  }

  db = client.db()
})
```

`'mongodb://localhost:27017/'` is the default URL of the MongoDB server running on our computer, and `'guide'` is the name of our database. Now we can import `db` and use it to create our data source. Data sources are created in a function that we pass to `ApolloServer`:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/3_0.2.0...4_0.2.0)

```js
import Reviews from './data-sources/Reviews'
import { db } from './db'

const server = new ApolloServer({
  typeDefs: ...
  resolvers: ...
  dataSources: () => ({
    reviews: new Reviews(db.collection('reviews'))
  }),
  context: ...
})
```

Like the `context` function, the `dataSources` function is run for each request, so each request gets a new instance of each data source. `ApolloServer` adds data sources to the context so that we can access them in our resolvers like this:

```js
const server = new ApolloServer({
  typeDefs: ...
  resolvers: {
    Query: {
      me: (_, __, context) => context.user,
      hello: () => 'ðŸŒðŸŒðŸŒŽ',
      reviews: (_, __, { dataSources }) => dataSources.reviews.all()
    },
    ...
  },
  dataSources: () => ({
    reviews: new Reviews(db.collection('reviews'))
  }),
  context: ...
})
```

We always get context as the third argument to our resolvers, and here in the `Query.reviews` resolver weâ€™re destructuring contextâ€™s `dataSources` property. Then we get the instance of our `Reviews` data source, `dataSources.reviews`, and call its `.all()` method. Now when we do our reviews query again, we get an empty array, since nothing is yet in the `reviews` collection:

![reviews query with empty array result](img/empty-reviews.png)

<!-- { reviews { text } } -->

### File structure

> If youâ€™re jumping in here, `git checkout 4_0.2.0` (tag [4_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/4_0.2.0), or compare [4...5](https://github.com/GraphQLGuide/guide-api/compare/4_0.2.0...5_0.2.0))

Our `src/index.js` file is getting long, and continuing to put most of our code in one file would get ridiculous ðŸ˜„. Letâ€™s really simplify this file and get our `ApolloServer` creation down to just:

```js
const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context
})
``` 

with each parameter imported from other files. Thereâ€™s no one right way to structure the other files, but our favorite is:

- directories for the schema, resolvers, and data sources
- one file for each major type, for example:
  - `schema/Review.graphql` for the `Review` type schema
  - `resolvers/Review.js` for the resolvers associated with the `Review` type 
  - `data-sources/Reviews.js` for the `reviews` collection data source

With this structure, our `src/` looks like:

```
.
â”œâ”€â”€ context.js
â”œâ”€â”€ data-sources
â”‚   â”œâ”€â”€ Reviews.js
â”‚   â””â”€â”€ index.js
â”œâ”€â”€ db.js
â”œâ”€â”€ index.js
â”œâ”€â”€ resolvers
â”‚   â”œâ”€â”€ Review.js
â”‚   â”œâ”€â”€ User.js
â”‚   â””â”€â”€ index.js
â”œâ”€â”€ schema
â”‚   â”œâ”€â”€ Review.graphql
â”‚   â”œâ”€â”€ User.graphql
â”‚   â””â”€â”€ schema.graphql
â””â”€â”€ util
    â””â”€â”€ auth.js
```

Some notes on the above:

- We havenâ€™t yet made a data source for the users collection.
- We have context in a single file `context.js`, but if that ever got too long, we could make a `context/` directory and split it into multiple files.
- We have `index.js` files so that we can import the directory (for example `import resolvers from './resolvers'` imports from `'./resolvers/index.js'`). 
- We donâ€™t have an `index.js` in `schema/` because theyâ€™re `.graphql` files, and you canâ€™t import a directory with GraphQL imports.

For GraphQL imports, weâ€™re using a babel plugin called [`babel-plugin-import-graphql`](https://github.com/detrohutt/babel-plugin-import-graphql) which replaces our imported `.graphql` files with schema objects (the same ones that the `gql` template string tag creates). We could have instead done JS files with template strings and given an array of them as our `typeDefs` parameter, which would look like this:

```js
// schema/Review.js
import gql from 'graphql-tag'

export default gql`
type Review {
  text: String!
  stars: Int
  fullReview: String!  
}
`

// schema/User.js
import gql from 'graphql-tag'

export default gql`
type User {
  firstName: String
  lastName: String
}
`

// schema/index.js
import reviewSchema from './Review.js'
import userSchema from './User.js'

export default [reviewSchema, userSchema]

// index.js
import typeDefs from './schema'

const server = new ApolloServer({
  typeDefs,
  ...
})
```

Instead, we have:

[`src/schema/schema.graphql`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/schema/schema.graphql)

```gql
type Query {
  hello: String!
}

# import Review first
#import 'Review.graphql'
#import 'User.graphql'
```

And the babel plugin makes the `#import` statements work, bringing in these files:

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/schema/Review.graphql)

```gql
type Review {
  text: String!
  stars: Int
  fullReview: String!
}

extend type Query {
  reviews: [Review!]!
}

type Mutation {
  createReview(review: CreateReviewInput!): Review
}

input CreateReviewInput {
  text: String!
  stars: Int
}
```

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/schema/User.graphql)

```gql
type User {
  firstName: String
  lastName: String
}

extend type Query {
  me: User
}
```

`extend type Query` adds fields to the existing `Query` type (which we defined first in `schema.graphql`). `Review.graphql` is the first to define `Mutation`, so it doesnâ€™t use `extend`. And we import it first so that future files we import below can all do `extend type Mutation`. (And we include the `# import Review first` comment in the file so that othersâ€”or our future selves ðŸ˜„â€”wonâ€™t change the order.)

Thanks to our babel plugin, our `schema.graphql` can be imported like this:

```js
import typeDefs from './schema/schema.graphql'
```

In our `resolvers/` directory we have `Review.js` and `User.js`, which just have the resolvers related to the `Review` and `User` types, respectively:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/resolvers/Review.js)

```js
export default {
  Query: {
    reviews: (_, __, { dataSources }) => dataSources.reviews.all()
  },
  Review: {
    fullReview: review =>
      `Someone on the internet gave ${review.stars} stars, saying: "${
        review.text
      }"`
  },
  Mutation: {
    createReview: (_, { review }) => {
      reviews.push(review)
      return review
    }
  }
}
```

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/resolvers/User.js)

```js
export default {
  Query: {
    me: (_, __, context) => context.user
  }
}
```

We combine them in `index.js`:

[`src/resolvers/index.js`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/resolvers/index.js)

```js
const resolvers = {
  Query: {
    hello: () => 'ðŸŒðŸŒðŸŒŽ'
  }
}

import Review from './Review'
import User from './User'

export default [resolvers, Review, User]
```

We can now import all resolvers with:

```js
import resolvers from './resolvers'
```

Next up is data sources! We already have `src/data-sources/Review.js`, so all we need is an `index.js` that will combine future data sources with our `Review.js` and export the function that creates new instances:

[`src/data-sources/index.js`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/data-sources/index.js)

```js
import Reviews from './Reviews'
import { db } from '../db'

export default () => ({
  reviews: new Reviews(db.collection('reviews'))
})
```

The last thing we want to move out of `src/index.js` is our context function. Itâ€™s small enough that we can put it in a single file:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/context.js)

```js
import { getAuthIdFromJWT } from './util/auth'

export default async ({ req }) => {
  const context = {}

  const jwt = req.headers.authorization
  const authId = await getAuthIdFromJWT(jwt)
  if (authId === 'github|1615') {
    context.user = {
      firstName: 'John',
      lastName: 'Resig'
    }
  }

  return context
}
```

This brings our entire [`src/index.js`](https://github.com/GraphQLGuide/guide-api/blob/5_0.2.0/src/index.js) to just:

```js
import { ApolloServer } from 'apollo-server'
import typeDefs from './schema/schema.graphql'
import resolvers from './resolvers'
import dataSources from './data-sources'
import context from './context'

const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context
})

server
  .listen({ port: 4000 })
  .then(({ url }) => console.log(`GraphQL server running at ${url}`))
```

So clean! âœ¨

### Creating reviews

> If youâ€™re jumping in here, `git checkout 5_0.2.0` (tag [5_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/5_0.2.0), or compare [5...6](https://github.com/GraphQLGuide/guide-api/compare/5_0.2.0...6_0.2.0))

In [Setting up](#setting-up), we updated our `reviews` query to fetch from MongoDB, but our reviews database collection is empty! So letâ€™s get reviews into the database. API clients usually find it helpful if we give them an ID for objects we send them, so letâ€™s add one to the schema:

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/compare/5_0.2.0...6_0.2.0)

```gql
type Review {
  id: ID!
  text: String!
  stars: Int
  fullReview: String!
}
```

Letâ€™s update our `createReview` mutation to talk to the database:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/5_0.2.0...6_0.2.0)

```js
export default {
  ...
  Mutation: {
    createReview: (_, { review }, { dataSources }) =>
      dataSources.reviews.create(review)
  }
}
```

It just calls a method on our data source, which we need to define:

[`src/data-sources/Reviews.js`](https://github.com/GraphQLGuide/guide-api/compare/5_0.2.0...6_0.2.0)

```js
export default class Reviews extends MongoDataSource {
  all() {
    return this.collection.find().toArray()
  }

  create(review) {
    this.collection.insertOne(review)
    return review
  }
}
```

`createReview` resolves to a `Review`, so we need to return `review`. And it needs to have an ID. MongoDBâ€™s [`insertOne()`](http://mongodb.github.io/node-mongodb-native/3.2/api/Collection.html#insertOne) synchronously adds a generated `_id` to the argument we give it, so when we `return review`, `review._id` is filled in. We return before the MongoDB node library talks to the database in order to send a response to the client as quickly as possible. If we wanted to wait until after we knew that the database operation had completed successfully, we could `await`:

```js
  async create(review) {
    await this.collection.insertOne(review)
    return review
  }
```

In this case, if there were a problem with the database insertion, `insertOne()` would throw an error, which Apollo Server would format and send to the client. Our method is now `async`, which means it returns a Promise, which means our `createReview` resolver returns a Promise. Apollo Server waits for Promises to resolve before continuing the GraphQL execution process.

While itâ€™s good that in either case, the `_id` property is added to our `review` object, `_id` doesnâ€™t match with our schema (the schema says the `Review` type has a field named `id`, without an underscore). If we create a review and include `id` in the selection set:

```gql
mutation {
  createReview(review: { text: "Passing", stars: 3 }) {
    id
    text
    stars
  }
}
```

then we get this error:

![Review.id error in Playground](img/non-nullable-id-error.png)

Apollo Server is trying to resolve the `id` field in our selection set, looking at the review object we return from the `createReview` resolver, and not finding an `id` property on that object. When it canâ€™t find a property or `Review` field resolver, it normally returns `null`. However, the `Review` type in our schema has an `!` in the type of `id` (`id: ID!`), so it is non-nullable. Hence the error text: `"Cannot return null for non-nullable field Review.id."`

We can fix this by adding a `Review.id` resolver:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/5_0.2.0...6_0.2.0)

```js
export default {
  ...
  Review: {
    id: review => review._id,
    fullReview: review =>
      `Someone on the internet gave ${review.stars} stars, saying: "${
        review.text
      }"`
  },
  ...
}
```

`review._id` is an objectâ€”an instance of [`ObjectId`](http://mongodb.github.io/node-mongodb-native/3.2/api/ObjectId.html), MongoDBâ€™s default ID type. `Review.id` is supposed to resolve to the GraphQL `ID` scalar type, which is serialized as a string. This might make us think that we should be getting an error. But if we try our Playground mutation again, itâ€™s successful. The reason is that because the schema says the `id` resolver should return an `ID`, Apollo Server knows to call `.toString()` on the object we return.

![Successful createReview mutation in Playground](img/createReview-success.png)

We can now see the list of reviews in the databaseâ€”one for each time we ran the `createReview` mutation:

```gql
{ 
  reviews {
    id
    text
    stars
    fullReview
  }
}
```

![reviews query in Playground](img/reviews-query.png)

## Custom scalars

> If youâ€™re jumping in here, `git checkout 6_0.2.0` (tag [6_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/6_0.2.0), or compare [6...7](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0))

In the last section we mentioned that the `ID` scalar is serialized like a string, but what does that process look like, and how do we make our own scalars? The only built-in scalars are `Int`, `Float`, `String`, `Boolean`, and `ID`. Another scalar type that most apps use is a date. For example, it would be nice to have a `Review.createdAt`. We could make it an `Int`, but then is it seconds or milliseconds since the [Unix epoch](https://en.wikipedia.org/wiki/Epoch_(computing\))? Or it could be a `String`, but there are a lot of string date formats out there. And both ways are missing validation (testing whether the string is a valid date string) and the improved understanding that comes from being able to know, looking at the schema, which fields are meant to be dates. So letâ€™s make our own `Date` scalar. We can add it to our schema:

[`src/schema/schema.graphql`](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0)

```gql
scalar Date

type Query {
  hello: String!
  isoString(date: Date!): String!
}

#import 'Review.graphql'
#import 'User.graphql'
```

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0)

```gql
type Review {
  id: ID!
  text: String!
  stars: Int
  fullReview: String!
  createdAt: Date!
  updatedAt: Date!
}

...
```

First we declare the new scalar type (`scalar Date`), and then we use it for a new `isoString` query as well as `createdAt` and `updatedAt` fields on `Review`. We make them non-nullable because all Review objects will have them.

> We can use the word `Date` for our type because we donâ€™t have other types of dates or times in our app. If we also had a `Date` that had no time component, like a birthday, or a `Time` that had no date component, like 14:00 (2 p.m.), we could call our new scalar `DateTime`.

`isoString` takes a `Date` as an argument and returns the date formatted as a string in the [ISO format](https://en.wikipedia.org/wiki/ISO_8601):

[`src/resolvers/index.js`](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0)

```js
const resolvers = {
  Query: {
    hello: () => 'ðŸŒðŸŒðŸŒŽ',
    isoString: (_, { date }) => date.toISOString()
  }
}
```

Next we add to our resolvers a `GraphQLScalarType`, which tells Apollo Server how to handle a custom scalar. It will look like this:

[`src/resolvers/Date.js`](https://github.com/GraphQLGuide/guide-api/blob/7_0.2.0/src/resolvers/Date.js)

```js
import { GraphQLScalarType } from 'graphql'

export default {
  Date: new GraphQLScalarType({
    name:
    description:
    parseValue(value) {}
    parseLiteral(ast) {}
    serialize(date) {}
  })
}
```

`GraphQLScalarType` takes five parameters:

- `name` matches the scalar name we added to the schema, so `'Date'`
- `description` is shown in the schema section of GraphiQL and Playground. It says what the scalar represents and how it appears in the JSON response from a server. The built-in description for `ID`, for instance, is:

> The `ID` scalar type represents a unique identifier, often used to refetch an object or as a key for a cache. The ID type appears in a JSON response as a String; however, it is not intended to be human-readable. When expected as an input type, any string (such as `"4"`) or integer (such as `4`) input value will be accepted as an ID.

- `parseValue(value)` is a function called when the server receives a query variable for a Date argument. The variableâ€™s value is passed to `parseValue()`, and the function should return the value in our desired formatâ€”in this case, a JavaScript Date object. For example, if the client sends this query:

```gql
query ISOString($date: Date!) {
  isoString(date: $date)
}
```

with this as the variables JSON:

```json
{
  "date": 1442188800000
}
```

then `parseValue` is passed the integer `1442188800000` and should return a JS Date object, which Apollo Server will provide to our resolver, which calls `.toISOString()` on the JS Date object:

```
  isoString: (_, { date }) => date.toISOString()
```

![isoString query with a variable](img/isoString-with-variable.png)

- `parseLiteral(ast)` is called when the server receives a query with a literal argumentâ€”meaning the argument is written in the query document itself instead of being provided separately in JSON (as variables are). `ast` stands for abstract syntax tree, which is an object that Apollo Server uses to parse the query document. `ast.value` has the literal value, and is always a string. Similar to `parseValue()`, `parseLiteral()` should return the serverâ€™s internal representation of the scalar type. If the client sends this query document:

```gql
{
  isoString(date: 1442188800000)
}
```

Then `parseLiteral(ast)` will be called, and `ast.value` will be `"1442188800000"`.

- `serialize(date)` is called when the server is formatting a JSON response for the client. A resolver returns a JS Date object, then Apollo Server calls `serialize()` with that object, and `serialize()` returns the date in a format that can be put into the JSON responseâ€”which in our implementation of the `Date` scalar is an integer. For example, if the `Review.createdAt` resolver returns a JS Date, we would see an integer in the response:

![Query for Review.createdAt returning an integer](img/reviews-createdAt.png)

> If you're following along, this query won't work until we fill in `Date.js` and add it to `src/resolvers/index.js`.

Hereâ€™s a basic implementation of the above:

```js
import { GraphQLScalarType } from 'graphql'

export default {
  Date: new GraphQLScalarType({
    name: 'Date',
    description: `The \`Date\` scalar type represents a single moment in time. 
    It is serialized as an integer, equal to the number of milliseconds since 
    the Unix epoch.`,
    parseValue: value => new Date(value),
    parseLiteral: ast => new Date(parseInt(ast.value)),
    serialize: date => date.getTime()
  })
}
```

`parseValue()` takes the integer and creates a `Date`. `parseLiteral()` gets the `ast.value` string, converts it into an integer, and creates a `Date`. `serialize()` takes the date and returns the milliseconds since epoch.

One important aspect of defining a custom scalar that weâ€™re missing is validation. If we check the values weâ€™re getting and throw errors with descriptive messages, it will help people using our API. Letâ€™s do that:

[`src/resolvers/Date.js`](https://github.com/GraphQLGuide/guide-api/blob/7_0.2.0/src/resolvers/Date.js)

```js
import { GraphQLScalarType } from 'graphql'
import { Kind } from 'graphql/language'

const isValid = date => !isNaN(date.getTime())

export default {
  Date: new GraphQLScalarType({
    name: 'Date',
    description:
      `The \`Date\` scalar type represents a single moment in time. It is serialized as an integer, equal to the number of milliseconds since the Unix epoch.',

    parseValue(value) {
      if (!Number.isInteger(value)) {
        throw new Error('Date values must be integers')
      }

      const date = new Date(value)
      if (!isValid(date)) {
        throw new Error('Invalid Date value')
      }

      return date
    },

    parseLiteral(ast) {
      if (ast.kind !== Kind.INT) {
        throw new Error('Date literals must be integers')
      }

      const date = new Date(parseInt(ast.value))
      if (!isValid) {
        throw new Error('Invalid Date literal')
      }

      return date
    },

    serialize(date) {
      if (!(date instanceof Date)) {
        throw new Error(
          'Resolvers for Date scalars must return JavaScript Date objects'
        )
      }

      if (!isValid(date)) {
        throw new Error('Invalid Date scalar')
      }

      return date.getTime()
    }
  })
}
```

In `parseValue()` and `parseLiteral()`, we check whether the client sent an integer, then we create a JS Date and check whether itâ€™s valid. In `serialize()` we check that the value returned from a resolver is a JS Date object, then we check if itâ€™s a valid date, and finally we return the milliseconds since epoch.

We add this file to our resolvers in `resolvers/index.js` by importing and adding to our `resolversByType` array:

[`src/resolvers/index.js`](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0)

```js
...

import Review from './Review'
import User from './User'
import Date from './Date'

export default [resolvers, Review, User, Date]
```

We saw our `isoString` query working above, but now if we make a mistake, we get a helpful error message:

![Error passing a string as a date literal](img/date-literal-error.png)

![Error passing a string as a date variable](img/date-variable-value-error.png)

The last part of our schema change for which we have to implement resolvers is `Review`â€™s `createdAt` and `updatedAt`. In MongoDB, the creation time is included in the default ID format, [ObjectId](https://docs.mongodb.com/manual/reference/method/ObjectId/). The first 4 bytes are the seconds since Unix epoch, so we can get the creation time from that. (And since itâ€™s the first 4 bytes, we can also sort by an ObjectId to order by most/least recently created.) The `mongodb` node library provides a method `ObjectId.getTimestamp()` that extracts the date for us:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0)

```js
export default {
  Query: ...
  Review: {
    ...
    createdAt: review => review._id.getTimestamp()
  },
  Mutation: ...
}
```

`updatedAt` is a field that weâ€™ll have to store in the database when reviews are created and update when reviews are modified. We donâ€™t have a way of modifying reviews yet, so weâ€™ll just add a line to our creation method:

[`src/data-sources/Reviews.js`](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0)

```js
import { MongoDataSource } from 'apollo-datasource-mongodb'

export default class Reviews extends MongoDataSource {
  ...

  create(review) {
    review.updatedAt = new Date()
    this.collection.insertOne(review)
    return review
  }
}
```

Now we can include `updatedAt` in our `reviews` query, but we get the error `Cannot return null for non-nullable field Review.updatedAt`:

![Error querying for Review.updatedAt](img/review-updatedAt-error.png)

Apollo Server is telling us that it canâ€™t return `null` for `Review.updatedAt` to the client because the schema says itâ€™s a non-nullable field. Why is it *trying* to return `null` for `Review.updatedAt`? Itâ€™s notâ€”our resolver is. Our `reviews` resolver is returning reviews fetched from the database, but none of them have an `updatedAt` property because they were inserted before we updated our `Reviews.create()` data source method. We could fix our reviews in the database by adding an `updatedAt` field, but letâ€™s just delete them and re-create. If youâ€™d like a GUI (*Graphical User Interface*, i.e., a program that runs in its own window instead of in the command line) for interacting with MongoDB, we recommend [MongoDB Compass](https://www.mongodb.com/products/compass). Hereâ€™s how to delete all of our reviews using the `mongo` command-line shell:

```sh
$ mongo
MongoDB shell version v4.0.3
connecting to: mongodb://127.0.0.1:27017
...

> use guide
switched to db guide
> db.reviews.find({})
{ "_id" : ObjectId("5cdfb1946df8548efb438535"), "text" : "Passing", "stars" : 3 }
{ "_id" : ObjectId("5cdfb1e4a1cf288f4d86dced"), "text" : "Passing", "stars" : 3 }
{ "_id" : ObjectId("5cdfb28e48435b90119bd2c6"), "text" : "Passing", "stars" : 3 }
> db.reviews.remove({})
WriteResult({ "nRemoved" : 3 })
> db.reviews.find({})
> exit
bye
```

Our second call to `db.reviews.find({})` doesnâ€™t show results because the collection is now empty. And when we do our `reviews` query, we get back an empty array. Now if we use Playground to send a `createReview` mutation, then we can do a `reviews` query with the `createdAt` and `updatedAt` fields:

![reviews query with createdAt and updatedAt in the selection set](img/reviews-with-updatedAt.png)

The last three digits of `createdAt` will always be `000` because the API returns milliseconds since Epoch, and all thatâ€™s stored in the ObjectId is *seconds* since Epoch.

An alternative to clearing the database collection would have been to add a resolver for `Review.updatedAt` that returns `Review.createdAt` when thereâ€™s no `updatedAt` property on the review object. In order to call another resolver, weâ€™d need to name the resolverâ€™s object and move `export default` to the end:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/6_0.2.0...7_0.2.0)

```js
const resolvers = {
  Query: {
    reviews: ...
  },
  Review: {
    id: ...
    fullReview: ...
    createdAt: review => review._id.getTimestamp(),
    updatedAt: review => review.updatedAt || resolvers.Review.createdAt(review)
  },
  Mutation: {
    createReview: ...
  }
}

export default resolvers
```

Then we could reference another resolver function (`resolvers.Review.createdAt(review)`).

In this section we created a new `Date` scalar type, added `Query.isoString`, which has a `Date` argument, and `Review.createdAt` and `Review.updatedAt`, which resolve to `Date`s. Weâ€™ll continue to use the `Date` type in the rest of our app, for instance for `User.createdAt/updatedAt` in the next section.

## Creating users

* [Protecting with secret key](11.md#protecting-with-secret-key)
* [Setting user context](11.md#setting-user-context)
* [Linking users to reviews](11.md#linking-users-to-reviews)

Currently our `User` type just has two fields (`firstName` and `lastName`), and we arenâ€™t storing users in the database. If we wanted to continue without storing users in the database, we could fetch any further information we want, like email address or GitHub username, from Auth0 or GitHub whenever we needed it. However, this would be a little more complicated than querying our database, introduce latency (it takes longer for our server to talk to their servers than to query our database), and introduce another point of failure (if their services went down or there was a network failure between us and them). Furthermore, weâ€™re going to have to store some new user data (for instance, which sections theyâ€™ve read, or which reviews theyâ€™ve favorited), so we might as well have other user data we need stored along with it. In the first part of this section, weâ€™ll create user documents in a new users Mongo collection. In the second part, weâ€™ll query the collection to set the user context for resolvers.

### Protecting with secret key

> If youâ€™re jumping in here, `git checkout 7_0.2.0` (tag [7_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/7_0.2.0), or compare [7...8](https://github.com/GraphQLGuide/guide-api/compare/7_0.2.0...8_0.2.0))

There are two ways we could create our user doc. One is, in our context function, checking if the user we decode from the JWT exists in the database, and if they donâ€™t, fetching their data from Auth0 and GitHub and saving it to the database. The other method is to use an Auth0 hookâ€”a function we write that runs on a certain trigger. The â€œPost User Registrationâ€ hook runs whenever a user first uses their GitHub account to log in. Inside of our hook function, we can put together the user data we want and send it to the server in a mutation. The Guide hook looks something like this:

```js
const request = require('graphql-request').request
const pick = require('lodash').pick

const query = `
mutation createUserFromHook($user: CreateUserInput!, $secretKey: String!) {
  createUser(user: $user, secretKey: $secretKey) {
    id
  }
}`

module.exports = function (user, context, cb) {
  const secretKey = context.webtask.data.secretKey
  const input = pick(user, 'username', 'email')
  input.authId = user.id
  const variables = {
    user: input,
    secretKey
  }
  request('https://api.graphql.guide/graphql', query, variables).then(data => cb(null, data))
};
```

The exported function is given data about the user, and then sends a `createUser` mutation to the Guide server. The mutation takes as arguments both the user data and a `secretKey`â€”a secret string that the server verifies before running the mutation, so that no one but the hook can create users.

When we want to protect a query, mutation, or field from being accessed by anyone, normally we use a JWT in the authorization header. We could create a JWT for this purpose, but itâ€™s easier to generate a random string (i.e. key). We could put the key in the authorization header like is usually done for API keys, which would look like this:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/compare/7_0.2.0...8_0.2.0)

```js
import { getAuthIdFromJWT } from './util/auth'

const API_KEYS = ['alohomora', 'speak-friend']

export default async ({ req }) => {
  const context = {}

  if (API_KEYS.includes(req.headers.authorization)) {
    context.apiUser = true
  } else {
    const jwt = req.headers.authorization
    const authId = await getAuthIdFromJWT(jwt)
    if (authId === 'github|1615') {
      context.user = {
        firstName: 'John',
        lastName: 'Resig'
      }
    }
  }

  return context
}
```

We add an if statement and set `context.apiUser` to `true`, which we can check inside our resolvers.

However, since we only need the key for this one mutation, weâ€™ll add a `secretKey` argument to it. As always, we start with the schema:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/7_0.2.0...8_0.2.0)

```gql
type User {
  firstName: String
  lastName: String
}

extend type Query {
  me: User
}

extend type Mutation {
  createUser(user: CreateUserInput!, secretKey: String!): User
}

input CreateUserInput {
  firstName: String!
  lastName: String!
  username: String!
  email: String!
  authId: String!
}
```

Weâ€™re extending the `Mutation` type that first appears in `src/schema/Review.graphql`, and we follow the standard practice of our creation mutation resolving to the type it creates, `User`. And we create a new input type with the user fields we want. Next, we implement the `createUser` resolver:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/7_0.2.0...8_0.2.0)

```js
export default {
  Query: {
    me: (_, __, context) => context.user
  },
  Mutation: {
    createUser(_, { user, secretKey }, context) {
      // TODO
    }
  }
}
```

We have three things to do in our resolver:

- verify `secretKey` is correct
- create the user
- return the user

Best practice is to avoid committing secrets to git, so we wonâ€™t do `if (secretKey !== 'foo')`. Instead, weâ€™ll use the [`dotenv`](https://github.com/motdotla/dotenv#readme) package to set an environment variable. First we need to generate a secret:

```sh
$ node
> require('crypto').randomBytes(15, (e, buffer) => console.log(buffer.toString('hex')))
9e769699fae6f594beafb46e9078c2
> .exit
```

Then we put it in a file named `.env`:

```
SECRET_KEY=9e769699fae6f594beafb46e9078c2
```

That we have git ignore:

`.gitignore`

```
node_modules/
dist/
.env
```

And then we have `dotenv` read the values listed in `.env` into `process.env` at the beginning of our code (the first line of `src/index.js`):

```js
import 'dotenv/config'
import { ApolloServer } from 'apollo-server'
import typeDefs from './schema/schema.graphql'
...
```

And then we can reference `process.env.SECRET_KEY` in our code:

`src/resolvers/User.js`

```js
import { AuthenticationError } from 'apollo-server'

export default {
  Query: ...
  Mutation: {
    createUser(_, { user, secretKey }, context) {
      if (secretKey !== process.env.SECRET_KEY) {
        throw new AuthenticationError('wrong secretKey')
      }
      
      // TODO
    }
  }
}
```

> Weâ€™ll learn about errors in the [Errors section](#errors).

The next step is creating the user, for which we need a users data source! We create a new file:

[`src/data-sources/Users.js`](https://github.com/GraphQLGuide/guide-api/compare/7_0.2.0...8_0.2.0)

```js
import { MongoDataSource } from 'apollo-datasource-mongodb'

export default class Users extends MongoDataSource {
  create(user) {
    user.updatedAt = new Date()
    this.collection.insertOne(user)
    return user
  }
}
```

The `create()` method adds an `updatedAt` property, inserts, and returns, just like our `Reviews` data source. We include our new data source in the index file:

[`src/data-sources/index.js`](https://github.com/GraphQLGuide/guide-api/compare/7_0.2.0...8_0.2.0)

```js
import Reviews from './Reviews'
import Users from './Users'
import { db } from '../db'

export default () => ({
  reviews: new Reviews(db.collection('reviews')),
  users: new Users(db.collection('users'))
})
```

So now `users` will be available in our resolvers at `context.dataSources.users`:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/7_0.2.0...8_0.2.0)

```js
export default {
  Query: ...
  Mutation: {
    createUser(_, { user, secretKey }, { dataSources } ) {
      if (secretKey !== process.env.SECRET_KEY) {
        throw new AuthenticationError('wrong secretKey')
      }
      
      return dataSources.users.create(user)
    }
  }
}
```

Now the `createUser` should work (using your own data and `authId` for the `user` argument):

![Successful createUser query in Playground](img/createUser.png)

```gql
mutation {
  createUser(
    user: {
      firstName: "John"
      lastName: "Resig"
      username: "jeresig"
      email: "john@graphql.guide"
      authId: "github|1615"
    }
    secretKey: "9e769699fae6f594beafb46e9078c2"
  ) {
    firstName
    lastName
  }
}
```

### Setting user context

> If youâ€™re jumping in here, `git checkout 8_0.2.0` (tag [8_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/8_0.2.0), or compare [8...9](https://github.com/GraphQLGuide/guide-api/compare/8_0.2.0...9_0.2.0))

Now that we have our user document in the database, we can fetch it and put it in context:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/compare/8_0.2.0...9_0.2.0)

```js
import { getAuthIdFromJWT } from './util/auth'
import { db } from './db'

export default async ({ req }) => {
  const context = {}

  const jwt = req.headers.authorization
  const authId = await getAuthIdFromJWT(jwt)
  const user = await db.collection('users').findOne({ authId })
  if (user) {
    context.user = user
  }

  return context
}
```

One possible concern with this method is latencyâ€”every authenticated request now has to wait for a round trip to the database before resolvers are run, and if the request is one that doesnâ€™t use `context.user`, weâ€™ve wasted that time. Itâ€™s usually not a long enough period of time to be concerned about, but if we were, we could solve it in a couple of ways:

- Store whatever user data we needed in the JWT. Then we wouldnâ€™t have to fetch it from the databaseâ€”weâ€™d just decode it. This takes some additional coding, and what the code looks like depends on how youâ€™re creating the JWT (in this case weâ€™d be talking to Auth0 via their API). JWTs have a limited size (~7k sent in an HTTP header), but that wouldnâ€™t be a limiting factor for us, since we donâ€™t have that much user data. 
- Put a Promise on the context instead of the doc:

```js
import { getAuthIdFromJWT } from './util/auth'
import { db } from './db'

export default async ({ req }) => {
  const context = {}

  const jwt = req.headers.authorization
  const authId = await getAuthIdFromJWT(jwt)
  context.userPromise = db.collection('users').findOne({ authId })

  return context
}
```

And then any resolvers that needed user data would do:

```js
const user = await context.userPromise
```

That would clutter the code a little, so letâ€™s stick with our `context.user` code. âœ¨ðŸ˜Š

Now if we do the `me` query (and set our authorization header as we did in the [Authenticating](#authenticating) section), we should be able to get the name from our user document:

![me query with authorization header and returned name](img/me-with-name.png)

Thereâ€™s more data about a user that our web client will need, so letâ€™s add to our schema:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/8_0.2.0...9_0.2.0)

```gql
type User {
  id: ID!
  firstName: String!
  lastName: String!
  username: String!
  email: String!
  photo: String!
  createdAt: Date!
  updatedAt: Date!
}

extend type Query {
  me: User
}

extend type Mutation {
  createUser(user: CreateUserInput!, secretKey: String!): User
}

input CreateUserInput {
  firstName: String!
  lastName: String!
  username: String!
  email: String!
  authId: String!
}
```

`username`, `email`, and `updatedAt` are fields of the user document, so we donâ€™t need resolvers for them. We do need resolvers for `id`, `photo`, and `createdAt`. Also note that we donâ€™t have a `User.authId` field: while itâ€™s part of `CreateUserInput` and is stored in the user document, we donâ€™t need the client to be able to access it, so leaving it out of the `User` type means they wonâ€™t be able to query for it. 

For the `createdAt` resolver, we can do the same as the `Review.createdAt` resolver, calling the `getTimestamp()` method of the `ObjectId`:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/8_0.2.0...9_0.2.0)

```js
export default {
  Query: {
    me: (_, __, context) => context.user
  },
  User: {
    id: ({ _id }) => _id,
    photo(user) {
      // user.authId: 'github|1615'
      const githubId = user.authId.split('|')[1]
      return `https://avatars.githubusercontent.com/u/${githubId}`
    },
    createdAt: user => user._id.getTimestamp()
  },
  Mutation: ...
}
```

For the userâ€™s photo field, we can use GitHub avatar URLs, which have the GitHub user ID at the end, like:

```
https://avatars.githubusercontent.com/u/1615
```

And we can get the GitHub user ID number from the second part of the `authId`, after the `|` character (for example `github|1615`).

Now we can query for all `User` fields:

![me query will all fields selected](img/me-with-all-fields.png)

### Linking users to reviews

> If youâ€™re jumping in here, `git checkout 9_0.2.0` (tag [9_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/9_0.2.0), or compare [9...10](https://github.com/GraphQLGuide/guide-api/compare/9_0.2.0...10_0.2.0))

Another thing we can add now that we have a users collection is associate users with reviews. We want our client to be able to show the userâ€™s name and photo next to reviews, so we can update our `Review` type with an `author` field that resolves to a `User`:

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/compare/9_0.2.0...10_0.2.0)

```gql
type Review {
  id: ID!
  author: User!
  text: String!
  stars: Int
  fullReview: String!
  createdAt: Date!
  updatedAt: Date!
}
```

When we create the review, we need to save the authorâ€™s ID. The author is the currently logged-in user, which is stored at `context.user`. Inside data sources, the context is available at `this.context`. So we can save `this.context.user._id` to an `authorId` field of the review document:

[`src/data-sources/Reviews.js`](https://github.com/GraphQLGuide/guide-api/compare/9_0.2.0...10_0.2.0)

```js
export default class Reviews extends MongoDataSource {
  ...

  create(review) {
    review.authorId = this.context.user._id
    review.updatedAt = new Date()
    this.collection.insertOne(review)
    return review
  }
}
```

Now our new `Review.author` resolver can use this `authorId` prop to fetch the user doc:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/9_0.2.0...10_0.2.0)

```js
export default {
  Query: ...
  Review: {
    id: ...
    author: (review, _, { dataSources }) =>
      dataSources.users.findOneById(review.authorId),
    fullReview: ...
    createdAt: ...
  },
  Mutation: ...
}
```

The next task is updating our current reviews in the database to have an `authorId` field (because we made `author` non-nullable, weâ€™ll get an error without one). Using our own user ID (from a `{ me { id } }` query) in the below `ObjectId`:

```sh
$ mongo
> use guide
switched to db guide
> db.reviews.updateMany({}, {$set: {authorId: ObjectId('5cf8331934e9730c83399fd5')}})
{ "acknowledged" : true, "matchedCount" : 2, "modifiedCount" : 2 }
> exit
```

we should now be able to add `author` to our selection set for our `reviews` query:

![reviews query with author selected](img/reviews-with-author.png)

```gql
{
  reviews {
    text
    stars
    author {
      id
      firstName
      photo
    }
  }
}
```

And we should also be able to create a review and select the author, if we include our JWT in the authorization header:

![createReview mutation with author selected](img/createReview-with-author.png)

The last thing to update is `Review.fullReview`: letâ€™s change â€œSomeone on the internet gave N starsâ€ to use the authorâ€™s name. Currently we have:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/9_0.2.0...10_0.2.0)

```js
export default {
  Query: {
    reviews: (_, __, { dataSources }) => dataSources.reviews.all()
  },
  Review: {
    id: review => review._id,
    author: (review, _, { dataSources }) =>
      dataSources.users.findOneById(review.authorId),
    fullReview: review =>
      `Someone on the internet gave ${review.stars} stars, saying: "${
        review.text
      }"`,
    createdAt: review => review._id.getTimestamp()
  },
  Mutation: ...
}
```

Weâ€™d like to do:

```js
    fullReview: review =>
      `${review.author.firstName} ${review.author.lastName} gave ${
        review.stars
      } stars, saying: "${review.text}"`,
```

But trying to query `{ reviews { fullReview } }` gives the error `Cannot read property 'firstName' of undefined`, which means that `review.author` is undefined. This is because `review` is a MongoDB document and has an `authorId` property, not an `author` property. We could either call the other resolver (as we saw in [Custom scalars](#custom-scalars) with `Review.updatedAt`) or use the data source directly:

```js
export default {
  Query: ...
  Review: {
    id: review => review._id,
    author: (review, _, { dataSources }) =>
      dataSources.users.findOneById(review.authorId),
    fullReview: async (review, _, { dataSources }) => {
      const author = await dataSources.users.findOneById(review.authorId)
      return `${author.firstName} ${author.lastName} gave ${
        review.stars
      } stars, saying: "${review.text}"`
    },
    createdAt: review => review._id.getTimestamp()
  },
  Mutation: ...
}
```

```gql
{ 
  reviews { 
    fullReview 
  } 
}
```

![reviews query with author names in fullReview](img/fullReview-with-author.png)

## Authorizing

> If youâ€™re jumping in here, `git checkout 10_0.2.0` (tag [10_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/10_0.2.0), or compare [10...11](https://github.com/GraphQLGuide/guide-api/compare/10_0.2.0...11_0.2.0))

In this section weâ€™ll implement an authorization check for a field on the `User` type. Later, in the [Error checking](#error-checking) section, weâ€™ll talk about how to find the places we need to do authorization checks.

Letâ€™s first add a new `user` query for fetching a single user by id:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/10_0.2.0...11_0.2.0)

```gql
extend type Query {
  me: User
  user(id: ID!): User
}
```

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/10_0.2.0...11_0.2.0)

```js
import { ObjectId } from 'mongodb'

export default {
  Query: {
    me: (_, __, context) => context.user,
    user: (_, { id }, { dataSources }) =>
      dataSources.users.findOneById(ObjectId(id))
  },
  User: ...
  Mutation: ...
```

We have to turn the `id` string we receive as an argument into an `ObjectId` before calling `findOneById()`. The alternative would be to create an `ObjID` [custom scalar](#custom-scalars) that parsed string arguments into `ObjectId` objects, and then if we changed the argument type from `ID` to `ObjID`, then the `id` argument would be an `ObjectId` object by the time it reached our resolver, and we could call `findOneById()` directly:

```gql
extend type Query {
  me: User
  user(id: ObjID!): User
}
```

```js
    user: (_, { id }, { dataSources }) =>
      dataSources.users.findOneById(id)
```

```js
import { GraphQLScalarType } from 'graphql'
import { ObjectId } from 'mongodb'

export default {
  ObjID: new GraphQLScalarType({
    name: 'ObjID',
    description: ...
    parseValue: value => ObjectId(value),
    parseLiteral: ast => ObjectId(ast.value),
    serialize: objectId => objectId.toString()
  })
}
```

Letâ€™s try our new `user` query:

![user query](img/user-query.png)

We might now notice an issue. This query works without being logged in (i.e., including an authorization header), and it returns the userâ€™s email address. Similarly, we can query `{ reviews { author { email } } }` without being logged in. Our users would probably prefer their email addresses to not be publicly available! ðŸ˜„

There are a few possible ways to solve this issue:

1. We could remove the `email` field from the `User` type. However, it would be nice to be able to show users their own email address on their profile page.
2. We could check whether the user is fetching their own email. 

We could do the check in three places:

- **Resolver:** we just add an if statement to the beginning of a `User.email` resolver function.
- **Data source:** this doesnâ€™t have the granularity of the `User.email` resolver. If we threw an error in the data source method, the client wouldnâ€™t get any of the userâ€™s data. Doing authorization checks in data sources works well for preventing access to whole objects: for instance, if we wanted to prevent clients from fetching any user but their own. It works particularly well when there are multiple places in the schema the user can be accessed from. Instead of doing the check both in `Query.user` and `Review.author`, we can do it once in the `findOneById()` method of the `Users` data source.
- **Schema:** we can add a [custom directive](https://blog.apollographql.com/reusable-graphql-schema-directives-131fb3a177d1) like @isCurrentUser:

```gql
type User {
  id: ID!
  firstName: String!
  lastName: String!
  email: String! @isCurrentUser
  ...
}
```

(And weâ€™d make more directives for other authorization checks, like `@isLoggedIn` to deny access to a field from anonymous clients or `@isAdmin` to only allow admins to access a field.)

Wherever we do the check, when the user being requested doesnâ€™t match the logged-in user, we could either:

- Throw an error.
- Return `null`. The upside is itâ€™s easier for clients to handle than an error. (For example, if they query for 20 reviews with their authors, theyâ€™d get 20 errors to sort through.) The downside is they donâ€™t know why theyâ€™re getting a `null` responseâ€”they might think the user just doesnâ€™t have an email.
- Use a union type that combines the normal result with the error result, like:

```gql
union EmailResult = Email | Forbidden

type Email {
  address: String!
  verified: Boolean!
}

type Forbidden {
  message: String!
}

type User {
  id: ID!
  firstName: String!
  lastName: String!
  email: EmailResult!
  ...
}
```

Weâ€™ll cover [union errors](#union-errors) in the next section. 

In this case, letâ€™s do the check in a resolver and throw an error. We currently donâ€™t have a resolver for `User.email`, because Apollo Server just uses the email property on the user object. It does the equivalent of this tiny resolver:

```js
{
  User: {
    email: user => user.email
    ...
  }
}
```

When we provide our own resolver, Apollo Server will call our resolver instead of automatically returning `user.email`. Hereâ€™s what our resolver looks like:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/10_0.2.0...11_0.2.0)

```js
import { ForbiddenError } from 'apollo-server'

export default {
  Query: {
    me: (_, __, context) => context.user,
    user: (_, { id }, { dataSources }) =>
      dataSources.users.findOneById(ObjectId(id))
  },
  User: {
    id: ({ _id }) => _id,
    email(user, _, { user: currentUser }) {
      if (!currentUser || !user._id.equals(currentUser._id)) {
        throw new ForbiddenError(`cannot access othersâ€™ emails`)
      }

      return user.email
    },
    ...
  },
  Mutation: ...
}
```

Weâ€™d have a naming conflict if we destructured `user` from context, so we assign to a new variable name `currentUser`. First we test whether thereâ€™s any user at all, and then we test whether itâ€™s the same user. In the next section weâ€™ll see what the error looks like to the client! ðŸ‘€

## Errors

* [Nullability](11.md#nullability)
* [Union errors](11.md#union-errors)
* [formatError](11.md#formaterror)
  * [Logging errors](11.md#logging-errors)
  * [Masking errors](11.md#masking-errors)
* [Error checking](11.md#error-checking)
* [Custom errors](11.md#custom-errors)

In [Nullability](11.md#nullability), weâ€™ll see what a thrown error looks like to the client, and weâ€™ll look at how data in the response changes based on whether fields are nullable. In [Union errors](11.md#union-errors) weâ€™ll use the union type to return errors instead of throwing them. In [formatError](11.md#formaterror) we log and mask errors, in [Error checking](11.md#error-checking) we go through all the other errors we might want to check for or handle, and in [Custom errors](11.md#custom-errors) we create our own type of Apollo error.

### Nullability

> If youâ€™re jumping in here, `git checkout 11_0.2.0` (tag [11_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/11_0.2.0), or compare [11...12](https://github.com/GraphQLGuide/guide-api/compare/11_0.2.0...12_0.2.0))

In the last section, we throw an error when the client requests an email address and theyâ€™re either not logged in or itâ€™s not their email. Letâ€™s see what that error looks like by making a `user` query without an authorization header:

![user query with null result and error](img/user-null-forbidden.png)

```gql
{
  user(id: "[id of a user in our database]") {
    id
    firstName
    lastName
    email
  }
}
```

We get an errors array with one item (an object with fields `message`, `locations`, `path`, and `extensions`) and `null` data:

```json
{
  "errors": [
    {
      "message": "cannot access othersâ€™ emails",
      "locations": [
        {
          "line": 6,
          "column": 5
        }
      ],
      "path": [
        "user",
        "email"
      ],
      "extensions": {
        "code": "FORBIDDEN",
        "exception": {
          "stacktrace": [
            "ForbiddenError: cannot access othersâ€™ emails",
            ...
          ]
        }
      }
    }
  ],
  "data": {
    "user": null
  }
}
```

- The `message` matches the string we created our error with:

```js
  throw new ForbiddenError(`cannot access othersâ€™ emails`)
```

- The `path` says the error occurred in the `email` field of the `user` query, and `locations` gives the line and column number of the `email` field in the clientâ€™s query document. 
- `extensions.code` is set to `FORBIDDEN` by the `ForbiddenError()` weâ€™re using. If we use a plain `Error` (`throw new Error("cannot access othersâ€™ emails")`), then `extensions.code` would be `INTERNAL_SERVER_ERROR`.
- The stack trace is included unless `NODE_ENV` is set to `'production'`.

It would be nice if the server returned the rest of the user data we requested (`id`, `firstName`, and `lastName`) instead of just `null`. The reason it doesnâ€™t is `User.email` is non-nullable (`String!`), so a *null cascade* occurs: without an email value, the server isn't able to return a whole valid `User` type, so it returns `null` for the whole `Query.user` field. If we make it nullable by removing the `!`, throwing an error from the `User.email` resolver will return `null` just for the `email` fieldâ€”the server will still return the rest of the `User` fields:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/11_0.2.0...12_0.2.0)

```gql
type User {
  id: ID!
  firstName: String!
  lastName: String!
  username: String!
  email: String
  ...
}
```

![user query with a null email response](img/user-null-email.png)

ðŸ’ƒ This is a great improvement, especially since in the non-nullable case, a thrown error results in not just `null` for the user, but anything at a higher level as well! For example, hereâ€™s a `reviews` query requesting a non-nullable `email`:

```gql
{
  reviews {
    text
    stars
    author {
      email
    }
  }
}
```

![reviews query with null response](img/null-reviews-with-email.png)

Apollo server tries to return null for email, but itâ€™s non-nullable, so then it tries to return null for `Review.author`, but itâ€™s non-nullable, so then it tries to return `null` for the review, but the review is non-nullable and the list of reviews is non-nullable so we donâ€™t even end up with `"data": {"reviews": null}`â€”we just get `"data": null`!

So when we throw an error for a certain field but still want the client to get the rest of the data, we want to remember to make that field nullable. âŒâ—

### Union errors

> If youâ€™re jumping in here, `git checkout 12_0.2.0` (tag [12_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/12_0.2.0), or compare [12...13](https://github.com/GraphQLGuide/guide-api/compare/12_0.2.0...13_0.2.0))

As mentioned in the [Authorizing](#authorizing) section, an alternative to throwing an error is returning `null`. The downside is the client canâ€™t determine whether the server is returning `null` because thereâ€™s no data or because the client doesnâ€™t have access to it. It might be helpful to know they donâ€™t have access so that they can prompt the user to log in. 

When a fieldâ€™s type is an object type, an alternative to returning `null` from the resolver is returning an error object. We can do this by changing the type to a union. Instead of:

```gql
type Query {
  item(id: Int!): Item
}

type Item {
  id: Int
  name: String
}
```

we can do:

```gql
type Query {
  item (id: Int!): ItemResult
}

type Item {
  id: Int
  name: String
}

type ItemError {
  reason: String
}

union ItemResult = Item | ItemError
```

Now the `item` query resolver is able to return either an `Item` or an `ItemError`. This query:

```gql
{
  item(id: 1) {
    __typename
    ... on Item {
      name
    }
    ... on ItemError {
      reason
    }
  }
}
```

can return either of these two JSON responses:

```json
{
  "data": {
    "item": {
      "__typename": "Item",
      "id": 1,
      "name": "GraphQL hacky sack"
    }
  }
}
```

```json
{
  "data": {
    "item": {
      "__typename": "ItemError",
      "reason": "This item has been discontinued."
    }
  }
}
```

Why do this? It can be easier for the client to handle the errors if theyâ€™re inline in the `"data"` attribute of the JSON rather than the `"errors"` attribute. For example, imagine a `searchUsers` query that returned a long list of users. If we wanted the client to be able to show some information about deleted or suspended users, and we threw errors for each one, the client would have to go through an array of `"errors"` in the JSON response and match them up with holes in the `data.searchUsers` results. Further, they would have to be familiar with what type of errors are thrown and the format of the error data. Versus if we document in the schema the types of *expected* errors and return them from resolvers, clients know what data possibilities to expect, and they can smoothly iterate over just the `data.searchUsers` JSON array that they get. 

> *Expected* is highlighted because unexpected errors (like an unauthorized error or database failure) are usually kept as thrown errors, for the client to handle outside of its normal process of presenting expected data on the screen.

Letâ€™s implement this `searchUsers` query to see what it looks like. As usual, weâ€™ll start with the schema:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/12_0.2.0...13_0.2.0)

```gql
extend type Query {
  me: User
  user(id: ID!): User
  searchUsers(term: String!): [UserResult!]!
}

type DeletedUser {
  username: String!
  deletedAt: Date!
}

type SuspendedUser {
  username: String!
  reason: String!
  daysLeft: Int!
}

union UserResult = User | DeletedUser | SuspendedUser
```

A `UserResult` union type can be either a `User`, `DeletedUser`, or `SuspendedUser`, each of which have a `__typename` and `username` but have different other fields. Letâ€™s implement the `searchUsers` resolver next:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/12_0.2.0...13_0.2.0)

```js
export default {
  Query: {
    me: ...
    user: ...
    searchUsers: (_, { term }, { dataSources }) =>
      dataSources.users.search(term)
  },
```

We take the search `term` parameter and pass it to a `search()` method, which will talk to the database:

[`src/data-sources/Users.js`](https://github.com/GraphQLGuide/guide-api/compare/12_0.2.0...13_0.2.0)

```js
export default class Users extends MongoDataSource {
  ...
  
  search(term) {
    return this.collection.find({ $text: { $search: term } }).toArray()
  }
}
```

`$text: { $search: term }` does a [text search](https://docs.mongodb.com/manual/text-search/) of the users collection. For it to work, we need to create a `text` index, which includes all the fields we want to searchâ€”in this case, the name and username fields. In MongoDB, we usually use the [`collection.createIndex()`](http://mongodb.github.io/node-mongodb-native/3.1/api/Collection.html#createIndex) method, which checks if the index already exists, and creates it if not. It would be nice to put the command in the same file as our `search()` method so that itâ€™s easy to see which fields are being searched. One method we know will get called is the constructor, so we can put it there:

```js
export default class Users extends MongoDataSource {
  constructor(collection) {
    super(collection)

    this.collection.createIndex({
      firstName: 'text',
      lastName: 'text',
      username: 'text'
    })
  }

  ...
}
```

Weâ€™re currently instantiating this data source with:

```
new Users(db.collection('users'))
```

so in order to maintain that functionality, we need to take that object argument `collection` and pass it to `super()`.

> A new `Users` object is created for every request, which is far more often than we need to be calling `createIndex()`â€”once at server startup would be sufficientâ€”but the performance impact is miniscule, so we neednâ€™t worry about it until weâ€™re at Google scale ðŸ˜„.

Now our `search()` method returns a list of users, but theyâ€™re all normal usersâ€”we donâ€™t have any suspended or deleted users yet. Letâ€™s create three users in our database, all with the same first name so that they come up in a single search:

> If you generated your own secret key, use that. Itâ€™s located in your `.env` file.

```gql
mutation {
  createUser(
    user: {
      firstName: "John"
      lastName: "Resig"
      username: "jeresig"
      email: "john@graphql.guide"
      authId: "github|1615"
    }
    secretKey: "9e769699fae6f594beafb46e9078c2"
  ) {
    firstName
    lastName
  }
}
```

```gql
mutation {
  createUser(
    user: {
      firstName: "John"
      lastName: "Smith"
      username: "jsmith"
      email: "jsmith@example.com"
      authId: "github|1"
    }
    secretKey: "9e769699fae6f594beafb46e9078c2"
  ) {
    firstName
    lastName
  }
}
```

```gql
mutation {
  createUser(
    user: {
      firstName: "John"
      lastName: "Rest"
      username: "rest4eva"
      email: "rest4eva@example.com"
      authId: "github|2"
    }
    secretKey: "9e769699fae6f594beafb46e9078c2"
  ) {
    firstName
    lastName
  }
}
```

Now we can use the mongo shell to mark John Rest deleted and John Smith suspended:

```sh
$ mongo
> use guide
> db.users.updateOne({ username: 'rest4eva' }, { $set: { deletedAt: new Date() } })
{ "acknowledged" : true, "matchedCount" : 1, "modifiedCount" : 1 }
> db.users.updateOne({ username: 'jsmith' }, { $set: { suspendedAt: new Date(), durationInDays: 300, reason: 'Terms of Service violation' } })
{ "acknowledged" : true, "matchedCount" : 1, "modifiedCount" : 1 }
```

Now letâ€™s go back to our codeâ€”our resolver returns a list of users:

```js
this.collection.find({ $text: { $search: term } }).toArray()
```

But we donâ€™t want all of the returned objects to be of type `User`â€”then the client would get the user data of the deleted/suspended users and not know they were deleted/suspended. Whenever we use a union type, we need to tell Apollo which objects are of which type. For that we use a special resolver called `__resolveType`:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/12_0.2.0...13_0.2.0)

```js
export default {
  Query: {
    me: 
    user: ...
    searchUsers: (_, { term }, { dataSources }) =>
      dataSources.users.search(term)
  },
  UserResult: {
    __resolveType: result => {
      if (result.deletedAt) {
        return 'DeletedUser'
      } else if (result.suspendedAt) {
        return 'SuspendedUser'
      } else {
        return 'User'
      }
    }
  },
```

Now when we return an object from a resolver thatâ€™s supposed to return a `UserResult`, Apollo gives that object to `UserResult.__resolveType()`, which returns the type of the object. So now the server canâ€™t return the `firstName` of a deleted user, because itâ€™s not a field of `DeletedUser` in the schema.

The last piece we need to add is `SuspendedUser.daysLeft`, which isnâ€™t stored in the database (we only store `suspendedAt` and `durationInDays` in the database). So we create a resolver for it:

```js
import { addDays, differenceInDays } from 'date-fns'

export default {
  Query: ...
  UserResult: ...
  SuspendedUser: {
    daysLeft: user => {
      const end = addDays(user.suspendedAt, user.durationInDays)
      return differenceInDays(end, new Date())
    }
  },
```

[`addDays`](https://date-fns.org/v1.30.1/docs/addDays) returns a date, and [`differenceInDays`](https://date-fns.org/v1.30.1/docs/differenceInDays) returns an integer. Now we can make our query:

```gql
{
  searchUsers(term: "john") {
    __typename
    ... on User {
      username
      firstName
      lastName
      photo
    }
    ... on DeletedUser {
      username
      deletedAt
    }
    ... on SuspendedUser {
      username
      reason
      daysLeft
    }
  }
}
```

![searchUsers query with 3 results](img/searchUsers.png)

> Even though `username` is common to all possible types, with unions, the only field we can select outside of an [inline fragment](2.md#fragments) is the meta field `__typename`.

Now the client can iterate over `data.searchUsers` and check the `__typename`, and if itâ€™s a `DeletedUser` or `SuspendedUser`, display that user differently.

### formatError

Thereâ€™s an Apollo Server option called [`formatError`](https://www.apollographql.com/docs/apollo-server/features/errors/#masking-and-logging-errors) that allows us to log and modify errors. In this section weâ€™ll see a couple situations in which we might use it.

#### Logging errors

Background: [Json Web Tokens](bg.md#tokens-vs-sessions)

> If youâ€™re jumping in here, `git checkout 13_0.2.0` (tag [13_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/13_0.2.0), or compare [13...14](https://github.com/GraphQLGuide/guide-api/compare/13_0.2.0...14_0.2.0))

Usually when there are server errors, we see them in the `errors` field of the JSON response. In Playground, itâ€™s usually easy to see all the error information, including the stack trace, but when itâ€™s not easy to see the error on the client, it would be nice to be able to see the error in the server output on the command line. And in production we need some way of tracking the errors our users trigger.

There is one case in which Playground doesnâ€™t conveniently show us the server error: when it receives an error from an introspection query. Playground periodically sends an introspection query to our server to get an up-to-date schema to back its query checking and schema tab. When we set an HTTP header, Playground uses it for the introspection query as well. So when we set an invalid authorization header, the server returns an error for the introspection query, but Playground might not show it to usâ€”it might just say â€œResponse not successfulâ€:

```gql
{
  me {
    email
  }
}
```

```json
{
  "authorization": "it's me, john!"
}
```

![Query with bad authorization header giving an error](img/status-code-500.png)

If we go into the devtools Network tab and select a `localhost` request, we can see the GraphQL `errors` field, but itâ€™s hard to read the stack traceâ€”we either have to scroll and visually parse the newlines or paste it into a JSON formatter (we recommend [jq](https://stedolan.github.io/jq/): `brew install jq`, copy, `pbpaste | jq .`).

![localhost internal server error](img/localhost-internal-server-error.png)

Since that takes a few steps, letâ€™s instead log the error using [`formatError`](https://www.apollographql.com/docs/apollo-server/features/errors/#masking-and-logging-errors):

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/13_0.2.0...14_0.2.0)

```js
import formatError from './formatError'

const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context,
  formatError
})
```

[`src/formatError.js`](https://github.com/GraphQLGuide/guide-api/compare/13_0.2.0...14_0.2.0)

```js
export default error => {
  console.log(error)
  return error
}
```

Now the error is logged to the terminal:

```
{ [JsonWebTokenError: Context creation failed: jwt malformed]
  message: 'Context creation failed: jwt malformed',
  locations: undefined,
  path: undefined,
  extensions: 
   { code: 'INTERNAL_SERVER_ERROR',
     exception: { stacktrace: [Array] } } }
```

But we donâ€™t see the stack trace, so letâ€™s log that as well, if the error has one:

```js
import get from 'lodash/get'

export default error => {
  console.log(error)
  console.log(get(error, 'extensions.exception.stacktrace'))
  return error
}
```

And now we also get:

```
[ 'JsonWebTokenError: Context creation failed: jwt malformed',
  '    at module.exports (/guide-api/node_modules/jsonwebtoken/verify.js:63:17)',
  '    at internal/util.js:230:26',
  '    at verify (/guide-api/src/util/auth.js:24:31)',
  '    at ApolloServer._default [as context] (/guide-api/src/context.js:8:24)',
  '    at ApolloServer.<anonymous> (/guide-api/node_modules/apollo-server-core/src/ApolloServer.ts:535:24)',
  '    at Generator.next (<anonymous>)',
  '    at /guide-api/node_modules/apollo-server-core/dist/ApolloServer.js:7:71',
  '    at new Promise (<anonymous>)',
  '    at __awaiter (/guide-api/node_modules/apollo-server-core/dist/ApolloServer.js:3:12)',
  '    at ApolloServer.graphQLServerOptions (/guide-api/node_modules/apollo-server-core/dist/ApolloServer.js:316:16)' ]
```

And we can further debug! The error starts in `node_modules/` (`/guide-api/node_modules/jsonwebtoken/verify.js:63:17`), so letâ€™s look for the first lines that are inside our code (`src/`):

```
  '    at verify (/guide-api/src/util/auth.js:24:31)',
  '    at ApolloServer._default [as context] (/guide-api/src/context.js:8:24)',
```

Now letâ€™s look at `src/context.js`:

```js
import { getAuthIdFromJWT } from './util/auth'
import { db } from './db'

export default async ({ req }) => {
  const context = {}

  const jwt = req.headers.authorization
  const authId = await getAuthIdFromJWT(jwt)
  const user = await db.collection('users').findOne({ authId })
  if (user) {
    context.user = user
  }

  return context
}
```

Line 8 is `const authId = await getAuthIdFromJWT(jwt)`. So the error message `"jwt malformed"` means the authorization header is not formatted as a valid JWT.

We achieved our goal of using `formatError` to log the error so that we could debug it. We canâ€™t prevent clients from sending bad authorization headers, but we can improve the errors we throw. The two most common errors thrown during JWT parsing are `jwt malformed` and `jwt expired`, so letâ€™s cover those:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/compare/13_0.2.0...14_0.2.0)

```js
import { AuthenticationError } from 'apollo-server'

export default async ({ req }) => {
  const context = {}

  const jwt = req.headers.authorization
  let authId

  if (jwt) {
    try {
      authId = await getAuthIdFromJWT(jwt)
    } catch (e) {
      let message
      if (e.message.includes('jwt expired')) {
        message = 'jwt expired'
      } else {
        message = 'malformed jwt in authorization header'
      }
      throw new AuthenticationError(message)
    }

    const user = await db.collection('users').findOne({ authId })
    context.user = user
  }

  return context
}
```

We catch errors from `getAuthIdFromJWT()`, and use a different error message depending on the kind of error. Then we use Apolloâ€™s `AuthenticationError` error type, which adds an `extensions.code` of `"UNAUTHENTICATED"` to the error. The other errors that might occur are from the database (during the `findOne()`)â€”weâ€™ll cover these in the [next section](#masking-errors). Letâ€™s also throw an error when there is no matching user in the database:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/compare/13_0.2.0...14_0.2.0)

```js
export default async ({ req }) => {
  const context = {}

  const jwt = req.headers.authorization
  let authId

  if (jwt) {
    ...

    const user = await db.collection('users').findOne({ authId })
    if (user) {
      context.user = user
    } else {
      throw new AuthenticationError('no such user')
    }
  }

  return context
}
```

Now letâ€™s repeat our bad-header query and see what new error we get in the console:

```
{ [AuthenticationError: Context creation failed: malformed jwt in authorization header]
  message: 'Context creation failed: malformed jwt in authorization header',
  locations: undefined,
  path: undefined,
  extensions: { code: 'UNAUTHENTICATED', exception: { stacktrace: [Array] } } }
[ 'AuthenticationError: Context creation failed: malformed jwt in authorization header',
  '    at ApolloServer._default [as context] (/Users/me/gh/guide-api/src/context.js:21:13)',
  '    at <anonymous>',
  '    at runMicrotasksCallback (internal/process/next_tick.js:121:5)',
  '    at _combinedTickCallback (internal/process/next_tick.js:131:7)',
  '    at process._tickCallback (internal/process/next_tick.js:180:9)' ]
```

We now see the `UNAUTHENTICATED` error code and the more detailed error message. Our piece of the messageâ€”`malformed jwt in authorization header`â€”is preceded by `Context creation failed:`, which is added by Apollo for any errors that occur in the context function, and `AuthenticationError:`, which is taken from the name of the error object.

#### Masking errors

> If youâ€™re jumping in here, `git checkout 14_0.2.0` (tag [14_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/14_0.2.0), or compare [14...15](https://github.com/GraphQLGuide/guide-api/compare/14_0.2.0...15_0.2.0))

`formatError()` isnâ€™t just for loggingâ€”as the name indicates, we can change the error. The most common change is masking an error we donâ€™t want the client to see. 

You may have noticed that we return the error in the last line of the `formatError()` function:

[`src/formatError.js`](https://github.com/GraphQLGuide/guide-api/compare/14_0.2.0...15_0.2.0)

```js
export default error => {
  console.log(error)
  console.log(get(error, 'extensions.exception.stacktrace'))
  return error
}
```

When an error is thrown in our code, Apollo catches it and gives it to `formatError()`, which returns an error object, which Apollo serializes into JSON and sends in the `errors` attribute to the client. Inside `formatError()`, we can modify the error objectâ€”by editing, adding, or removing propertiesâ€”or return a new error. 

A common category of error to mask is database errorsâ€”we might want to hide the original error message for security reasons or to avoid confusing non-technical users with messages they donâ€™t understand. Letâ€™s see, for example, what errors happen when the server canâ€™t reach the database. We can stop the database with this command:

```sh
$ brew services stop mongodb-community
```

If we wait 30 seconds and then make a request, we get a `MongoNetworkError`:

![MongoNetworkError: failed to reconnect after 30 attempts with interval 1000 ms](img/mongo-network-error.png)

And if we keep making requests, we start getting `"MongoError: Topology was destroyed"`:

![MongoError: Topology was destroyed](img/topology-was-destroyed.png)

Letâ€™s mask both of those with a new error:

```js
export default error => {
  console.log(error)
  console.log(get(error, 'extensions.exception.stacktrace'))

  const name = get(error, 'extensions.exception.name') || ''
  if (name.startsWith('Mongo')) {
    return new Error('Internal server error')
  } else {
    return error
  }
}
```

When we edit our code, the server fails to restart because it canâ€™t connect to the database. So in order to test, we can restart Mongo:

```sh
$ brew services start mongodb-community
```

And then restart the server, and then stop Mongo:

```sh
$ brew services stop mongodb-community
```

Now we get our masked error instead of either of the Mongo errors:

![Error with message: Internal server error](img/internal-server-error.png)

One last note on `formatError`â€”in production, weâ€™ll usually want to send our errors to an error tracking or logging service instead of logging them to the server console:

```js
const inProduction = process.env.NODE_ENV === 'production'

export default error => {
  if (inProduction) {
    // send error to tracking service
  } else {
    console.log(error)
    console.log(get(error, 'extensions.exception.stacktrace'))
  }

  ...
}
```

### Error checking

> If youâ€™re jumping in here, `git checkout 15_0.2.0` (tag [15_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/15_0.2.0), or compare [15...16](https://github.com/GraphQLGuide/guide-api/compare/15_0.2.0...16_0.2.0))

So far weâ€™ve dealt with the `User.email` authorization error, users who have been deleted or suspended, authentication errors, and MongoDB errors. Letâ€™s go through our entire app and think about all the possible errors we want to handle or throw:

- **Network:** If our node server is cut off from the internet, or if thereâ€™s a DNS issue, the client wonâ€™t be able to connect to our server, and will see an error that will look different depending on their browser or platform.
- Servers
  - **Node:** If our Node GraphQL application server isnâ€™t running, then the client wonâ€™t be able to connect, and will see the same error as when thereâ€™s a network failure.
  - **MongoDB:** We mask errors with our MongoDB server, including inability to connect, in `formatError()`
- **Request:** If the network request isnâ€™t a valid GraphQL HTTP request, then the error will be handled before it reaches our codeâ€”either by our serverâ€™s operating system, Node, or Apollo Server.
- **Context:** Assuming the request is a valid GraphQL request (including valid against our schema), the server starts by setting the context for resolvers. This process often involves looking at request headers. We covered errors that might occur while creating context in the [Logging errors](#logging-errors) section.
- Resolvers:
  - **Arguments:** Apollo validates the argumentsâ€™ data types, but we often want to do further validation on the argument values.
  - **Execution:** We want to handle any possible errors that might occur in the running of our resolver codeâ€”things like invalid JWT decoding, dividing by zero, or trying to access a 3rd party service thatâ€™s offline.
  - **Authorization:** If thereâ€™s data or functions that we donâ€™t want certain people to access or trigger, we need to avoid returning the data / running the functions.

In this section weâ€™ll go through our resolvers. Letâ€™s start with authorization. For data access, letâ€™s look at our main data types:

```gql
type Review {
  id: ID!
  author: User!
  text: String!
  stars: Int
  fullReview: String!
  createdAt: Date!
  updatedAt: Date!
}

type User {
  id: ID!
  firstName: String!
  lastName: String!
  username: String!
  email: String
  photo: String!
  createdAt: Date!
  updatedAt: Date!
}
```

Depending on our app, we might consider `createdAt` and `updatedAt` to be sensitive, but for us, the only field we donâ€™t want to be public is `email`, which we already [have a check for](#authorizing). If we had an app for which an entire data type was restricted, then in order to verify it was restricted properly, we would need to search for that type everywhere it was referenced in the schema and make sure those queries, mutations, or other fields were restricted. For instance, if we only wanted logged-in users to be able to view user data, then weâ€™d look for `User` in the above and below parts of the schema:

```gql
type Query {
  hello(date: Date): String!
  isoString(date: Date!): String!
  reviews: [Review!]!
  me: User
  user(id: ID!): User
  searchUsers(term: String!): [UserResult!]!
}

type Mutation {
  createReview(review: CreateReviewInput!): Review
  createUser(user: CreateUserInput!, secretKey: String!): User
}
```

We would need to restrict `Review.author`, `Query.user`, and `Query.searchUsers`, and make sure that:

- `Query.me`, which returns a `User`, only returns the current user.
- `Mutation.createUser`, which also returns a `User`, doesnâ€™t return any user but the one just created by that client.

Thatâ€™s all for authorization on data access. The other part is authorization on running functionsâ€”specifically, functions that change things. While itâ€™s possible for a `Query` resolver function to change something, itâ€™s better to make those functions `Mutations`. Letâ€™s assume weâ€™ve defined our `Query` and `Mutation` types properly, and havenâ€™t accidentally modified data in our `Query` resolvers. That means we only need to check our mutations, `createReview` and `createUser`. `createUser` we already [protected with a `secretKey`](#protecting-with-secret-key). `createReview` can currently be run by anyone, but we want it to be run only by logged-in users. Letâ€™s fix that:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/15_0.2.0...16_0.2.0)

```js
import { ForbiddenError } from 'apollo-server'

export default {
  Query: ...
  Review: ...
  Mutation: {
    createReview: (_, { review }, { dataSources, user }) => {
      if (!user) {
        throw new ForbiddenError('must be logged in')
      }

      dataSources.reviews.create(review)
    }
  }
}
```

Now when we try the mutation without an authorization header, we get an error with the message `"must be logged in"` and code `"FORBIDDEN"`:

```gql
mutation {
  createReview(review: { text: "Grrrreeeeaat!", stars: 5 }) {
    id
    text
    author {
      firstName
    }
  }
}
```

![createReview mutation with error: "must be logged in"](img/must-be-logged-in.png)

That concludes authorization in resolvers. Next letâ€™s check arguments. First we look back at the schema and think about which Query arguments need further validation:

```gql
type Query {
  hello(date: Date): String!
  isoString(date: Date!): String!
  reviews: [Review!]!
  me: User
  user(id: ID!): User
  searchUsers(term: String!): [UserResult!]!
}
```

We donâ€™t need to do anything with the first two queriesâ€”our custom scalar checks validity, and any valid date is fine for those queries. The third and fourth donâ€™t have arguments. The last two do. Hereâ€™s what they currently look like:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/15_0.2.0...16_0.2.0)

```js
export default {
  Query: {
    me: ...
    user: (_, { id }, { dataSources }) =>
      dataSources.users.findOneById(ObjectId(id)),
    searchUsers: (_, { term }, { dataSources }) =>
      dataSources.users.search(term)
  },
  ...
}
```

Searching with an empty string gives back an empty list, which is fine. We also donâ€™t need to worry about a NoSQL injection attack with a text search. So letâ€™s leave the searching to handle any string, blank or malicious, and move on to `Query.user`. Any string validates as an `ID`, so letâ€™s see what happens when we try to get a user with an ID of `'_why'`:

```gql
{
  user(id: "_why") {
     firstName
  }
}
```

![user query ](img/invalid-objectid-error.png)

We get an error with the message `"Argument passed in must be a single String of 12 bytes or a string of 24 hex characters"` and code `"INTERNAL_SERVER_ERROR"`. We can tell from the stack trace that itâ€™s coming from our `ObjectId(id)` call, but it may very well be confusing to the client. Letâ€™s help the client out by giving them a better error message:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/15_0.2.0...16_0.2.0)

```js
import { UserInputError } from 'apollo-server'

const OBJECT_ID_ERROR =
  'Argument passed in must be a single String of 12 bytes or a string of 24 hex characters'

export default {
  Query: {
    me: ...
    user: (_, { id }, { dataSources }) => {
      try {
        return dataSources.users.findOneById(ObjectId(id))
      } catch (error) {
        if (error.message === OBJECT_ID_ERROR) {
          throw new UserInputError('invalid id', {
            invalidArgs: ['id']
          })
        } else {
          throw error
        }
      }
    },
```

We use another built-in error type called `UserInputError`, which sets `extensions.code` to `BAD_USER_INPUT` and lists the invalid arguments in `extensions.invalidArgs`:

![user query with error "invalid id"](img/invalid-id.png)

Weâ€™re done checking Query arguments. Now letâ€™s do Mutation arguments:

```gql
type Mutation {
  createUser(user: CreateUserInput!, secretKey: String!): User
  createReview(review: CreateReviewInput!): Review
}
```

Because of `secretKey`, we can trust that our own code is the only one calling `createUser`. Letâ€™s also trust that our code sends good data for the `user` argument, so we can leave that resolver alone. Lastly is `createReview` with `CreateReviewInput`:

```gql
input CreateReviewInput {
  text: String!
  stars: Int
}
```

Inside our resolver, we can trust that `review.text` is a string and that `review.stars` is either undefined or an integer. We need to further check that `review.text` is a valid length (letâ€™s say at least two characters ðŸ˜„) and that `review.stars` is between 0 and 5.

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/15_0.2.0...16_0.2.0)

```js
import { ForbiddenError, UserInputError } from 'apollo-server'

const MIN_REVIEW_LENGTH = 2
const VALID_STARS = [0, 1, 2, 3, 4, 5]

export default {
  Query: ...
  Review: ...
  Mutation: {
    createReview: (_, { review }, { dataSources, user }) => {
      if (!user) {
        throw new ForbiddenError('must be logged in')
      }

      if (review.text.length < MIN_REVIEW_LENGTH) {
        throw new UserInputError(
          `text must be at least ${MIN_REVIEW_LENGTH} characters`,
          { invalidArgs: ['text'] }
        )
      }

      if (review.stars && !VALID_STARS.includes(review.stars)) {
        throw new UserInputError(`stars must be between 0 and 5`, {
          invalidArgs: ['stars']
        })
      }

      return dataSources.reviews.create(review)
    }
  }
}
```

Since `CreateReviewInput!` is non-null, we donâ€™t have to check that `review` is defined. Similarly, we donâ€™t have to check that `review.text` is defined. Letâ€™s check both errors:

```gql
mutation {
  createReview(review: { text: "A", stars: 6 }) {
    id
    text
  }
}
```

![error message when text is too short](img/review-text-too-short.png)

![error message when stars is 6](img/stars-between-0-and-5.png)

Thatâ€™s all of our input validation, and the last of our error checking! âœ…

### Custom errors

> If youâ€™re jumping in here, `git checkout 16_0.2.0` (tag [16_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/16_0.2.0), or compare [16...17](https://github.com/GraphQLGuide/guide-api/compare/16_0.2.0...17_0.2.0))

In addition to the built-in `UserInputError`, `ForbiddenError`, and `AuthenticationError` that weâ€™ve used, thereâ€™s also their superclass, `ApolloError`, which we can use directly to add arbitrary error data or extend to make our own error classes. Weâ€™ll do both in this section.

In the last section, when checking the `review` argument to `createReview`, we threw an error for either `review.text` or `review.stars`. If both were incorrect, the client would just get the first error, for `review.text`. Once the client fixed that and tried again, they would then get the `review.stars` error. It would be helpful to the client if we can give both errors at the same time. 

We could do `{ invalidArgs: ['text', 'stars'] }` and combine the two error messages into one message, but it would be better to associate each error message with the corresponding argumentâ€”that way, for instance, the client can display individual error messages next to each invalid form field. It turns out that `UserInputError` takes any object as its second argument (and adds it to the response JSONâ€™s `extensions.exception`). Letâ€™s keep the recommended `invalidArgs` attribute, but change the value from an array to an object:

```js
{
  invalidArgs: {
    text: 'must be at least 2 characters',
    stars: 'must be between 0 and 5'
  }
}
```

To get this, we update the code to:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/16_0.2.0...17_0.2.0)

```js
import { isEmpty } from 'lodash'

export default {
  Query: ...
  Review: ...
  Mutation: {
    createReview: (_, { review }, { dataSources, user }) => {
      if (!user) {
        throw new ForbiddenError('must be logged in')
      }

      const errors = {}

      if (review.text.length < MIN_REVIEW_LENGTH) {
        errors.text = `must be at least ${MIN_REVIEW_LENGTH} characters`
      }

      if (review.stars && !VALID_STARS.includes(review.stars)) {
        errors.stars = `must be between 0 and 5`
      }

      if (!isEmpty(errors)) {
        throw new UserInputError('invalid review', { invalidArgs: errors })
      }

      return dataSources.reviews.create(review)
    }
  }
}
```

Now we see both errors together!

```gql
mutation {
  createReview(review: { text: "A", stars: 6 }) {
    id
    text
  }
}
```

![createReview query with both errors in extensions.exception.invalidArgs](img/multiple-invalid-createReview-args.png)

We use `UserInputError` in one other place. Letâ€™s update the `invalidArgs` format there as well to be consistent so that the client can easily programmatically work with `extensions.exception.invalidArgs`:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/16_0.2.0...17_0.2.0)

```js
if (error.message === OBJECT_ID_ERROR) {
  throw new UserInputError('invalid id', {
    invalidArgs: { id: 'not a valid Mongo ObjectId' }
  })
}
```

Weâ€™ll come back to `UserInputError` in a bit. For now letâ€™s consider this from `src/formatError.js`:

```js
return new Error('Internal server error')
```

The resulting response is bare, without even a stack trace:

![createReview response with just a message](img/createReview-internal-server-error.png)

Apollo only adds the `extensions` field (including a stack trace in development) for `ApolloError` and its subclasses. So letâ€™s use that:

[`src/formatError.js`](https://github.com/GraphQLGuide/guide-api/compare/16_0.2.0...17_0.2.0)

```js
import { ApolloError } from 'apollo-server'

...

  if (name.startsWith('Mongo')) {
    return new ApolloError(
      `Weâ€™re sorryâ€”an error occurred. Weâ€™ve been notified and will look into it.`,
      'INTERNAL_SERVER_ERROR'
    )
  } else {
    return error
  }
```

![error with code INTERNAL_SERVER_ERROR](img/INTERNAL_SERVER_ERROR.png)

[`ApolloError`](https://www.apollographql.com/docs/apollo-server/features/errors/#other-errors) takes three arguments: the error message, a code, and additional properties to add to `extensions.exception`. Weâ€™re using the first two. Having a code makes it easy for the client to handle all internal server errors similarly. Having a user-friendly message means that the client can show it directly to the user.

In case we want to throw an internal server error elsewhere in the future, letâ€™s make our own `InternalServerError` class:

[`src/util/errors.js`](https://github.com/GraphQLGuide/guide-api/blob/17_0.2.0/src/util/errors.js)

```js
import { ApolloError } from 'apollo-server'

export class InternalServerError extends ApolloError {
  constructor() {
    super(
      `Weâ€™re sorryâ€”an error occurred. Weâ€™ve been notified and will look into it.`,
      'INTERNAL_SERVER_ERROR'
    )

    Object.defineProperty(this, 'name', { value: 'InternalServerError' })
  }
}
```

`super()` gets the same arguments that the `ApolloError()` constructor got. The last thing is setting the objectâ€™s name, which is used at the beginning of the stack trace. Now our use of the error can be simplified:

[`src/formatError.js`](https://github.com/GraphQLGuide/guide-api/compare/16_0.2.0...17_0.2.0)

```js
import { InternalServerError } from './util/errors'

...

  if (name.startsWith('Mongo')) {
    return new InternalServerError()
  } else {
    return error
  }
}
```

Letâ€™s also make a custom input error. Currently weâ€™re using `UserInputError` like this:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/16_0.2.0...17_0.2.0)

```js
import { UserInputError } from 'apollo-server'

...

  if (!isEmpty(errors)) {
    throw new UserInputError('invalid review', { invalidArgs: errors })
  }
```

It would be simpler if we had an `InputError` class that we could use like this:

```js
import { InputError } from '../util/errors'

...

  if (!isEmpty(errors)) {
    throw new InputError({ review: errors })
  }
```

And then `InputError` could take care of the error message for us. We could also use it in our user resolver:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/16_0.2.0...17_0.2.0)

```js
import { InputError } from '../util/errors'

export default {
  Query: {
    me: ...
    user: (_, { id }, { dataSources }) => {
      try {
        return dataSources.users.findOneById(ObjectId(id))
      } catch (error) {
        if (error.message === OBJECT_ID_ERROR) {
          throw new InputError({ id: 'not a valid Mongo ObjectId' })
        } else {
          throw error
        }
      }
    },
```

The only difference here is that our argument `id` is a scalar type, so we pass `{ id: 'not a valid Mongo ObjectId' }` to `InputError()`, versus the `review` object type argument to `createReview`, which looked like:

```js
{
  review: {
    text: 'must be at least 2 characters',
    stars: 'must be between 0 and 5'
  }
}
```

So when we implement our `InputError` class, we have to cover both scenariosâ€”scalar arguments and their messages, as well as object arguments and their invalid field messages. As before, we subclass `ApolloError`, but this time the constructor creates the error `message`:

[`src/util/errors.js`](https://github.com/GraphQLGuide/guide-api/blob/17_0.2.0/src/util/errors.js)

```js
export class InputError extends ApolloError {
  constructor(errors) {
    let messages = []

    for (const arg in errors) {
      if (typeof errors[arg] === 'string') {
        // scalar argument
        const errorReason = errors[arg]
        messages.push(`Argument ${arg} is invalid: ${errorReason}.`)
      } else {
        // object argument
        const errorObject = errors[arg]
        for (const prop in errorObject) {
          const errorReason = errorObject[prop]
          messages.push(`Argument ${arg}.${prop} is invalid: ${errorReason}.`)
        }
      }
    }

    const fullMessage = messages.join(' ')

    super(fullMessage, 'INVALID_INPUT', { invalidArgs: errors })

    Object.defineProperty(this, 'name', { value: 'InputError' })
  }
}
```

Now when we make an invalid query, we see:

- a very detailed error `message`
- our own error code `INVALID_INPUT`
- a different `invalidArgs` object, from which we can tell what argument the fields `text` and `stars` are on (`review`)
- â€œInputErrorâ€ at the beginning of the stack trace

![invalid createReview query with new InputError response](img/InputError.png)

In this section we went over:

- passing arbitrary `extensions.exception` properties as the second argument to `UserInputError()` (or the third argument of `ApolloError()`)
- using `ApolloError()` directly
- creating our own error classes: `InternalServerError` and `InputError`

## Subscriptions

* [githubStars](11.md#githubstars)
* [reviewCreated](11.md#reviewcreated)

GraphQL subscriptions, along with the rest of the spec, are transport-agnostic: that is, the two parties communicating GraphQL donâ€™t need to use a specific way of sending messages. You can even do GraphQL with your friend by passing paper notes back and forth ðŸ˜„.

The transport weâ€™ve been using (HTTP) wonâ€™t work for subscriptions because HTTP is unidirectionalâ€”only the client can initiate messages to the server, and the server only has a single opportunity to respond. We need a bidirectional transportâ€”the client needs to be able to tell the server to start and stop the subscription, and the server needs to send subscription events. The main bidirectional transport used in web programming (and most often used for GraphQL subscriptions) is WebSockets.

> In HTTP/2, the server can push resources to the client, but not messages to client code. With SSE ([Server-sent events](https://en.wikipedia.org/wiki/Server-sent_events)), the server can send messages to the client, and if we combine it with HTTP/2, we can do bidirectional communication over a single connection. However, WebSockets are more widely supported and easier to set up.

Subscriptions over WebSockets is supported by Apollo Server (at `ws://hostname/graphql`â€”`ws://localhost:4000/graphql` in development). In the next section, weâ€™ll see what that looks like with a simple example. Then in [Review updates](#review-updates) weâ€™ll code a more complex example.

### githubStars

> If youâ€™re jumping in here, `git checkout 17_0.2.0` (tag [17_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/17_0.2.0), or compare [17...18](https://github.com/GraphQLGuide/guide-api/compare/17_0.2.0...18_0.2.0))

The simplest subscription used on the Guide site is for a single integerâ€”the number of stars on the [GraphQLGuide/guide](https://github.com/GraphQLGuide/guide) repo. As always, we start with the schema:

[`src/schema/Github.graphql`](https://github.com/GraphQLGuide/guide-api/blob/18_0.2.0/src/schema/Github.graphql)

```gql
type Subscription {
  githubStars: Int!
}
```

This means that each subscription event that the server sends the client will contain a single integer and be in this format:

```json
{
  "data": {
    "githubStars": <integer>
  }
}
```

We include our new `.graphql` file by adding this to the bottom of `schema.graphql`:

[`src/schema/schema.graphql`](https://github.com/GraphQLGuide/guide-api/compare/17_0.2.0...18_0.2.0)

```graphql
#import 'Github.graphql'
```

We need a publish and subscribe system to keep track of which clients to send events to. Apollo Server has an interface that all pub/sub packages implement, so whichever we use, the API will be the same. We create an instance of the `PubSub` class, use its `.asyncIterator()` method in the subscription resolver, and its `.publish()` method to send events. Letâ€™s start with the first step, using the in-memory, for-use-in-development version of `PubSub` included in Apollo Server:

[`src/util/pubsub.js`](https://github.com/GraphQLGuide/guide-api/blob/18_0.2.0/src/util/pubsub.js)

```js
import { PubSub } from 'apollo-server'

export const pubsub = new PubSub()
```

Our resolver is:

[`src/resolvers/Github.js`](https://github.com/GraphQLGuide/guide-api/blob/18_0.2.0/src/resolvers/Github.js)

```js
import { pubsub } from '../util/pubsub'

export default {
  Subscription: {
    githubStars: {
      subscribe: () => pubsub.asyncIterator('githubStars')
    }
  }
}
```

For subscriptions, instead of defining the function on `Subscription.field`, we use `Subscription.field.subscribe` and return an iterator. Weâ€™re naming the iterator `'githubStars'`, so to send events to the interator, weâ€™ll do `pubsub.publish('githubStars', { githubStars: 1337 })`. 

Next we include the resolver:

[`src/resolvers/index.js`](https://github.com/GraphQLGuide/guide-api/compare/17_0.2.0...18_0.2.0)

```js
...

import Github from './Github'

export default [resolvers, Review, User, Date, Github]
```

Now where do we call `pubsub.publish()`? We have to get the information first. Where do we get it from? GitHub, of course! The first three versions of their API were REST-based, but their v4 is a GraphQL APIâ€”letâ€™s use that. [Their docs](https://developer.github.com/v4/guides/forming-calls/#the-graphql-endpoint) say the endpoint is `https://api.github.com/graphql` and that we need to [create an access token](https://developer.github.com/v4/guides/forming-calls/#authenticating-with-graphql) to use the API. Once weâ€™ve done that, we add a new `GITHUB_TOKEN` environment variable with the token we created:

`.env`

```
SECRET_KEY=9e769699fae6f594beafb46e9078c2
GITHUB_TOKEN=...
```

Now we can use `process.env.GITHUB_TOKEN` in our auth header to the GitHub API. Letâ€™s put our code in the `data-sources/` directory. Even though it doesnâ€™t talk to our database or follow Apolloâ€™s `DataSource` API (since we donâ€™t need context, a new instance for every request, batching, or caching), it is a source of data used in our app.

[`src/data-sources/Github.js`](https://github.com/GraphQLGuide/guide-api/blob/18_0.2.0/src/data-sources/Github.js)

```js
import { GraphQLClient } from 'graphql-request'

const githubAPI = new GraphQLClient('https://api.github.com/graphql', {
  headers: {
    authorization: `bearer ${process.env.GITHUB_TOKEN}`
  }
})
```

The simplest way to make GraphQL requests is with the [`graphql-request`](https://github.com/prisma/graphql-request) npm package. Now we can call `githubAPI.request(queryString)`, and our query will be sent to GitHub with our auth header. 

To determine what our query should be, we can browse GitHubâ€™s GraphQL Explorer (an authenticated GraphiQL). A repoâ€™s star count should be included in a repositoryâ€™s information, so letâ€™s look for a root Query field for getting a repository:

![GitHubâ€™s GraphiQL with Query fields](img/github-graphql-explorer.png)

We find:

```gql
# Lookup a given repository by the owner and repository name.
repository(owner: String!, name: String!): Repository
```

Clicking on the `Repository` type gives us a long list of fields, including a `stargazers` field:

![Repository field list](img/github-stargazers.png)

And clicking on the `StargazerConnection` type gives us:

![StargazerConnection field list](img/github-stargazers-totalCount.png)

And we find that `totalCount` is the field we need. Putting all of that together gives us:

```js
const GUIDE_STARS_QUERY = `
query GuideStars {
  repository(owner: "GraphQLGuide", name: "guide") {
    stargazers {
      totalCount
    }
  }
}
`
```

We can make this query periodically to keep the count up to date. Letâ€™s create a `startPolling()` function that does that. When it gets a new number, it will call `pubsub.publish()`:

[`src/data-sources/Github.js`](https://github.com/GraphQLGuide/guide-api/blob/18_0.2.0/src/data-sources/Github.js)

```js
import { pubsub } from '../util/pubsub'

...

export default {
  async fetchStarCount() {
    const data = await githubAPI.request(GUIDE_STARS_QUERY).catch(console.log)
    return data && data.repository.stargazers.totalCount
  },

  startPolling() {
    let lastStarCount

    setInterval(async () => {
      const starCount = await this.fetchStarCount()
      const countChanged = starCount && starCount !== lastStarCount
      
      if (countChanged) {
        pubsub.publish('githubStars', { githubStars: starCount })
        lastStarCount = starCount
      }
    }, 1000)
  }
}
```

The first argument to `pubsub.publish()` is the name of the async iterator and the second argument is the event data, the format of which needs to match our Subscription field in the schema (`type Subscription { githubStars: Int! }`).

Next we need to call `startPolling()` on startup. The place where all the other data sources are included seems a fitting place:

[`src/data-sources/index.js`](https://github.com/GraphQLGuide/guide-api/compare/17_0.2.0...18_0.2.0)

```js
import Github from './Github'

Github.startPolling()
```

The last change we need to make is to our context function:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/compare/17_0.2.0...18_0.2.0)

```js
export default async ({ req }) => {
  const context = {}

  const jwt = req.headers.authorization
```

Weâ€™re getting a `req` argument and assuming that it has `headers.authorization` properties. But actually, `req` will be undefined for subscriptions. So letâ€™s guard against that:

```js
export default async ({ req }) => {
  const context = {}

  const jwt = req && req.headers.authorization
```

Now we test out our new subscription:

```gql
subscription {
  githubStars
}
```

![githubStars subscription in Playground with two events](img/githubStars-subscription.png)

When we hit the play button, it turns red, but nothing appears on the rightâ€”thatâ€™s because we havenâ€™t received an event from the server yet, because the server only publishes when the value changes. But if we star [the repo](https://github.com/GraphQLGuide/guide), weâ€™ll see an event of the form:

```json
{
  "data": {
    "githubStars": 87
  }
}
```

And when we unstar the repo, we see another event with the number one lower. Great, weâ€™ve got realtime updates! ðŸ™Œ 

Wellâ€¦ depending on your definition of realtime. Since weâ€™re polling once a second, we might lag around a second. In the next section weâ€™ll see even faster updates, where the publish happens as soon as the server receives a userâ€™s action.

Lastly, letâ€™s see what the WebSocket communication looks like. If we open devtools Network tab, hit the stop button in Playground, hit play, unstar and re-star the repo, select the `graphql` item in the list on the bottom-left, and select the Messages tab, weâ€™ll see something like:

![Network tab with a list of WebSocket messages](img/subscription-websocket-start.png)

The rows with the green up arrow are messages sent over the WebSocket to the server, and the rows with the red down arrow are messages sent from the server to the browser. When we hit the play button, Playground opens the connection to `ws://localhost:4000/graphql` and sends two messages: one with type `connection_init` and one with:

- `type: "start"`â€”Weâ€™re starting a subscription.
- `payload.query`â€”The GraphQL document containing our subscription (what we typed on the left side of the Playground).
- `id: 1`â€”We might start more subscriptions over this websocket, so we have a number to identify this one that weâ€™re starting in this message.

Then the server sends a message with type `connection_ack` (**ack**nowledging receipt of the `connection_init`), and messages like this:

![WebSocket message containing a subscription event](img/subscription-websocket-event.png)

- `type: "data"`â€”This message contains a subscription event.
- `id: 1`â€”This event corresponds to the subscription with an `id` of 1.
- `payload: {data: {githubStars: 89}}`â€”This is the subscription event, which Playground displays in the right-side panel.

Similar to how Playground took our subscription document and put it in WebSocket messages in the right format, and how it parsed the response messages and displayed the payload on the page, most of our clients will be using libraries that take care of the messaging part, so that all theyâ€™ll get is the payload object: `{data: {githubStars: 89}}`.

### reviewCreated

> If youâ€™re jumping in here, `git checkout 18_0.2.0` (tag [18_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/18_0.2.0), or compare [18...19](https://github.com/GraphQLGuide/guide-api/compare/18_0.2.0...19_0.2.0))

In the last section we set up our first subscription for a single integer based on an external source of data. In this section weâ€™ll set up a subscription for an object type (`Review`) based on a user action (creating a review). The subscription will be named `reviewCreated`, and whenever any user creates a review, the server will send an event with that review data to all the clients that are subscribed to the `reviewCreated` subscription.

Letâ€™s start with the schema!

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/compare/18_0.2.0...19_0.2.0)

```gql
type Subscription {
  reviewCreated: Review!
}
```

We now have an error because weâ€™re declaring `type Subscription` in two places, so letâ€™s change the one in `Github.graphql` (which we can see in `src/schema/schema.graphql` is included after `Review.graphql` is included) to `extend type Subscription`:

[`src/schema/Github.graphql`](https://github.com/GraphQLGuide/guide-api/compare/18_0.2.0...19_0.2.0)

```gql
extend type Subscription {
  githubStars: Int!
}
```

Now we only need to do two things: 

- add a `Subscription.reviewCreated.subscribe` function that returns an iterator
- at the end of the `createReview` resolver, publish the new review object to that iterator

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/18_0.2.0...19_0.2.0)

```js
import { pubsub } from '../util/pubsub'

export default {
  Query: ...
  Review: ...
  Mutation: {
    createReview: (_, { review }, { dataSources, user }) => {
      ...

      const newReview = dataSources.reviews.create(review)

      pubsub.publish('reviewCreated', {
        reviewCreated: newReview
      })

      return newReview
    }
  },
  Subscription: {
    reviewCreated: { subscribe: () => pubsub.asyncIterator('reviewCreated') }
  }
}
```

The second argument to `pubsub.publish` is the event data, which needs to match the schema (`reviewCreated: Review!`): a `reviewCreated` attribute with an object of type `Review` for the value.

Aaaaand weâ€™re done! That was easy. To test, we start the subscription in one Playground tab:

```gql
subscription {
  reviewCreated {
    id
    text
    stars
    createdAt
  }
}
```

And create the review in another:

```gql
mutation {
  createReview(review: { text: "Now thatâ€™s a downtown job!", stars: 5 }) {
    id
    text
  }
}
```

![createReview completed in Playground](img/createReview-downtown-job.png)

Now when we go back to the subscription tab, weâ€™ll see the event:

![reviewCreated subscription with data received](img/subscription-downtown-job.png)

Other common types of subscriptions include when objects are edited and deleted:

```gql
type Subscription {
  reviewEdited: Review!
  reviewDeleted: ID!
}
```

`reviewEdited` events would include the review post-edit, and `reviewDeleted` events would just include the ID of the deleted review, so that clients can remove it from their cache. Weâ€™ll discuss subscriptions in more depth in the section Extended topics -> Subscription design.

# Testing

Background: [Testing](bg#testing)

- [Static testing](#static-tests)
- [Review integration tests](#review-integration-tests)
- [Code coverage](#code-coverage)
- [User integration tests](#user-integration-tests)
- [Unit tests](#unit-tests)
- [End-to-end tests](#end-to-end-tests)

In the Background chapter we go over [mocking](bg.md#mocking) and [which types of tests](bg.md#types-of-tests) are best to write. 

In this section weâ€™ll start out by setting up static testing. Next weâ€™ll write integration tests for our review operations. Then weâ€™ll check how much of the code weâ€™ve tested using a code coverage tool. Next weâ€™ll fill in some of the coverage gaps with more integration tests and unit tests. Finally, weâ€™ll write an end-to-end test.

## Static testing

> If youâ€™re jumping in here, `git checkout 19_0.2.0` (tag [19_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/19_0.2.0), or compare [19...20](https://github.com/GraphQLGuide/guide-api/compare/19_0.2.0...20_0.2.0))

Static testing is done through linting, a type of static code analysis. Itâ€™s called *static* because, unlike the tests we write code for, no code is being run during static testingâ€”instead, a tool analyzes the code for certain types of mistakes that can be found by just looking at the code and not running it. One such mistake is when we use a variable without declaring it. In JavaScript, the main tool for static analysis is [ESLint](https://eslint.org/docs/about/), and hereâ€™s a [list of possible *rules*](https://eslint.org/docs/rules/)â€”things it can analyze that we can choose to disallow in our code. 

We have `eslint` and `eslint-plugin-node` installed as dev dependencies, so all we need to do is configure ESLint:

[`.eslintrc.js`](https://github.com/GraphQLGuide/guide-api/blob/20_0.2.0/.eslintrc.js)

```js
module.exports = {
  env: {
    es6: true,
    node: true
  },
  extends: 'plugin:node/recommended',
  parserOptions: {
    sourceType: 'module'
  }
}
```

`env` says that weâ€™re using ES6 in Node.js, `extends` says to use `eslint-plugin-node`â€™s set of recommended linting rules, and `sourceType: 'module'` means that weâ€™re using modules. We can add an npm script for linting:
 
[`package.json`](https://github.com/GraphQLGuide/guide-api/compare/19_0.2.0...20_0.2.0)

```js
{
  ...
  "scripts": {
    ...
    "lint": "eslint src/"
  }
}
```

When we try it out (`npm run lint`), we get errors saying:

```
error  Import and export declarations are not supported yet  node/no-unsupported-features/es-syntax
```

Itâ€™s warning us that using the keywords `import` and `export` in our code wonâ€™t work because itâ€™s not supported by node. Our code actually does work, because weâ€™re using babel. So letâ€™s disable this rule (the name of the rule is printed at the end):

[`.eslintrc.js`](https://github.com/GraphQLGuide/guide-api/blob/20_0.2.0/.eslintrc.js)

```js
module.exports = {
  ...
  rules: {
    'node/no-unsupported-features/es-syntax': 0
  }
}
```

Now when we do `npm run lint`, it succeedsâ€”no errors are found.

A common practice is setting up linting to occur as a *pre-commit hook*â€”that is, a command that will automatically be run whenever we enter `git commit`, and if the command fails, the commit will be canceled. The easiest way to set this up is with [husky](https://github.com/typicode/husky), one of our dev dependencies, which simply uses a `package.json` attribute:

[`package.json`](https://github.com/GraphQLGuide/guide-api/compare/19_0.2.0...20_0.2.0)

```json
{
  ...
  "husky": {
    "hooks": {
      "pre-commit": "npm run lint"
    }
  }
}
```

Now if we commit, we see that `eslint src/` is run before the commit happens:

```sh
$ git commit -m 'Set up linting'
husky > pre-commit (node v8.11.3)

> guide-api@0.1.0 lint /guide-api
> eslint src/

[20 bfe4bf1] Set up linting
 2 files changed, 21 insertions(+), 1 deletion(-)
 create mode 100644 .eslintrc.js
```

## Review integration tests

> If youâ€™re jumping in here, `git checkout 20_0.2.0` (tag [20_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/20_0.2.0), or compare [20...21](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0))

The different types of testing are basically defined by how much is mocked ðŸ˜„. In integration tests, we usually just mock network requests. The main type of network request our server makes is to the database, so weâ€™ll be mocking our MongoDB collection methods. We also wonâ€™t need our tests to make network requests to the GraphQL server because Apollo has [`createTestClient()`](https://www.apollographql.com/docs/apollo-server/features/testing/#createtestclient) which allows us to query the server without starting the server. It puts our queries through the Apollo Server request pipeline as if they were HTTP requests. 

`createTestClient()` returns an object with `query` and `mutate` functions, which each take a `GraphQLRequest` object:

[`apollo-server-types`](https://github.com/apollographql/apollo-server/blob/master/packages/apollo-server-types/src/index.ts)

```ts
export interface GraphQLRequest {
  query?: string;
  operationName?: string;
  variables?: VariableValues;
  extensions?: Record<string, any>;
  http?: Pick<Request, 'url' | 'method' | 'headers'>;
}
```

Usually we just use the `query` and `variables` properties, but we can also use `http`, for instance to include an authorization header:

```js
const { query } = createTestClient(server)
query({
  query: gql`...`,
  http: {
    headers: {
      authorization: `Bearer ${token}`
    }
  }
})
```

Then the server would run our context function, decode the auth token, and add the user doc to the context that it gives to resolvers.

`createTestClient()`â€™s only parameter is an instance of Apollo Server, so our tests will need one. We canâ€™t use the one created in `src/index.js` because our tests will need to be able to modify data sources and context. So letâ€™s make a `createTestServer()` function. And letâ€™s create a new file that exports all of our testing helper functions and data, so that the test files can import whatever they need from one place:

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/test/guide-test-utils.js)

```js
import { ApolloServer } from 'apollo-server'

import { Reviews, Users } from '../src/data-sources/'
import {
  typeDefs,
  resolvers,
  context as defaultContext,
  formatError
} from '../src/'

export const createTestServer = ({ context = defaultContext } = {}) => {
  const reviews = new Reviews({})

  const users = new Users({})

  const server = new ApolloServer({
    typeDefs,
    resolvers,
    dataSources: () => ({ reviews, users }),
    context,
    formatError
  })

  return { server, dataSources: { reviews, users } }
}

export { createTestClient } from 'apollo-server-testing'
export { default as gql } from 'graphql-tag'
```

`createTestServer()` returns both the server instance and the data sources (so that tests can spy on or modify data source functions). In order for the above code to work, we need to add some exports:

[`src/data-sources/index.js`](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0)

```js
...

export { Reviews, Users, Github }
```

[`src/index.js](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0)

```js
...

export { typeDefs, resolvers, context, formatError }
```

Now that weâ€™ve got our `guide-test-utils.js` file, we can import from it into our test files. It would be nice if we could import without thinking about relative paths, as if it were a node module:

```js
import {
  createTestServer,
  createTestClient,
  gql
} from 'guide-test-utils'
```

To enable this, we can create a config file:

[`jest.config.js`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/jest.config.js)

```js
const path = require('path')

module.exports = {
  moduleDirectories: ['node_modules', path.join(__dirname, 'test')]
}
```

Jest will now look for modules both in `node_modules/` and in `test/`. ([Jest](https://jestjs.io), made by Facebook, is the most popular JavaScript testing framework.)

While it will run, it wonâ€™t pass linting, which weâ€™ll find out either in our editorâ€”if ESLint is enabledâ€”or when we try to commit and it fails:

```sh
husky > pre-commit (node v8.11.3)

> guide-api@0.1.0 lint /guide-api
> eslint src/


/guide-api/src/resolvers/Review.test.js
  6:8  error  "guide-test-utils" is not found  node/no-missing-import

âœ– 1 problem (1 error, 0 warnings)
```

ESLint is looking in our `node_modules/` to make sure that anything we import is there. But there is no `node_modules/guide-test-utils/`, so it gives an error. If we look at the documentation for the [`node/no-missing-import` rule](https://github.com/mysticatea/eslint-plugin-node/blob/master/docs/rules/no-missing-import.md), we learn that thereâ€™s a way to tell it to look in additional locations for modulesâ€”in this case, we want it to look in the `./test` directory:

[`.eslintrc.js`](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0)

```js
module.exports = {
  ...
  rules: {
    ...
    'node/no-missing-import': [
      'error',
      {
        resolvePaths: ['./test']
      }
    ]
  }
}
```

Now committing or doing `npm run lint` succeeds âœ….

Letâ€™s move on to writing the review tests themselves. Since the entry point to review operations and most of the logic is in the resolvers, letâ€™s put our test file next to the `Review.js` resolvers file, adding `.test` to the filename:

[`src/resolvers/Review.test.js`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/src/resolvers/Review.test.js)

```js
import {
  createTestServer,
  createTestClient,
  gql
} from 'guide-test-utils'

test('something', () => {
  const result = ...

  expect(result).toSomething()
})
```

Jest provides a set of [global functions](https://jestjs.io/docs/en/api#describename-fn), including the basic test function [`test()`](https://jestjs.io/docs/en/api#testname-fn-timeout) (or `it()`), in which we run part of our code and assert something about the result. We use `expect()` for assertions, which is followed by any of [a number of *matcher* methods](https://jestjs.io/docs/en/expect), such as:

```js
expect(result).toBeTruthy()
expect(result).toBe('this string')
expect(array).not.toContain(10)
expect(doSomething).toThrow('must be logged in')
```

Weâ€™ll write two tests, one for each review operation (`reviews` query and `createReview` mutation):

```js
import {
  createTestServer,
  createTestClient,
  gql
} from 'guide-test-utils'

test('reviews', () => {

})

test('createReview', () => {

})
```

For the first, weâ€™ll start by first creating a test server and then a test client:

```js
import {
  createTestServer,
  createTestClient,
  gql
} from 'guide-test-utils'

test('reviews', async () => {
  const { server } = createTestServer()
  const { query } = createTestClient(server)

  const result = await query({ query: ... })
})
```

We need a query document to give to `query()`. To try to cover as many resolvers as possible, letâ€™s select all `Review` and `User` fields except `User.email` (it requires authentication, which weâ€™ll do in the second test).

```js
const REVIEWS = gql`
  query {
    reviews {
      id
      text
      stars
      author {
        id
        firstName
        lastName
        username
        photo
        createdAt
        updatedAt
      }
      createdAt
      updatedAt
    }
  }
`

test('reviews', async () => {
  const { server } = createTestServer()
  const { query } = createTestClient(server)

  const result = await query({ query: REVIEWS })
})
```

This test will send the `REVIEWS` query via the test client to our server. But before we make an assertion and run our code, we have to mock the database! Specifically, we have to mock the collection functions that will be called when our query is run. Looking at `src/resolvers/Review.js`, we see that `dataSources.reviews.all` and `dataSources.users.findOneById` are called. They both call `this.collection.find().toArray()`, so we need to mock `.find().toArray()` for both collections, as well as `this.collection.createIndex()`, which we call in the `Users` data source constructor.

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/test/guide-test-utils.js)

```js
export const createTestServer = ({ context = defaultContext } = {}) => {
  const reviews = new Reviews({
    find: () => ({
      toArray: jest.fn().mockResolvedValue(mockReviews)
    })
  })

  const users = new Users({
    createIndex: jest.fn(),
    find: () => ({
      toArray: jest.fn().mockResolvedValue(mockUsers)
    })
  })

  const server = new ApolloServer({
    dataSources: () => ({ reviews, users }),
    ...
  })

  ...
}
```

Weâ€™ll create a mock function using [`jest.fn()`](https://jestjs.io/docs/en/mock-function-api). By default it returns `undefined`, which works for `createIndex()`, but for `find()` we need to return an object that has a `toArray()` method that returns a Promise that resolves to an array of documents ðŸ˜µðŸ˜„. Weâ€™ll also need to create the `mockReviews` and `mockUsers` constants:

```js
import { ObjectId } from 'mongodb'

const updatedAt = new Date('2020-01-01')

export const mockUser = {
  _id: ObjectId('5d24f846d2f8635086e55ed3'),
  firstName: 'First',
  lastName: 'Last',
  username: 'mockA',
  authId: 'mockA|1',
  email: 'mockA@gmail.com',
  updatedAt
}

const mockUsers = [mockUser]

const reviewA = {
  _id: ObjectId('5ce6e47b5f97fe69e0d63479'),
  text: 'A+',
  stars: 5,
  updatedAt,
  authorId: mockUser._id
}

const reviewB = {
  _id: ObjectId('5cf8add4c872001f31880a97'),
  text: 'Passable',
  stars: 3,
  updatedAt,
  authorId: mockUser._id
}

const mockReviews = [reviewA, reviewB]
```

Now our `'reviews'` test should return `reviewA` and `reviewB`, both with author `mockUser`. Letâ€™s complete the test with an assertion:

```js
test('reviews', async () => {
  const { server } = createTestServer()
  const { query } = createTestClient(server)

  const result = await query({ query: REVIEWS })
  expect(result).toMatchSnapshot()
})
```

To run the test, letâ€™s add an npm script:

[`package.json`](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0)

```json
{ 
  ...
  "scripts": {
    ...
    "test": "jest"    
  }
}
```

Now when we do `npm run test` (or just `npm test`), Jest will find all `*.test.js` files and run the tests it finds inside them. 

Our assertion `expect(result).toMatchSnapshot()` will save a snapshot (a serialization of the result, saved to a new `__snapshots__/` directory). Whenever we get a different result from the saved snapshot, the test will fail, and weâ€™ll either need to fix the code or (in the case when the result is correctly different) tell Jest to update the snapshot.

> Snapshots should be added to git.

```
$ npm test

> guide-api@0.1.0 test /guide-api
> jest

 PASS  src/resolvers/Review.test.js
  âœ“ reviews (58ms)

 â€º 1 snapshot written.
  console.log src/index.js:22
    GraphQL server running at http://localhost:4000/

Snapshot Summary
 â€º 1 snapshot written from 1 test suite.

Test Suites: 1 passed, 1 total
Tests:       1 passed, 1 total
Snapshots:   1 written, 1 total
Time:        3.375s, estimated 4s
Ran all test suites.
Jest did not exit one second after the test run has completed.

This usually means that there are asynchronous operations that werenâ€™t stopped in your tests. Consider running Jest with `--detectOpenHandles` to troubleshoot this issue.
```

> To terminate the command, type `Ctrl-C`.

We see that our one test passes, and a new snapshot is written. We can look at the file to make sure itâ€™s correct:

[`src/resolvers/__snapshots__/Review.test.js.snap`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/src/resolvers/__snapshots__/Review.test.js.snap)

```js
// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`reviews 1`] = `
Object {
  "data": Object {
    "reviews": Array [
      Object {
        "author": Object {
          "createdAt": 1562703942000,
          "firstName": "First",
          "id": "5d24f846d2f8635086e55ed3",
          "lastName": "Last",
          "photo": "https://avatars.githubusercontent.com/u/1",
          "updatedAt": 1577836800000,
          "username": "mockA",
        },
        "createdAt": 1558635643000,
        "id": "5ce6e47b5f97fe69e0d63479",
        "stars": 5,
        "text": "A+",
        "updatedAt": 1577836800000,
      },
      Object {
        "author": Object {
          "createdAt": 1562703942000,
          "firstName": "First",
          "id": "5d24f846d2f8635086e55ed3",
          "lastName": "Last",
          "photo": "https://avatars.githubusercontent.com/u/1",
          "updatedAt": 1577836800000,
          "username": "mockA",
        },
        "createdAt": 1559801300000,
        "id": "5cf8add4c872001f31880a97",
        "stars": 3,
        "text": "Passable",
        "updatedAt": 1577836800000,
      },
    ],
  },
  "errors": undefined,
  "extensions": undefined,
  "http": Object {
    "headers": Headers {
      Symbol(map): Object {},
    },
  },
}
`;
```

That looks good! Weâ€™ve got what we expected in the `"data"` result attribute and nothing in the `"errors"` attribute. However, if we look at the end of the test output, we see a problem:

```
Jest did not exit one second after the test run has completed.

This usually means that there are asynchronous operations that werenâ€™t stopped in your tests. Consider running Jest with `--detectOpenHandles` to troubleshoot this issue.
```

Itâ€™s saying weâ€™ve started code running that hasnâ€™t stopped running. If we look above that, we see this output:

```
  console.log src/index.js:22
    GraphQL server running at http://localhost:4000/
```

It looks like our non-test server is runningâ€”thatâ€™s the running code that Jest is warning us about. So we need to edit `src/index.js` to not start the server during tests. Jest sets `NODE_ENV` to `'test'`, so letâ€™s use that:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0)

```js
const start = () => {
  server
    .listen({ port: 4000 })
    .then(({ url }) => console.log(`GraphQL server running at ${url}`))
}

if (process.env.NODE_ENV !== 'test') {
  start()
}
```

Instead of starting the server with `server.listen()` at the top level, we put it in a function and only call it when weâ€™re not testing. However, when we run `npm test` again, while we no longer get the `console.log`, we still get the warning, which means there must be more code that we start running at the top level...

The database connection! Letâ€™s put that in a function as well:

[`src/db.js`](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0)

```js
import { MongoClient } from 'mongodb'

export let db

const URL = 'mongodb://localhost:27017/guide'

export const connectToDB = () => {
  const client = new MongoClient(URL, { useNewUrlParser: true })
  client.connect(e => {
    if (e) {
      console.error(`Failed to connect to MongoDB at ${URL}`, e)
      return
    }

    db = client.db()
  })
}
```

And weâ€™ll call it from `start()`. Weâ€™ll also move `Github.startPolling()` from the top level of `src/data-sources/index.js`:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...21_0.2.0)

```js
import dataSources, { Github } from './data-sources'
import { connectToDB } from './db'

const start = () => {
  connectToDB()
  Github.startPolling()
  server
    .listen({ port: 4000 })
    .then(({ url }) => console.log(`GraphQL server running at ${url}`))
}

if (process.env.NODE_ENV !== 'test') {
  start()
}
```

Now `npm test` completes normally. To recap, we set up integration tests for review operations by:

- Creating a test version of the server.
- Making a test utilities file that can be used like a node module.
- Writing a test.
- Mocking MongoDB collection methods.
- Preventing long-running server code from starting during testing.

Lastly, we have our second test to writeâ€”`'createReview'`:

[`src/resolvers/Review.test.js`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/src/resolvers/Review.test.js)

```js
test('createReview', async () => {
  const { server } = createTestServer({
    context: () => ({ user: mockUser })
  })
  const { mutate } = createTestClient(server)

  const result = await mutate({
    mutation: CREATE_REVIEW,
    variables: { review: { text: 'test', stars: 1 } }
  })
  expect(result).toMatchSnapshot()
})
```

Similarly to `'reviews'`, we create a test server and client, send an operation via the test client, and assert the response matches the snapshot. The differences are:

- We need to set the serverâ€™s context as if weâ€™re logged in as `mockUser` so that we donâ€™t get the `ForbiddenError`.
- We use `mutate()` instead of `query()`, and provide the `review` variable.

For the mutation, we have:

```js
const CREATE_REVIEW = gql`
  mutation CreateReview($review: CreateReviewInput!) {
    createReview(review: $review) {
      id
      text
      stars
      author {
        id
        email
      }
      createdAt
    }
  }
`
```

We include `email`, which weâ€™ll have access to because weâ€™re logged in as `mockUser` and `mockUser` will be used for the new reviewâ€™s `author` field.

The one thing we havenâ€™t done yet is update our database mock functions. It looks like the only new function that will be called is `this.collection.insertOne()`, which is used in `src/data-sources/Reviews.js`:

```js
export default class Reviews extends MongoDataSource {
  ...

  create(review) {
    review.authorId = this.context.user._id
    review.updatedAt = new Date()
    this.collection.insertOne(review)
    return review
  }
}
```

The only thing we were depending on `insertOne()` doing was adding an `_id` property, so letâ€™s mock that:

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/test/guide-test-utils.js)

```js
export const createTestServer = ({ context = defaultContext } = {}) => {
  const reviews = new Reviews({
    find: jest.fn(() => ({
      toArray: jest.fn().mockResolvedValue(mockReviews)
    })),
    insertOne: jest.fn(
      doc => (doc._id = new ObjectId('5cf8b6ff37568a1fa500ba4e'))
    )
  })

  ...
}
```

Now when we run the tests, we see that two are passing, and one new snapshot is written:

```
$ npm test

> guide-api@0.1.0 test /guide-api
> jest

 PASS  src/resolvers/Review.test.js
  âœ“ reviews (41ms)
  âœ“ createReview (21ms)

 â€º 1 snapshot written.
Snapshot Summary
 â€º 1 snapshot written from 1 test suite.

Test Suites: 1 passed, 1 total
Tests:       2 passed, 2 total
Snapshots:   1 written, 1 passed, 2 total
Time:        3.745s
Ran all test suites.
```

And one new snapshot is written:

[`src/resolvers/__snapshots__/Review.test.js.snap`](https://github.com/GraphQLGuide/guide-api/blob/21_0.2.0/src/resolvers/__snapshots__/Review.test.js.snap)

```js
exports[`createReview 1`] = `
Object {
  "data": Object {
    "createReview": Object {
      "author": Object {
        "email": "mockA@gmail.com",
        "id": "5d24f846d2f8635086e55ed3",
      },
      "createdAt": 1559803647000,
      "id": "5cf8b6ff37568a1fa500ba4e",
      "stars": 1,
      "text": "test",
    },
  },
  "errors": undefined,
  "extensions": undefined,
  "http": Object {
    "headers": Headers {
      Symbol(map): Object {},
    },
  },
}
`;

...
```

Looks good! âœ…

## Code coverage

> If youâ€™re jumping in here, `git checkout 21_0.2.0` (tag [21_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/21_0.2.0), or compare [21...22](https://github.com/GraphQLGuide/guide-api/compare/20_0.2.0...22_0.2.0))

Jest analyzes *code coverage*â€”how much of our code gets run during our testsâ€”with the `--coverage` flag. We can look at code coverage to see which parts of the code arenâ€™t covered by tests, so that we know what our new tests should cover. 

Letâ€™s update our test script:

[`package.json`](https://github.com/GraphQLGuide/guide-api/compare/21_0.2.0...22_0.2.0)

```js
{
  ...
  "scripts": {
    ...
    "test": "jest --coverage",
    "open-coverage": "open coverage/lcov-report/index.html"
  },
```

When `jest --coverage` runs, it both logs statistics and updates the coverage report, which is located in the `coverage/` directory. We can now do `npm run open-coverage` for opening the HTML report. We can run jest without coverage with:

```sh
$ npx jest
``` 

Or to keep it open, re-running tests whenever we edit our code:

```sh
$ npx jest --watch
```

Or to keep it open with code coverage, one of these commands:

```sh
$ npx jest --coverage --watch
$ npm test -- --watch
```

> `--` after an npm script tells npm to apply the subsequent arguments to the script.

We should tell git to ignore the generated `coverage/` report directory:

[`.gitignore`](https://github.com/GraphQLGuide/guide-api/compare/21_0.2.0...22_0.2.0)

```
node_modules/
dist/
.env
coverage/
```

And we need to tell Jest which JavaScript files it should analyze coverage for, using the `collectCoverageFrom` config:

[`jest.config.js`](https://github.com/GraphQLGuide/guide-api/compare/21_0.2.0...22_0.2.0)

```js
module.exports = {
  moduleDirectories: ['node_modules', path.join(__dirname, 'test')],
  collectCoverageFrom: ['src/**/*.js']
}
```

Hereâ€™s the new output of `npm test`:

![Jest with coverage statistics](img/coverage-bash.png)

We see the coverage overall, for each directory, and each JS file, in percentage of statements, logic branches, functions, and lines. To see which lines of code are covered, we can view the HTML report:

```sh
$ npm run open-coverage
```

![Coverage report overview HTML page](img/coverage-web.png)

And follow links to a particular file weâ€™d like to look at, like `src/index.js`:

![Coverage report for src/index.js](img/coverage-index.png)

The red highlighted code shows what wasnâ€™t run during the test. Anything at the top level was run, like the imports, `ApolloServer` instance creation, and the if statement condition, but the body of the if statementâ€”and the body of the start function, which wasnâ€™t calledâ€”wasnâ€™t run and thus is red. The highlighting isnâ€™t perfectâ€”notice that `.listen` and `.then` should also be red but arenâ€™t.

If we want to make sure that contributors to our project write tests that cover new code, we can set a minimum coverage threshold, below which the test command will fail. We can set it for any statisticâ€”statements, branches, functions, or linesâ€”and either globally or for individual files. Letâ€™s set a global statement threshold of 40%:

[`jest.config.js`](https://github.com/GraphQLGuide/guide-api/compare/21_0.2.0...22_0.2.0)

```js
module.exports = {
  moduleDirectories: ['node_modules', path.join(__dirname, 'test')],
  collectCoverageFrom: ['src/**/*.js'],
  coverageThreshold: {
    global: {
      statements: 40
    }
  }
}
```

Now the test will fail whenever the code coverage statement ratio is below 40%. Weâ€™re currently below 40%, so when we re-run `npm test`, it fails:

![global coverage threshold for statements (40%) not met: 34.39%](img/coverage-below-threshold.png)

## User integration tests

> If youâ€™re jumping in here, `git checkout 22_0.2.0` (tag [22_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/22_0.2.0), or compare [22...23](https://github.com/GraphQLGuide/guide-api/compare/22_0.2.0...23_0.2.0))

Letâ€™s try to meet our 40% coverage threshold. Looking at `src/resolvers/User.js`, we can see that our queries are red:

![HTML coverage report of src/resolvers/User.js](img/coverage-user-red.png)

This makes sense, as our tests havenâ€™t sent any user queriesâ€”theyâ€™ve just selected `User` fields in review operations. Accordingly, when we scroll down, we see the only covered lines are for `User` field resolvers:

![User field resolvers mostly not red](img/coverage-user-fields.png)

Letâ€™s write some integration tests that query user operations. Weâ€™ll start with the same imports and test format (one for each operation) as we did with `Review.test.js`:

[`src/resolvers/User.test.js`](https://github.com/GraphQLGuide/guide-api/blob/23_0.2.0/src/resolvers/User.test.js)

```js
import {
  createTestServer,
  createTestClient,
  gql,
  mockUser
} from 'guide-test-utils'

test('me', async () => {
  ...
})

test('user', async () => {
  ...
})

test('searchUsers', async () => {
  ...
})

test('createUser', async () => {
  ...
})
```

For the `me` test, we can set the `context` to a user with a certain `_id`, and then check to make sure the resultâ€™s `id` matches:

```js
const ME = gql`
  query {
    me {
      id
    }
  }
`

test('me', async () => {
  const { server } = createTestServer({
    context: () => ({ user: { _id: 'itme' } })
  })
  const { query } = createTestClient(server)

  const result = await query({ query: ME })
  expect(result.data.me.id).toEqual('itme')
})
```

We donâ€™t need to worry about selecting and testing other fields, as we know theyâ€™ve been covered.

Next is the `user` query. We know our mock users collection always returns `mockUser`, so weâ€™ll query for that user:

```js
const USER = gql`
  query User($id: ID!) {
    user(id: $id) {
      id
    }
  }
`

test('user', async () => {
  const { server } = createTestServer()
  const { query } = createTestClient(server)

  const id = mockUser._id.toString()
  const result = await query({
    query: USER,
    variables: { id }
  })
  expect(result.data.user.id).toEqual(id)
})
```

For the `searchUsers` test, letâ€™s set it up so that multiple results are returned. For that, weâ€™ll need to temporarily change the mocked `users.find` function. To get access to that function, we need to get the dataSources from `createTestServer()`:

```js
test('searchUsers', async () => {
  const userA = { _id: 'A' }
  const userB = { _id: 'B' }
  const { server, dataSources } = createTestServer()

  dataSources.users.collection.find.mockReturnValueOnce({
    toArray: jest.fn().mockResolvedValue([userA, userB])
  })
```

`mockReturnValueOnce()` will have `users.find` return the given value once and then go back to returning `[mockUser]` as it was before. After we make the query, we can also test to see what `users.find` was called with:

```js
  expect(dataSources.users.collection.find).toHaveBeenCalledWith({
    $text: { $search: 'foo' }
  })
```

All together, thatâ€™s:

```js
const SEARCH_USERS = gql`
  query SearchUsers($term: String!) {
    searchUsers(term: $term) {
      ... on User {
        id
      }
    }
  }
`

test('searchUsers', async () => {
  const userA = { _id: 'A' }
  const userB = { _id: 'B' }
  const { server, dataSources } = createTestServer()

  dataSources.users.collection.find.mockReturnValueOnce({
    toArray: jest.fn().mockResolvedValue([userA, userB])
  })

  const { query } = createTestClient(server)

  const result = await query({
    query: SEARCH_USERS,
    variables: { term: 'foo' }
  })

  expect(dataSources.users.collection.find).toHaveBeenCalledWith({
    $text: { $search: 'foo' }
  })
  expect(result.data.searchUsers[0].id).toEqual('A')
  expect(result.data.searchUsers[1].id).toEqual('B')
})
```

For the last test, our `createUser` mutation will be calling `users.insertOne`, which we havenâ€™t mocked yet. Letâ€™s reuse the `insertOne` function we used for reviews:

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/compare/22_0.2.0...23_0.2.0)

```js
const insertOne = jest.fn(
  doc => (doc._id = new ObjectId('5cf8b6ff37568a1fa500ba4e'))
)

export const createTestServer = ({ context = defaultContext } = {}) => {
  const reviews = new Reviews({
    find: jest.fn(() => ({
      toArray: jest.fn().mockResolvedValue(mockReviews)
    })),
    insertOne
  })

  const users = new Users({
    createIndex: jest.fn(),
    find: jest.fn(() => ({
      toArray: jest.fn().mockResolvedValue(mockUsers)
    })),
    insertOne
  })
  
  ...
```

For the mutation input, letâ€™s `pick` the fields from `mockUser`:

[`src/resolvers/User.test.js`](https://github.com/GraphQLGuide/guide-api/blob/23_0.2.0/src/resolvers/User.test.js)

```js
import { pick } from 'lodash'

const CREATE_USER = gql`
  mutation CreateUser($user: CreateUserInput!, $secretKey: String!) {
    createUser(user: $user, secretKey: $secretKey) {
      id
    }
  }
`

test('createUser', async () => {
  const { server } = createTestServer()
  const { mutate } = createTestClient(server)

  const user = pick(mockUser, [
    'firstName',
    'lastName',
    'username',
    'email',
    'authId'
  ])

  const result = await mutate({
    mutation: CREATE_USER,
    variables: {
      user,
      secretKey: process.env.SECRET_KEY
    }
  })

  expect(result).toMatchSnapshot()
})
```

Whenever weâ€™re using a snapshot, we should check it on the first run to make sure itâ€™s correct. If we run `npm test`, then we should see a new file:

[`src/resolvers/__snapshots__/User.test.js.snap`](https://github.com/GraphQLGuide/guide-api/blob/23_0.2.0/src/resolvers/__snapshots__/User.test.js.snap)

```js
// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`createUser 1`] = `
Object {
  "data": Object {
    "createUser": Object {
      "id": "5cf8b6ff37568a1fa500ba4e",
    },
  },
  "errors": undefined,
  "extensions": undefined,
  "http": Object {
    "headers": Headers {
      Symbol(map): Object {},
    },
  },
}
`;
```

Looks good! âœ… We can also see that our statement coverage is above the 40% minimum, so our tests pass!

![Passing console output with statements at 42.68%](img/coverage-above-threshold.png)

## Unit tests

> If youâ€™re jumping in here, `git checkout 23_0.2.0` (tag [23_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/23_0.2.0), or compare [23...24](https://github.com/GraphQLGuide/guide-api/compare/23_0.2.0...24_0.2.0))

Weâ€™ve written integration tests that cover most of our queries and mutations. If we want a higher test coverage, we could write more integration tests with different arguments or mock data that result in different parts of the code getting run. We could also write unit tests that cover individual functions. In this section weâ€™ll write a unit test that covers the `user` query. As we can see in the coverage report, weâ€™re missing three lines:

![User.js coverage showing 64%](img/coverage-user-16.png)

Letâ€™s first write a unit test that triggers the invalid ObjectId error. We can either add it to `User.test.js` or create separate files for unit tests named `File.unit.test.js`. The latter has the benefit of smaller files and we can run all the unit tests together with `npm test -- unit`. 

> An alternative file structure would be to move all integration tests to the `test/` directory and only place unit tests next to the files theyâ€™re testing. So `test/User.test.js` for integration and `src/resolvers/User.test.js` for unit testing `src/resolvers/User.js`.

Instead of using the test server and client, we can import the resolver function and call it ourselves:

[`src/resolvers/User.unit.test.js`](https://github.com/GraphQLGuide/guide-api/blob/24_0.2.0/src/resolvers/User.unit.test.js)

```js
import resolvers from './User'
import { InputError } from '../util/errors'

test('user throws InputError', () => {
  expect(() =>
    resolvers.Query.user(
      null,
      { id: 'invalid' },
      { dataSources: { users: { findOneById: jest.fn() } } }
    )
  ).toThrow(InputError)
})
```

We mock the `dataSources.users.findOneById` function, and we assert that an instance of `InputError` will be thrown.

However if we want to fit the strict definition of a unit test that says everything must be mocked, we need to mock `ObjectId()`. Since itâ€™s imported from an NPM module, we can use the [`jest.mock()`](https://jestjs.io/docs/en/jest-object#jestmockmodulename-factory-options) function, which mocks the module for all the tests in the same file:

```js
jest.mock('mongodb', () => ({
  ObjectId: id => {
    if (id === 'invalid') {
      throw new Error(
        'Argument passed in must be a single String of 12 bytes or a string of 24 hex characters'
      )
    }
  }
}))
```

Now when `User.js` imports the function (`import { ObjectId } from 'mongodb'`), it will get our version of it.

> For further examples of `jest.mock()`, check out the [SQL testing](11.md#sql-testing) section later on in this chapter.

When we re-run `npm test` and refresh the coverage report, we see that the statements coverage has gone up from 16/25 to 18/25:

![User.js coverage showing 72%](img/coverage-user-18.png)

Thereâ€™s one statement left in this function: the `throw error` line. For that, we need to have `dataSources.users.findOneById()` throw a different error and make sure that `resolvers.Query.user()` throws the same error.

[`src/resolvers/User.unit.test.js`](https://github.com/GraphQLGuide/guide-api/blob/24_0.2.0/src/resolvers/User.unit.test.js)

```js
import resolvers from './User'
import { InputError } from '../util/errors'

test('user throws data source errors', () => {
  const MOCK_MONGO_ERROR = 'Unable to connect to DB'

  expect(() =>
    resolvers.Query.user(
      null,
      { id: mockMongoId },
      {
        dataSources: {
          users: {
            findOneById: () => {
              throw new Error(MOCK_MONGO_ERROR)
            }
          }
        }
      }
    )
  ).toThrow(MOCK_MONGO_ERROR)
})
```

![User.js coverage showing 76%](img/coverage-user-19.png)

Now the `user` query is completely green. And we could continue writing unit tests for more functions or files, either until we covered the most important pieces of logic, or until we met our overall desired test coverage percentage.

## End-to-end tests

> If youâ€™re jumping in here, `git checkout 24_0.2.0` (tag [24_0.2.0](https://github.com/GraphQLGuide/guide-api/tree/24_0.2.0), or compare [24...25](https://github.com/GraphQLGuide/guide-api/compare/24_0.2.0...25_0.2.0))

The final type of testing is end-to-end, or e2e. In backend e2e testing, we start the server and database, and then we test by sending HTTP requests to the server. So our tests will look something like this:

```js
beforeAll(startE2EServer)
afterAll(stopE2EServer)

test('query A', () => {
  const result = makeHttpRequest(queryA)

  expect(result).toMatchSnapshot()
})
```

Letâ€™s start by writing the `startE2EServer()` helper. We want it to look like this:

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/compare/24_0.2.0...25_0.2.0)

```js
export const startE2EServer = () => {
  // start server and connect to db

  return {
    stop: () => // stops server and db client
    request: () => // send http request to server
  }
}
```

It returns the `stop()` and `request()` functions for the tests to use. We can fill in the first comment:

```js
import { server } from '../src/'
import { connectToDB } from '../src/db'

export const startE2EServer = async () => {
  // start server and connect to db
  const e2eServer = await server.listen({ port: 0 })
  await connectToDB()

  return {
    stop: () => // stops server and db client
    request: () => // send http request to server
  }
}
```

`{ port: 0 }` uses any available port, which we do because the default port (4000) will be in use if our dev server is running while we run our tests. In order to `await` our call to `connectToDB()`, we need to make it async instead of callback-based:

[`src/db.js`](https://github.com/GraphQLGuide/guide-api/compare/24_0.2.0...25_0.2.0)

```js
export let db

export const connectToDB = async () => {
  const client = new MongoClient(URL, { useNewUrlParser: true })
  await client.connect()
  db = client.db()
  return client
}
```

We also need to `return client` so that we can close the connection when testing is done. For stopping the server, there is a `e2eServer.server.close`, but itâ€™s callback-based. We can use nodeâ€™s [`promisify()`](https://nodejs.org/api/util.html#util_util_promisify_original) to turn it into a Promise that we can `await`:

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/compare/24_0.2.0...25_0.2.0)

```js
import { promisify } from 'util'

export const startE2EServer = async () => {
  const e2eServer = await server.listen({ port: 0 })
  const dbClient = await connectToDB()

  const stopServer = promisify(e2eServer.server.close.bind(e2eServer.server))

  return {
    stop: async () => {
      await stopServer()
      await dbClient.close()
    }
    request: () => // send http request to server
  }
}
```

We also use [bind](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/bind) to maintain the functionâ€™s `this`.

We can make our function run faster by performing startup and stopping in parallel using [`Promise.all()`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/all):

```js
export const startE2EServer = async () => {
  const [e2eServer, dbClient] = await Promise.all([
    server.listen({ port: 0 }),
    connectToDB()
  ])

  const stopServer = promisify(e2eServer.server.close.bind(e2eServer.server))

  return {
    stop: () => Promise.all([stopServer(), dbClient.close()]),
    request: () => // send http request to server
  }
}
```

Lastly, we can send HTTP requests to the server using Apollo Link. [`apollo-link-http`](https://www.apollographql.com/docs/link/links/http/) has the basic `HttpLink` and `apollo-link` has [`execute()`](https://github.com/apollographql/apollo-link/blob/70f342380117fdfdbb5bad0987cd120689659ef2/packages/apollo-link/src/link.ts#L126-L138), a function that sends GraphQL operations over a link, and `toPromise()`, which converts the Observable that `execute()` returns into a Promise. All together, thatâ€™s:

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/compare/24_0.2.0...25_0.2.0)

```js
import { promisify } from 'util'
import { HttpLink } from 'apollo-link-http'
import fetch from 'node-fetch'
import { execute, toPromise } from 'apollo-link'

import { server } from '../src/'
import { connectToDB } from '../src/db'

export const startE2EServer = async () => {
  const [e2eServer, dbClient] = await Promise.all([
    server.listen({ port: 0 }),
    connectToDB()
  ])

  const stopServer = promisify(e2eServer.server.close.bind(e2eServer.server))

  const link = new HttpLink({
    uri: e2eServer.url,
    fetch
  })

  return {
    stop: () => Promise.all([stopServer(), dbClient.close()]),
    request: operation => toPromise(execute(link, operation))
  }
}
```

We also need `src/index.js` to add `server` to its exports:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/24_0.2.0...25_0.2.0)

```js
const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context,
  formatError
})

...

export { server, typeDefs, resolvers, context, formatError }
```

Now we can write our e2e test:

[`test/e2e.test.js`](https://github.com/GraphQLGuide/guide-api/blob/25_0.2.0/test/e2e.test.js)

```js
import { gql, startE2EServer } from 'guide-test-utils'

let stop, request

beforeAll(async () => {
  const server = await startE2EServer()
  stop = server.stop
  request = server.request
})

afterAll(() => stop())

const HELLO = gql`
  query {
    hello
  }
`

test('hello', async () => {
  const result = await request({ query: HELLO })

  expect(result).toMatchSnapshot()
})
```

We start the server in `beforeAll()` and stop it in `afterAll()`. Then we create our query document, which we send to the server using `request()` in our one test. After we run the test, we check the snapshot:

[`test/__snapshots__/e2e.test.js.snap`](https://github.com/GraphQLGuide/guide-api/blob/25_0.2.0/test/__snapshots__/e2e.test.js.snap)

```js
exports[`hello 1`] = `
Object {
  "data": Object {
    "hello": "ðŸŒðŸŒðŸŒŽ",
  },
}
`;
```

# Production

Once our GraphQL server code works and we want others (or our client code) to use it, we need to put it into production. This section contains the basic steps:

- [Deploying our code](#deployment)
- [Setting up our databases](#database-hosting)
- [Gathering analytics](11.md#analytics)
- [Tracking errors](11.md#error-reporting)

And in the last part of the chapter, [Extended topics](#extended-topics), we cover additional topics relevant to production, including [security](11.md#security) and [performance](11.md#performance).

## Deployment

* [Options](11.md#options)
* [Deploying](11.md#deploying)
* [Environment variables](11.md#environment-variables)

### Options

For our GraphQL API to be accessible, we need our code to run on a server that is *publicly addressable*â€”i.e., it can be reached via a public IP address. Our dev computer usually canâ€™t be reached because it has a local (non-public) IP address (often starting with `192.168.*.*`), and the router that connects us to the internet (which does have a public IP) usually doesnâ€™t respond to HTTP requests. While we could set the router up to forward requests to our dev computer, we then would have to leave our computer there and powered on, as well as do a number of other things to keep it working (like [DDNS](https://en.wikipedia.org/wiki/Dynamic_DNS)). Given the trouble and unreliability of that solution, we usually run our server code on a different computerâ€”a production serverâ€”thatâ€™s been built, set up, and maintained for that purpose.

The *deployment* process is copying the latest version of our code to the production server and running it. There are four main types of production servers we can use:

- **On-prem**: In *on premises*, we buy our own server, plug it into a power outlet, connect it to the internet, and then maintain it ourselves.
- **IaaS**: In *infrastructure as a service*, a company (like Amazon with its EC2 service) houses and maintains the physical servers, and we choose the operating system. We connect to the operating system over SSH to get a command prompt and then install Node, copy our code to the machine, and run it.
- **PaaS**: *Platform as a service* is like IaaS, except in addition to maintaining the physical servers, the company also maintains the operating system and software server. For example, a Node PaaS company would install and update Node.js, and we would send them our code, and they would run it with their version of Node.
- **FaaS**: *Function as a service* (also known as *serverless*) is like PaaS, except instead of sending them Node server code (which runs continuously and responds to any path / route), we send them individual JavaScript functions and configure which route triggers which function. Then, when we get HTTP requests, their server runs the right function. The function returns the response, which their server forwards to the client. Once the function returns, our code stops runningâ€”with FaaS, we donâ€™t have a continuously running server process.

These options appear in:

- decreasing order of complexity to use. Itâ€™s most difficult to run our own server, and itâ€™s easiest to write and upload a single function.
- increasing time order:
  - 1970s: On-prem was the original type of server since the beginning of the internet.
  - 2006: Amazon Web Services (AWS) came out with EC2, the most popular IaaS.
  - 2009: Heroku, which popularized PaaS, publicly launched.
  - 2014: AWS came out with Lambda, the most popular FaaS.

Currently, PaaS seems to be the most popular option in modern web development. However, FaaS is rising and may eclipse PaaS. Notably, the most popular PaaS in the Node community ([Vercel Now](https://vercel.com/home), formerly Zeit Now), switched to FaaS. While FaaS might be better for many applications, there are some disadvantages:

- **No continuous server process**: When we have a process (as we do with on-prem, IaaS, and PaaS), we can do things like:
  - Store data in memory between requests. The alternative that usually suffices is using an independent memory store, like a Redis server, which adds a small network latency (only ~0.2ms if itâ€™s inside the same AWS Availability Zone).
  - Open and maintain a WebSocket connection. However, some FaaS providers have added the ability to use WebSockets: At the end of 2018, AWS added support for WebSockets to its API Gateway, which can call a Lambda function when each message arrives over the socket.
- **Database limitations**: Since thereâ€™s no continuous server process, our database client library canâ€™t maintain a pool of connections for our requests to go out on; instead, each function makes its own connection. So the database has to be able to accept many connections over SSL.
- **Latency**: When thereâ€™s not an existing server process, the FaaS provider has to start a new process (with a copy of our code and npm packages) to handle an incoming request, and that takes time, which increases the latency (i.e., total response time of the server). For example, Lambda usually takes under 500ms to create a new instance to handle a request (also called a *cold start*). Once the function returns, the instance continues running and immediately handles the next request that arrives. If there are no requests for about ten seconds, it shuts down, and the next request is subject to the 500ms instance startup latency. Also, if thereâ€™s an existing instance handling a request and a second request arrives while the existing instance is busy, a second instance is cold-started.
- **Resource limits**: FaaS providers usually limit how much memory and CPU can be used and how long the function can run. One of the more flexible providers is Lambda. By default, it limits memory and duration to 128 MB and 3 seconds. The limits can be raised to a maximum 3,008 MB and 15 minutes, which costs more. CPU speed scales linearly with memory size.

An example of an application that isnâ€™t well-suited to FaaS is a [Meteor](https://www.meteor.com) app, which: 

- Keeps a WebSocket open to every client.
- Stores in memory a cache of each clientâ€™s data.
- Can use a lot of CPU to determine what data updates to send to each client.

Apollo Server [doesnâ€™t yet support](https://github.com/apollographql/apollo-server/issues/2129) GraphQL subscriptions on Lambda. [`aws-lambda-graphql`](https://github.com/michalkvasnicak/aws-lambda-graphql) is a different GraphQL server that does support subscriptions on Lambda. Aside from subscriptions, FaaS is 
a great fit for GraphQL because:

- GraphQL only has a single route, so we only need one function.
- The only thing stored in memory between requests is the data source cache, and thatâ€™s easy to swap out with a Redis cache.

Since our app uses subscriptions, letâ€™s use Heroku, a PaaS that supports Node. 

Itâ€™s worth noting that another option would be to split our application layer between two servers: 

- One that handles Queries and Mutations over HTTP, hosted on a FaaS.
- One that handles Subscriptions over WebSockets, hosted on a PaaS.

The former could publish subscription events to Redis, which the latter could subscribe to.

### Deploying

> If youâ€™re jumping in here, `git checkout 25_0.1.0` (tag [25_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/25_0.1.0), or compare [25...26](https://github.com/GraphQLGuide/guide-api/compare/25_0.1.0...26_0.1.0))

In this section weâ€™ll deploy our server to the Heroku PaaS, see how it breaks, and then fix it ðŸ¤“. 

We start by [creating an account](https://signup.heroku.com/dc). Then we do the following steps:

```sh
$ cd guide-api/
$ brew install heroku/brew/heroku
$ heroku login
$ heroku create
$ git push heroku 25:master
$ heroku open
```

1. `brew install heroku/brew/heroku`â€”Install the `heroku` command-line tool.
2. `heroku login`â€”Log in using the account we just created.
3. `heroku create`â€”Create a new Heroku app. This registers our server with Heroku and reserves a name (which is used in the deployed URL: `https://app-name.herokuapp.com/`). It also adds a Git remote named `heroku`.
4. `git push heroku 25:master`â€”Git push to the master branch of the Heroku remote. When Heroku receives the updated code, it builds and runs the server. This command assumes we currently have branch 25 checked out on our machine. If we were on `master`, we could just run `git push heroku master`.
5. `heroku open`â€”Open the deployed URL in the browser.

On the page thatâ€™s opened (`https://app-name.herokuapp.com/`), we see â€œApplication error,â€ which we can investigate by viewing the logs:

```sh
$ heroku logs
```

This prints a lot of logs, including:

```
2019-10-30T12:50:33.923678+00:00 heroku[web.1]: Error R10 (Boot timeout) -> Web process failed to bind to $PORT within 60 seconds of launch
2019-10-30T12:50:33.951435+00:00 heroku[web.1]: Stopping process with SIGKILL
```

When Heroku runs our code, it provides a `PORT` environment variable and waits for our code to start a server on that port. If our code doesnâ€™t do so within a minute, Heroku kills the process. Weâ€™re running our server on port 4000, so it killed us. ðŸ’€ðŸ˜ž

To resolve this problem, letâ€™s update our code to use `PORT`:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...26_0.2.0)

```js
server
  .listen({ port: process.env.PORT || 4000 })
  .then(({ url }) => console.log(`GraphQL server running at ${url}`))
```

We fall back to `4000` in development, where there is no `PORT` environment variable. Now to test, we can run `heroku logs --tail` in one terminal (`--tail` keeps the command running, displaying log lines in real time) and deploy in another. Since the deployment process for Heroku is `git push`, we have to create a new commit, so that the updated code is part of the push.

```sh
$ git add src/index.js
$ git commit -m 'Listen on process.env.PORT in production'
$ git push heroku 25:master
```

After the last command, we should start seeing log lines like this (plus timestamps) in the first terminal:

```
$ heroku logs --tail
...
app[api]: Build started by user  loren@graphql.guide
heroku[web.1]: State changed from crashed to starting
app[api]: Release v4 created by user  loren@graphql.guide
app[api]: Deploy 4f2d2e92 by user  loren@graphql.guide
app[api]: Build succeeded
heroku[web.1]: Starting process with command `npm start`
app[web.1]: 
app[web.1]: > guide-api@0.1.0 start /app
app[web.1]: > node dist/index.js
app[web.1]: 
app[web.1]: GraphQL server running at http://localhost:7668/
app[web.1]: (node:23) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [localhost:27017] on first connect [Error: connect ECONNREFUSED 127.0.0.1:27017
heroku[web.1]: State changed from starting to up
app[web.1]: Error: GraphQL Error (Code: 401): {"response":{"message":"Bad credentials","documentation_url":"https://developer.github.com/v4","status":401},"request":{"query":"\nquery GuideStars {\n  repository(owner: \"GraphQLGuide\", name: \"guide\") {\n    stargazers {\n      totalCount\n    }\n  }\n}\n"}}
```

Heroku didnâ€™t kill us! ðŸŽ‰ðŸ’ƒ 

> We can kill the logs process by hitting `Ctrl-C`.

The label `[web.1]` identifies which *dyno* (Herokuâ€™s term for a container) the log comes from. By default, our app only has one dyno, but we could scale up to multiple if we wanted. The lines labeled `heroku` are the dynoâ€™s general state changes:

```
heroku[web.1]: State changed from crashed to starting
heroku[web.1]: Starting process with command `npm start`
heroku[web.1]: State changed from starting to up
```

The lines labeled `app` are more granular and include all the output from our server process. The last two lines are errors that weâ€™ll fix in the next two sections:

```
app[web.1]: (node:23) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [localhost:27017] on first connect [Error: connect ECONNREFUSED 127.0.0.1:27017

app[web.1]: Error: GraphQL Error (Code: 401): {"response":{"message":"Bad credentials","documentation_url":"https://developer.github.com/v4","status":401},"request":{"query":"\nquery GuideStars {\n  repository(owner: \"GraphQLGuide\", name: \"guide\") {\n    stargazers {\n      totalCount\n    }\n  }\n}\n"}}
```

### Environment variables

> If youâ€™re jumping in here, `git checkout 26_0.1.0` (tag [26_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/26_0.1.0)).

There are a couple outstanding errors with our deployment. Letâ€™s look at this one:

```
app[web.1]: Error: GraphQL Error (Code: 401): {"response":{"message":"Bad credentials","documentation_url":"https://developer.github.com/v4","status":401},"request":{"query":"\nquery GuideStars {\n  repository(owner: \"GraphQLGuide\", name: \"guide\") {\n    stargazers {\n      totalCount\n    }\n  }\n}\n"}}
```

Itâ€™s an error response from our `GuideStars` query which our server is sending to GitHubâ€™s API. The error message is `Bad credentials`. Credentials are provided in the authorization header:

[`src/data-sources/Github.js`](https://github.com/GraphQLGuide/guide-api/blob/26_0.2.0/src/data-sources/Github.js)

```js
const githubAPI = new GraphQLClient('https://api.github.com/graphql', {
  headers: {
    authorization: `bearer ${process.env.GITHUB_TOKEN}`
  }
})
```

The problem is the `GITHUB_TOKEN` *environment variable* (env var) isnâ€™t defined, because our `.env` file isnâ€™t in Git, which means Heroku didnâ€™t get a copy of the file when we did `git push`. To set environment variables, PaaS and FaaS providers have a web UI and/or command-line tool. Heroku has bothâ€”letâ€™s fix our problem with its command-line tool:

```
$ heroku config:set GITHUB_TOKEN=...
Setting GITHUB_TOKEN and restarting â¬¢ graphql-guide... done, v5
GITHUB_TOKEN: ...
```

> Replace `...` with the value from our `.env` file.

Then, Heroku restarts the server to provide the new environment variable. We can now see with `heroku logs` that the `Bad credentials` error doesnâ€™t appear after the restart. 

We need to also set our other environment variable from `.env`:

```
$ heroku config:set SECRET_KEY=...
```

## Database hosting

* [MongoDB hosting](11.md#mongodb-hosting)
* [Redis hosting](11.md#redis-hosting)
  * [Redis PubSub](11.md#redis-pubsub)
  * [Redis caching](11.md#redis-caching)

### MongoDB hosting

> If youâ€™re jumping in here, `git checkout 26_0.1.0` (tag [26_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/26_0.1.0), or compare [26...27](https://github.com/GraphQLGuide/guide-api/compare/26_0.1.0...27_0.1.0))

Our last error is:

```
app[web.1]: (node:23) UnhandledPromiseRejectionWarning: MongoNetworkError: failed to connect to server [localhost:27017] on first connect [Error: connect ECONNREFUSED 127.0.0.1:27017
```

The error is coming from MongoDB, which weâ€™re setting up with:

[`src/db.js`](https://github.com/GraphQLGuide/guide-api/blob/26_0.2.0/src/db.js)

```js
const URL = 'mongodb://localhost:27017/guide'

export const connectToDB = async () => {
  const client = new MongoClient(URL, { useNewUrlParser: true })
  await client.connect()
  db = client.db()
  return client
}
```

In production, `localhost` is our Heroku container, which doesnâ€™t have a MongoDB database server running on it. We need a place to host our database, and then we can use that URL instead of `mongodb://localhost:27017/guide`. 

We have similar options to our Node [deployment options](#options): on-prem, IaaS, and DBaaS (similar to PaaS). Most people choose DBaaS because it requires the least amount of effort. With on-prem, weâ€™d have to house the machines, and with IaaS, weâ€™d have to configure and manage the OS and database software ourselves. MongoDB, Inc. runs their own DBaaS called [Atlas](https://www.mongodb.com/cloud/atlas).

Letâ€™s use the Atlas free plan to get a production MongoDB server. During setup, we have a choice of which cloud provider we want our database to be hosted on: AWS, Google Cloud Platform, or Microsoft Azure. Within the cloud provider, we also need to choose a region:

![List of AWS regions with us-east-1 selected](img/atlas-regions.png)

As discussed in the [Latency](bg.md#latency) background section, we want to pick the provider and region closest to our Heroku GraphQL server so that our GraphQL server can reach the database quickly. 

Here are all the Heroku regions:

```sh
$ heroku regions
ID         Location                 Runtime
â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
eu         Europe                   Common Runtime
us         United States            Common Runtime
dublin     Dublin, Ireland          Private Spaces
frankfurt  Frankfurt, Germany       Private Spaces
oregon     Oregon, United States    Private Spaces
sydney     Sydney, Australia        Private Spaces
tokyo      Tokyo, Japan             Private Spaces
virginia   Virginia, United States  Private Spaces
```

Our server is in the default region, `us`. We can look up more information about `us` using Herokuâ€™s API:

```sh
$ curl -n -X GET https://api.heroku.com/regions/us -H "Accept: application/vnd.heroku+json; version=3"
{
  "country":"United States",
  "created_at":"2012-11-21T20:44:16Z",
  "description":"United States",
  "id":"59accabd-516d-4f0e-83e6-6e3757701145",
  "locale":"Virginia",
  "name":"us",
  "private_capable":false,
  "provider":{
    "name":"amazon-web-services",
    "region":"us-east-1"
  },
  "updated_at":"2016-08-09T22:03:28Z"
}
```

Under the `provider` attribute, we can see that the Heroku `us` region is hosted on AWSâ€™s `us-east-1` region. So letâ€™s pick `AWS` and `us-east-1` for our Atlas database hosting location. Now it will take less than a millisecond for our GraphQL server to talk to our database.

After a few minutes, our cluster has been created, and we can click the â€œConnectâ€ button:

![Atlas dashboard with our cluster](img/atlas-cluster.png)

The first step is â€œWhitelist your connection IP address.â€ IP *safelisting* (formerly known as â€œwhitelistingâ€) only allows certain IP addresses to connect to the database. The IP address we want to be able to connect to the database is the IP of our GraphQL server. However, our Heroku dynos have different IPs, and the [IPs of `us-east-1`](https://docs.aws.amazon.com/general/latest/gr/aws-ip-ranges.html) change over time. And, even if they were static, it wouldnâ€™t be very secure to list them, as an attacker could rent a machine in `us-east-1` to run their code on. As an alternative, we could use a [Heroku add-on](https://elements.heroku.com/addons/categories/network) to provide a static outbound IP address for all of our dynos, but, for now letâ€™s go the easy and less secure route of safelisting all IP addresses. Use `0.0.0.0/0` to denote the range of all addresses.

> This issue isnâ€™t specific to Heroku or MongoDBâ€”it applies to any database thatâ€™s used by any server platform with shared IP addresses.

Next weâ€™ll create a username and password. On the â€œChoose a connection methodâ€ step, we choose â€œConnect your applicationâ€ and copy the connection string, which looks like this:

```
mongodb+srv://<username>:<password>@cluster0-9ofk6.mongodb.net/test?retryWrites=true&w=majority
```

The `cluster0-*****.mongodb.net` domain is the domain of our new MongoDB cluster, which can contain multiple databases. The `/test?` part determines the default database. Letâ€™s change ours to `/guide?`. We also need to replace `<username>` and `<password>` with the user we created.

Then we can set our URL as an environment variable:

```
$ heroku config:set MONGO_URL="mongodb+srv://***:***@cluster0-*****.mongodb.net/guide?retryWrites=true&w=majority"
```

And finally, we can reference it in the code:

[`src/db.js`](https://github.com/GraphQLGuide/guide-api/compare/26_0.2.0...27_0.2.0)

```js
const URL = process.env.MONGO_URL || 'mongodb://localhost:27017/guide'
```

At this point, our new database is empty. We can either recreate our user document using Compass or run this command to copy all our users and reviews from our local database to the production database:

`mongodump --archive --uri "mongodb://localhost:27017/guide" | mongorestore --archive --uri "mongodb+srv://..."`

> Replace `mongodb+srv://...` with your URL.

After we commit and push to Heroku, we can see our server is error-free! ðŸ’ƒ

```sh
$ heroku logs
heroku[web.1]: Starting process with command `npm start`
app[web.1]: 
app[web.1]: > guide-api@0.1.0 start /app
app[web.1]: > node dist/index.js
app[web.1]: 
app[web.1]: GraphQL server running at http://localhost:33029/
heroku[web.1]: State changed from starting to up
```

### Redis hosting

Background: [Redis](bg.md#redis)

> If youâ€™re jumping in here, `git checkout 27_0.1.0` (tag [27_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/27_0.1.0), or compare [27...28](https://github.com/GraphQLGuide/guide-api/compare/27_0.1.0...28_0.1.0))

There are two parts of our app that are only meant to run in development, and we need to change for production:

- Apollo Serverâ€™s included `PubSub` implementation, which we use for subscriptions.
- Apollo Serverâ€™s default cache, which is used by data sources.

Both of these things were designed to work when the server runs as a single continuous process. In production, there are usually multiple processes/containers/servers, PaaS containers are subject to being restarted, and FaaS definitely isnâ€™t continuous ðŸ˜„.

To get ready for production, letâ€™s use a `PubSub` implementation and cache library that were designed for [Redis](bg.md#redis), the most popular caching (in-memory) database. 

#### Redis PubSub

Our current `PubSub` comes from `apollo-server`:

[`src/util/pubsub.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import { PubSub } from 'apollo-server'

export const pubsub = new PubSub()
```

There are many `PubSub` implementations for different databases and queues (see [Apollo docs > Subscriptions > PubSub Implementations](https://www.apollographql.com/docs/apollo-server/data/subscriptions/#pubsub-implementations)). Weâ€™ll use `RedisPubSub` from [`graphql-redis-subscriptions`](https://github.com/davidyaha/graphql-redis-subscriptions) when weâ€™re in production:

```js
import { PubSub } from 'apollo-server'
import { RedisPubSub } from 'graphql-redis-subscriptions'

import { getRedisClient } from './redis'

const inProduction = process.env.NODE_ENV === 'production'

const productionPubSub = () => new RedisPubSub({
  publisher: getRedisClient(),
  subscriber: getRedisClient()
})

export const pubsub = inProduction ? productionPubSub() : new PubSub()
```

We have the same line checking `NODE_ENV` in `formatError.js`, so letâ€™s deduplicate by adding a new file:

[`src/env.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
export const inProduction = process.env.NODE_ENV === 'production'
```

[`src/formatError.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import { inProduction } from './env'
```

[`src/util/pubsub.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import { inProduction } from '../env'
```

The one piece we havenâ€™t seen yet is `getRedisClient`:

[`src/util/redis.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import Redis from 'ioredis'

const { REDIS_HOST, REDIS_PORT, REDIS_PASSWORD } = process.env

const options = {
  host: REDIS_HOST,
  port: REDIS_PORT,
  password: REDIS_PASSWORD,
  retryStrategy: times => Math.min(times * 50, 1000)
}

export const getRedisClient = () => new Redis(options)
```

We use our preferred Redis client library, [`ioredis`](https://www.npmjs.com/package/ioredis). The `retryStrategy` function returns how long to wait (in milliseconds) before trying to reconnect to the server when the connection is broken.

We need a public Redis server to connect to. For that, weâ€™ll use Redis Labs, the sponsor of Redis. They have a DBaaS, and it includes a [free 30MB tier](https://redislabs.com/redis-enterprise-cloud/essentials-pricing/) we can use. During sign-up, we have to choose a cloud provider and region (weâ€™ll use AWS and us-east-1, since thatâ€™s where our GraphQL server is hosted), as well as an eviction policy: `allkeys-lfu`. An eviction policy determines which keys get deleted when the 30MB of memory is full, and `lfu` stands for least frequently used. 

Once weâ€™ve signed up, weâ€™ll have connection info like this:

`.env`

```
REDIS_HOST=redis-10042.c12.us-east-1-4.ec2.cloud.redislabs.com
REDIS_PORT=10042
REDIS_PASSWORD=abracadabra
```

Once the info is added to our `.env` file, our `getRedisClient()` function (and our pubsub system) should start working. 

> We can check to make sure itâ€™s connecting to the right Redis server by turning on debug output: in the `dev` script in our `package.json`, add `DEBUG=ioredis:* ` before `babel-node src/index.js`.

We can also test our new Redis-backed pubsub by making a subscription in Playground, unstarring and starring the repo [on GitHub](https://github.com/GraphQLGuide/guide), and confirming that two events appear:

![githubStars subscription in Playground with two events](img/githubStars-subscription.png)

#### Redis caching

Apollo Serverâ€™s default cache for data sources is an in-memory LRU cache (*LRU* means that when the cache is full, the *least recently used* data gets evicted). To ensure our data source classes across multiple containers have the same cached data, weâ€™ll switch to a Redis cache. The ['apollo-server-cache-redis'](https://www.npmjs.com/package/apollo-server-cache-redis) library provides `RedisCache`:

[`src/util/redis.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import Redis from 'ioredis'
import { RedisCache } from 'apollo-server-cache-redis'

const { REDIS_HOST, REDIS_PORT, REDIS_PASSWORD } = process.env

const options = {
  host: REDIS_HOST,
  port: REDIS_PORT,
  password: REDIS_PASSWORD,
  retryStrategy: times => Math.min(times * 50, 1000)
}

export const getRedisClient = () => new Redis(options)

export const cache = new RedisCache(options)

export const USER_TTL = { ttl: 60 * 60 } // hour
```

We added the `cache` and `USER_TTL` exports. Now we can add `cache` to the `ApolloServer` constructor:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import { cache } from './util/redis'

const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context,
  formatError,
  cache
})
```

To use caching, we have to set a *TTL* (time to live) with our calls to `findOneById`. This argument denotes how many seconds an object will be kept in the cache, during which calls to `findOneById` with the same ID will return the cached object instead of querying the database.

We choose a TTL based on our app requirements and how often our objects change. Our user documents rarely change, and it wouldnâ€™t be a big deal for one to be less than an hour out of date after a change, so we can set the TTL for user documents to an hour (60 * 60 seconds). Weâ€™re not currently using `findOneById` for reviews, but if we did, we might use a lower TTLâ€”maybe a minuteâ€”if we want users to be able to edit their reviews and see those changes reflected in the app sooner.

Now letâ€™s add `USER_TTL` to our `User` and `Review` resolvers:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import { USER_TTL } from '../util/redis'

export default {
  Query: {
    me: ...
    user: (_, { id }, { dataSources }) => {
      try {
        return dataSources.users.findOneById(ObjectId(id), USER_TTL)
      } catch (error) {
        if (error.message === OBJECT_ID_ERROR) {
          throw new InputError({ id: 'not a valid Mongo ObjectId' })
        } else {
          throw error
        }
      }
    },
    searchUsers: ...
  },
  ...
}
```

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/27_0.2.0...28_0.2.0)

```js
import { USER_TTL } from '../util/redis'

export default {
  Query: {
    reviews: ...
  },
  Review: {
    id: ...
    author: (review, _, { dataSources }) =>
      dataSources.users.findOneById(review.authorId, USER_TTL),
    fullReview: async (review, _, { dataSources }) => {
      const author = await dataSources.users.findOneById(
        review.authorId,
        USER_TTL
      )
      return `${author.firstName} ${author.lastName} gave ${review.stars} stars, saying: "${review.text}"`
    },
    createdAt: ...
  },
  ...
}
```

Now after we make a query like `{ reviews { fullReview } }`, we should be able to see a user object stored in Redis. To view the databaseâ€™s contents, we can use the command line (`brew install redis` and then `redis-cli -h`) or a GUI like [Medis](http://getmedis.com/):

![Cache key with corresponding user object](img/redis-cached-user.png)

The cache key has the format `mongo-[collection]-[id]`, and the value is a string, formatted by Medis as JSON. We can also see the remaining TTL on the bottom right.

Finally, letâ€™s get Redis working in production. We update our environment variables on Heroku with:

```sh
$ heroku config:set \
REDIS_HOST=redis-10042.c12.us-east-1-4.ec2.cloud.redislabs.com \
REDIS_PORT=10042 \
REDIS_PASSWORD=abracadabra
```

And we push the latest code:

```sh
$ git commit -am 'Add Redis pubsub and caching'
$ git push heroku 27:master
```

Weâ€™ll learn in the next section how to query our production API. For now, we can test our Redis in production by deleting the `mongo-users-foo` key, making the same `{ reviews { fullReview } }` query, and then refreshing Medis to ensure the key has been recreated.

### Querying in production

> If youâ€™re jumping in here, `git checkout 28_0.1.0` (tag [28_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/28_0.1.0)).

Now when we visit our `app-name.herokuapp.com`, instead of â€œApplication errorâ€ we see:

```
GET query missing.
```

Usually GraphQL requests are sent by POST, but Apollo Server also supports receiving GET requests. The browser is making a `GET /` request when we load the page, but the format that Apollo supports is `GET /?query=X`. Letâ€™s test it with the `{ hello }` query:

[`app-name.herokuapp.com/?query={hello}`](https://graphql-guide.herokuapp.com/?query={hello})

![Webpage showing JSON GraphQL response](img/hello-get-query.png)

This method of querying our production server works, but it becomes inconvenient if queries are large or use variables, and we canâ€™t add an authorization header. The method we were using before, GraphQL Playground, is disabled by default in production. However, we can use the [Playground app](https://github.com/prisma-labs/graphql-playground/releases) (download the latest `.dmg` or `.exe` file) to query any GraphQL API. First, we select â€œURL ENDPOINTâ€ and enter our production URL:

![Playground appâ€™s â€œNew Workspaceâ€ screen](img/playground-app-url.png)

And then we query:

![hello query and response in Playground app](img/playground-app-hello.png)

While the query returns a response, we see the â€œServer cannot be reachedâ€ error. Query autocompletion doesnâ€™t work, and the schema tab doesnâ€™t load. These issues occur because *introspection*â€”the queries that return the schemaâ€”is disabled by default in production in order to obscure private APIs.

> *Private* APIs are meant to be used only by the companyâ€™s own applications, versus public APIs like the [GitHub API](https://developer.github.com/v4/) that are meant to be used by third parties.

If we were publishing a public API that we wanted third-party apps to query, we would want to enable at least introspection (and probably Playground as well) to make it easier for the third-party developers to query our API. 

We can enable both introspection and Playground in production by adding the last two options below:

`src/index.js`

```js
const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context,
  formatError,
  introspection: true,
  playground: true
})
```

```
$ git add src/index.js
$ git commit -m 'Enable introspection and Playground'
$ git push heroku 26:master
```

Now we can view the schema in the Playground app, and if we visit our index URL, the Playground website will load:

[`app-name.herokuapp.com/`](https://graphql-guide.herokuapp.com/)

If we want to undo the change, we can do:

```
$ git reset HEAD^
$ git checkout -- src/index.js
$ git push heroku 26:master -f
```

We need the `-f` (force push). A normal push will fail because the `heroku` remoteâ€™s `master` branch is in a different state from our branch `26` (`heroku` still has the â€œEnable introspection and Playgroundâ€ commit as the branch tip).

In summary, the ways we can interactively query our production GraphQL server are:

- `GET /?query=X`
- Playground app without introspection
- Playground app with introspection (the server must have introspection enabled)
- Playground website, if the server has it enabled

And we can, of course, continue to query it with POST requests on the command line or in code.

## Analytics

> If youâ€™re jumping in here, `git checkout 28_0.1.0` (tag [28_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/28_0.1.0)).

There are different types of server analytics that can be useful to track and a variety of tools that gather and display data. In this section, weâ€™ll cover a few of each.

The types of analytics can be split into two categories: those at the operating system (OS) level, and those at the code level. 

At the OS level, there are:

- Memory usage
- CPU usage
- Request statistics, which include:
  - Rate (e.g. 1000 requests received per second)
  - Response times (e.g. 95% of responses are sent within 100ms of receiving the request)
  - Error rates (e.g., 1% of responses have an HTTP code in the 500-599 range)

The code level can also measure things based on details in the code: for instance, [Express](https://en.wikipedia.org/wiki/Express.js) route names or GraphQL field usage. A popular *application performance management* (APM) tool that can do code-level measurement is [New Relic], which has an npm library that tracks requests by route name for a list of [supported frameworks](https://docs.newrelic.com/docs/agents/nodejs-agent/getting-started/compatibility-requirements-nodejs-agent) like Express. It also can monitor the performance of calls to several different databases, and it provides functions for custom instrumentation/metrics. 

The main APM tool for GraphQL servers is Apolloâ€™s Graph Manager, which tracks the request statistics listed above, as well as:

- Queries received
- Fields selected
- Resolver timelines
- Clients
- Deprecated field usage
- GraphQL errors

For memory and CPU usage, we could either use Herokuâ€™s [built-in metrics](https://devcenter.heroku.com/articles/metrics#metrics-gathered-for-all-dynos) or New Relic. However, these OS-level metrics are becoming less important, given the prevalence of autoscaling (where the PaaS automatically adds more containers when under a high load) and serverless (where we usually donâ€™t have to think about memory and CPU).

For the rest of the metrics, letâ€™s set up Graph Manager. First, we [sign up](https://engine.apollographql.com/signup), and then weâ€™re given an API key to set for the `ENGINE_API_KEY` env variable:

`.env`

```
ENGINE_API_KEY=service:guide-api:*****
```

We now start our server with `npm run dev`. Once it has finished starting up, we run this command in a new terminal:

```
$ npx apollo service:push --endpoint="http://localhost:4000"
```

This command sends Apollo our schema, which is used for GraphQL analytics and other Graph Manager features like [schema change validation](#schema-validation). When we change our schema, we need to re-run the command. Usually this is done automatically as part of [continuous integration](bg.md#continuous-integration) ([CircleCI example](https://www.apollographql.com/docs/graph-manager/schema-registry/#registering-a-schema-via-continuous-delivery)).

Now we can make queries in Playground, reload [Graph Manager](https://engine.apollographql.com/), select â€œMetricsâ€ from the menu, and see server analytics!

If we only want to see production analytics, we can remove `ENGINE_API_KEY` from `.env` and set it on Heroku:

```
$ heroku config:set ENGINE_API_KEY="service:guide-api:*****"
```

Hereâ€™s an example metrics dashboard:

![Metrics page of Graph Manager](img/graph-manager-metrics.png)

We see:

- A low total request rate of 0.094 rpm (requests per minute). The operation with the highest request rate (0.083 rpm) begins with `fragment FullType`, and it has 120 total requests, which we can see on the right in the Filters sidebar.
- A low p95 service time of 17.7ms, which means 95% of requests are responded to within 17.7ms.
- A high error rate of 92.65%. Most of the errors come from the `fragment FullType` operation, which is sent by Playground to request the schema (and fails because introspection is disabled on this production server).
- The request rate over time, and after we scroll down, request latency over time and request latency distribution.

We can also see how difficult it is to differentiate unnamed queriesâ€”for instance, the four different `searchUsers` queries. To see which query has the second-slowest service time, weâ€™d need to select it and then click on the â€œOperationâ€ tab:

![Unnamed operation in Graph Manager](img/graph-manager-unnamed-operation.png)

The â€œTracesâ€ tab shows us the timeline of when resolvers are called and how long they take to complete. Hereâ€™s a `reviews` query and its trace:

```gql
{
  reviews {
		text
    stars
    author {
      firstName
    }
  }
}
```

![Trace in Graph Manager](img/graph-manager-trace.png)

The `reviews` resolver fetches the list of reviews, which takes 3.57ms, and then Apollo Server calls `Review.*` field resolvers, starting with the first review (`reviews.0` in the trace), and ending with `reviews.11`, which is expanded so that we can see the timing of the field resolvers. `Review.text` and `Review.stars` return immediately, since theyâ€™re just fields on the review object, but `Review.author` requires a database lookup. That lookup is actually done in a single query for all reviews 0â€“11, as all the reviews have the same author and our datasource uses Dataloader, which deduplicates the 12 identical author queries. The query takes 2.76ms, after which the `User.firstName` resolver returns immediately, and the entire query response is ready to send to the client.

The Filters sidebar lets us filter by time range or by operation, but we can also filter by client type and version. To do that, we select â€œClientsâ€ from the left sidebar. Now clients are listed on the left half of the page. Currently we only see one labeled â€œUnidentified clientsâ€ and â€œAll versions.â€ Thatâ€™s because none of our clients have identified themselves yet. They can do so by setting two headers, `apollographql-client-name` (like â€œwebappâ€, â€œiOS-appâ€, â€œmarketing-scriptâ€, etc.) and `apollographql-client-version` (like `0.1.0`, `v2`, etc.). 

Letâ€™s open the HTTP headers section of Playground and enter these:

```
{
  "apollographql-client-name":"playground-test",
  "apollographql-client-version":"0.1.0"
}
```

> When using Apollo Client, we can use the `name` and `version` constructor options: `new ApolloClient({ link, cache, name: 'web', version: '1.0' })`.

Then, if we run a query, change the version, run more queries, and refresh Graph Manager, weâ€™ll see the new client type with two versions:

![Clients page of Graph Manager](img/graph-manager-clients.png)

Selecting a version and then an operation on the right takes us to the metrics page of that query for that client version. We can also look at other operations used by that client in the Filters sidebar.

## Error reporting

> If youâ€™re jumping in here, `git checkout 28_0.1.0` (tag [28_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/28_0.1.0), or compare [28...29](https://github.com/GraphQLGuide/guide-api/compare/28_0.1.0...29_0.1.0))

In this section weâ€™ll look at what kind of error reporting Graph Manager covers, and then weâ€™ll look at a dedicated error reporting service.

In the [last section](#analytics) we set up Apolloâ€™s Graph Manager and looked at its analytics. The one tab of the Metrics page we didnâ€™t get to is the Errors tab:

![General errors page of Graph Manager](img/graph-manager-errors.png)

The general errors page (without an operation selected) shows a timeline of total error count, followed by a list of all errors within the current time range, grouped by where they occurredâ€”either in a specific resolver, like the `user.email` errors at the bottom, or before the server starts calling resolvers (labeled as â€œoutside of the GraphQL contextâ€ above). The latter category often includes failures parsing or validating the requestâ€™s operation. In this example, the validation fails because the operation includes a `__schema` root Query field, but the field is not in the schema because introspection is turned off. 

We can expand the instances links to get a list of times and operations in which the error occurred:

![Error instances expanded](img/graph-manager-error-instances.png)

And when we have an operation selected, the Errors tab only shows us errors that occurred during the execution of that operation.

There are a few features that Graph Manager doesnâ€™t have that would be useful:

- Stack traces
- The contents of the `extensions` field of the GraphQL error (above we only see the `message` field)
- The ability to attach further information, like the current user
- The ability to ignore errors or mark them as fixed
- Team features like the ability to attach notes or assign errors to people
- The ability to search through the errors

There are a few error-tracking services that provide these features. Weâ€™ll set up [Sentry](https://sentry.io/)â€”one of the most popular onesâ€”but setting up another service would work similarly.

First we [create an account](https://sentry.io/signup/), and then we create our first Sentry project, choosing Node.js as the project type. Weâ€™re given a statement like `Sentry.init({ dsn: 'https:://...' })` with our new projectâ€™s ID filled in, which we paste into our code:

[`src/formatError.js`](https://github.com/GraphQLGuide/guide-api/compare/28_0.2.0...29_0.2.0)

```js
import * as Sentry from '@sentry/node'

Sentry.init({
  dsn: 'https://ceb14feec00b4c49bebd10a9674bb68d@sentry.io/5168151'
})
```

Now Sentry automatically gathers uncaught errors like this one:

```js
Sentry.init({
  dsn: 'https://ceb14feec00b4c49bebd10a9674bb68d@sentry.io/5168151'
})

myUndefinedFunction()
```

Within seconds of `npm run dev`, we should see a new error in our Sentry dashboard:

![The error detail page of the Sentry web app](img/sentry-uncaught-error.png)

We see the time, error message, stack trace, and line of code. And if the same error happens again, it will be grouped with this one so that we can see the total number of occurrences and graph occurrences over time.

This is all really useful, but the issue is that Apollo Server catches all errors that occur during GraphQL requests, which is where most of our errors will occur. Since Sentry is only gathering uncaught errors, it misses most of our errors. To tell Sentry about those errors, we can use one of two `ApolloServer()` options:

- `formatError` function
- `plugins` array with a new plugin we write

The first is simpler, and weâ€™re already using it:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/blob/28_0.2.0/src/index.js)

```js
import formatError from './formatError'

const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context,
  formatError,
  cache
})
```

[`src/formatError.js`](https://github.com/GraphQLGuide/guide-api/compare/28_0.2.0...29_0.2.0)

```js
export default error => {
  if (inProduction) {
    // send error to tracking service
  } else {
    console.log(error)
    console.log(get(error, 'extensions.exception.stacktrace'))
  }

  const name = get(error, 'extensions.exception.name') || ''
  if (name.startsWith('Mongo')) {
    return new InternalServerError()
  } else {
    return error
  }
}
```

Weâ€™re currently using the `formatError()` function to log errors in development and mask errors involving MongoDB. We can call `Sentry.captureException()` to tell Sentry about errors:

```js
import get from 'lodash/get'
import * as Sentry from '@sentry/node'
import { AuthenticationError, ForbiddenError } from 'apollo-server'

import { InternalServerError, InputError } from './util/errors'

const NORMAL_ERRORS = [AuthenticationError, ForbiddenError, InputError]
const NORMAL_CODES = ['GRAPHQL_VALIDATION_FAILED']
const shouldReport = e =>
  !NORMAL_ERRORS.includes(e.originalError) &&
  !NORMAL_CODES.includes(get(e, 'extensions.code'))

export default error => {
  if (inProduction) {
    if (shouldReport(error)) {
      Sentry.captureException(error.originalError)
    }
  } else {
    console.log(error)
    console.log(get(error, 'extensions.exception.stacktrace'))
  }

  ...
}
```

The `error` the function receives is the GraphQL error thatâ€™s included in the response to the client. To get the Node.js error object (which is what Sentry expects), we do `error.originalError`. We also use `shouldReport()` to avoid reporting normal errors, like auth and query format errors, since we donâ€™t need to track and fix them. 

> If we had a public API, we might want to track query-parsing errors in case we find that developers consistently make certain mistakes, in which case we could try to improve our schema or documentation.

To test, we can run `NODE_ENV=production npm run dev` and add an error to `Query.hello`:

[`src/resolvers/index.js`](https://github.com/GraphQLGuide/guide-api/compare/28_0.2.0...29_0.2.0)

```js
const resolvers = {
  Query: {
    hello: () => 'ðŸŒðŸŒðŸŒŽ' && myUndefinedFunction(),
    isoString: (_, { date }) => date.toISOString()
  }
}
```

![Sentry website with a list of 2 errors](img/sentry-formatError.png)

We can see the error message is the same, but the new entry shows a different function and file: `hello(resolvers:index)`.

If we want to track more information in Sentry, like details about the request and context (such as the current user), then we need to use a plugin instead of `formatError`. We use the `plugins` option:

```
const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context,
  formatError,
  cache,
  plugins: [sentryPlugin]
})
```

And we create `sentryPlugin` according to the [plugin docs](https://www.apollographql.com/docs/apollo-server/integrations/plugins/), defining the [`didEncounterErrors()`](https://www.apollographql.com/docs/apollo-server/integrations/plugins/#didencountererrors) method and using [`Sentry.withScope()`](https://docs.sentry.io/enriching-error-data/scopes/?platform=node#local-scopes).

One last thing to consider is that if our server is not runningâ€”if something happened to our Node.js process or our machineâ€”we wonâ€™t receive errors in Sentry. In many cases we wonâ€™t need to worry about this: for instance, a Node.js PaaS will automatically monitor and restart the process, and for a FaaS, itâ€™s irrelevant. But if it is relevant for our deployment setup, we can use an uptime / monitoring service that pings our server to see if itâ€™s still reachable over the internet and responsive. The URL we can use for that (as well as for a load balancer, if weâ€™re using one) is `/.well-known/apollo/server-health`, which should return status 200 and this JSON:

![Chrome navigated to the health check path on localhost:4000 and showing {status: "pass"}](img/health-check.png)

# More data sources

* [SQL](11.md#sql)
  * [SQL setup](11.md#sql-setup)
  * [SQL data source](11.md#sql-data-source)
  * [SQL testing](11.md#sql-testing)
  * [SQL performance](11.md#sql-performance)
* [REST](11.md#rest)
* [GraphQL](11.md#graphql)
* [Custom data source](11.md#custom-data-source)
* [Prisma](11.md#prisma)

There are lots of other sources of data out there we might want to use in our GraphQL servers, and when we want to query one, we use a *data source*. When we use the term â€œdata sourceâ€ in this chapter, weâ€™re usually talking about a JavaScript class that has Apolloâ€™s `DataSource` class as an ancestor, like the `MongoDataSource` we [used earlier](#data-sources). There are data sources on npm that others have written, and we can write our own. There are also alternatives, one of which weâ€™ll cover at the end called Prisma.

## SQL

Background: [SQL](bg.md#sql)

Contents:

* [SQL setup](11.md#sql-setup)
* [SQL data source](11.md#sql-data-source)
* [SQL testing](11.md#sql-testing)
* [SQL performance](11.md#sql-performance)

In this section we replace our use of MongoDB with SQL. In the first part weâ€™ll get our SQL database and table schemas set up. Then weâ€™ll replace our use of `MongoDataSource` with `SQLDataSource`. Then in [SQL testing](11.md#sql-testing), we update our tests, and, finally in [SQL performance](11.md#sql-performance), we improve our serverâ€™s database querying.

### SQL setup

> If youâ€™re jumping in here, `git checkout 25_0.1.0` (tag [25_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/25_0.1.0), or compare [25...sql](https://github.com/GraphQLGuide/guide-api/compare/25_0.1.0...sql_0.1.0))

A [SQL database](bg.md#sql) takes more setup than the MongoDB database weâ€™ve been using: We need to write *migrations*â€”code that creates or alters tables and their schemas. The most popular Node library for SQL is [Knex](https://knexjs.org/), and it includes the ability to write and run migrations. To start using it, we run `knex init`. Since we already have it in our `node_modules/`, we can run `npx knex init` in a new directory within our repository:

```sh
$ mkdir sql
$ cd sql/
$ npx knex init
```

This creates a config file:

[`sql/knexfile.js`](https://github.com/GraphQLGuide/guide-api/blob/sql_0.2.0/sql/knexfile.js)

```js
// Update with your config settings.

module.exports = {
  development: {
    client: 'sqlite3',
    connection: {
      filename: './dev.sqlite3'
    }
  },

  staging: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user: 'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  },

  production: {
    client: 'postgresql',
    ...
  }
}
```

By default, it uses **SQLite** and **PostgreSQL** (two types of SQL databases) for development and deployment, respectively.

One aspect of database setup thatâ€™s easier with SQL than MongoDB is running the database in developmentâ€”SQLite doesnâ€™t need to be installed with Homebrew and run as a service. Instead, it can be installed with a Node library and run off of a single file. So unless weâ€™re using a special feature that PostgreSQL supports but SQLite doesnâ€™t, we can use SQLite in development. 

We also wonâ€™t be deploying, so all we need is:

[`sql/knexfile.js`](https://github.com/GraphQLGuide/guide-api/blob/sql_0.2.0/sql/knexfile.js)

```js
module.exports = {
  development: {
    client: 'sqlite3',
    connection: {
      filename: './dev.sqlite3'
    },
    useNullAsDefault: true
  }
}
```

(We added `useNullAsDefault: true` to avoid a warning message.)

Now we can use Knex to create a migration that will set up our users and reviews tables:

```sh
$ npx knex migrate:make users_and_reviews
```

This generates a file in the following format:

[`sql/migrations/[timestamp]_users_and_reviews.js`](https://github.com/GraphQLGuide/guide-api/blob/sql_0.2.0/sql/migrations)

```js
exports.up = function(knex) {

}

exports.down = function(knex) {
  
}
```

Inside the `up` function, we create the two tables, and inside the `down` function, we *drop* (delete) them. To do all that, we use Knexâ€™s [schema-building API](https://knexjs.org/#Schema):

[`sql/migrations/20191228233250_users_and_reviews.js`](https://github.com/GraphQLGuide/guide-api/blob/sql_0.2.0/sql/migrations)

```js
exports.up = async knex => {
  await knex.schema.createTable('users', table => {
    table.increments('id')
    table.string('first_name').notNullable()
    table.string('last_name').notNullable()
    table.string('username').notNullable()
    table.string('email')
    table
      .string('auth_id')
      .notNullable()
      .unique()
    table.datetime('suspended_at')
    table.datetime('deleted_at')
    table.integer('duration_in_days')
    table.timestamps()
  })
}
```

- `knex.schema.createTable('users'` creates a table named `users`.
- `table.increments('id')` creates a primary index column named `id`. Itâ€™s auto-incrementing, which means the first record thatâ€™s inserted is given an `id` of 1, and the second record gets an `id` of 2, etc.
- `table.string('first_name').notNullable()` creates a `first_name` column that can hold a string and canâ€™t be null.
- `table.string('auth_id').notNullable().unique()` creates an `auth_id` non-nullable string column that has to be unique among all records in the table.
- `table.datetime('suspended_at')` creates a `suspended_at` column that can hold a datetime.
- `table.timestamps()` creates `created_at` and `updated_at` datetime columns.

Similarly, we can create the `reviews` table:

```js
exports.up = async knex => {
  await knex.schema.createTable('users', table => { ... })
  await knex.schema.createTable('reviews', table => {
    table.increments('id')
    table
      .integer('author_id')
      .unsigned()
      .notNullable()
      .references('id')
      .inTable('users')
    table.string('text').notNullable()
    table.integer('stars').unsigned()
    table.timestamps()
  })
}
```

The below part sets up a *foreign key constraint* on `author_id`, so the only values that can be stored in this column match an `id` field in the `users` table:

```js
    table
      .integer('author_id')
      .unsigned()
      .notNullable()
      .references('id')
      .inTable('users')
```

Finally, we call `dropTable()` in the `down` function:

[`sql/migrations/20191228233250_users_and_reviews.js`](https://github.com/GraphQLGuide/guide-api/blob/sql_0.2.0/sql/migrations)

```js
exports.up = async knex => {
  await knex.schema.createTable('users', table => { ... })
  await knex.schema.createTable('reviews', table => { ... })
}

exports.down = async knex => {
  await knex.schema.dropTable('users')
  await knex.schema.dropTable('reviews')
}
```

To run our migration `up` function, we use:

```sh
$ npx knex migrate:latest
```

And to undo, we would do `npx knex migrate:rollback --all`. If in the future we want to make a change to the schema, we would create another migration with a more recent timestampâ€”e.g., `[timestamp]_add_deleted_column_to_reviews.js`â€”that adds a `deleted` column to the `reviews` table, and commits it to git. Then, whenever a dev was on the version of the code that used the `reviews.deleted` column, they could migrate to the latest version of the database, and code that modifies a reviewâ€™s `deleted` field would work.

With MongoDB, we didnâ€™t have migrations, and we added or changed documents manually. With SQL, we could run migrations that drop our tables and everything in them, so re-inserting records manually would get tedious. So Knex supports *seed files* that we can run to automatically insert records. We start with `seed:make`, which creates an example seed file:

```sh
$ npx knex seed:make users
```

[`sql/seeds/users.js`](https://github.com/GraphQLGuide/guide-api/blob/sql_0.2.0/sql/seeds/users.js)

```js
exports.seed = function(knex) {
  // Deletes ALL existing entries
  return knex('table_name').del()
    .then(function () {
      // Inserts seed entries
      return knex('table_name').insert([
        {id: 1, colName: 'rowValue1'},
        {id: 2, colName: 'rowValue2'},
        {id: 3, colName: 'rowValue3'}
      ]);
    });
};
```

Now we modify the example file to use async/await and match our `users` table schema:

```js
exports.seed = async knex => {
  await knex('users').del()
  await knex('users').insert([
    {
      id: 1,
      firstName: 'John',
      lastName: 'Resig',
      username: 'jeresig',
      email: 'john@graphql.guide',
      authId: 'github|1615',
      created_at: new Date(),
      updated_at: new Date()
    }
  ])
}
```

And then copy the file for inserting reviews:

[`sql/seeds/reviews.js`](https://github.com/GraphQLGuide/guide-api/blob/sql_0.2.0/sql/seeds/reviews.js)

```js
exports.seed = async knex => {
  await knex('reviews').del()
  await knex('reviews').insert([
    {
      id: 1,
      author_id: 1,
      text: `Now that's a downtown job!`,
      stars: 5,
      created_at: new Date(),
      updated_at: new Date()
    },
    {
      id: 2,
      author_id: 1,
      text: 'Passable',
      stars: 3,
      created_at: new Date(),
      updated_at: new Date()
    }
  ])
}
```

We run the seed files with:

```sh
$ npx knex seed:run
```

We can view if it worked with either the command-line SQLite client or a GUI. The command-line client, `sqlite3`, is included by default on Macs. We give it the database file `sql/dev.sqlite3` as an argument, and then we can run SQL statements like `SELECT * FROM reviews;`.

```sh
$ sqlite3 sql/dev.sqlite3 
SQLite version 3.30.1 2019-10-10 20:19:45
Enter ".help" for usage hints.
sqlite> SELECT * FROM reviews;
1|1|Now that's a downtown job!|5|1578122461308|1578122461308
2|1|Passable|3|1578122461308|1578122461308
```

There are many SQL GUIs. Our favorite is [TablePlus](https://tableplus.com/), which works with not only different types of SQL databases, but other databases as well, including Redis and MongoDB. When creating a new connection, we select SQLite and then the file `sql/dev.sqlite3`, and hit Connect. Then on the left, we see the list of tables in our database, and if we double-click `reviews`, we see the tableâ€™s contents:

![TablePlus app with review records](img/tableplus-reviews.png)

Lastly, we no longer need to connect to a MongoDB database, so we can remove the call to `connectToDB()` in `src/index.js`.

Before we commit our changes, we want to add the below line to `.gitignore`:

```
sql/dev.sqlite3
```

We donâ€™t want our database in our code repositoryâ€”itâ€™s meant to be generated and modified by each individual developer using our migration and seed scripts.

### SQL data source

> If youâ€™re jumping in here, `git checkout sql_0.1.0` (tag [sql_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/sql_0.1.0), or compare [sql...sql2](https://github.com/GraphQLGuide/guide-api/compare/sql_0.1.0...sql2_0.1.0))

Now that weâ€™ve set up our SQL database and inserted records, we need to query them. So we look for a SQL data source class to use, either on the [community data sources list](https://www.apollographql.com/docs/apollo-server/data/data-sources/#community-data-sources) in the Apollo docs or by searching â€œapollo data source sqlâ€ on Google or npm. We find [`datasource-sql`](https://github.com/cvburgess/SQLDataSource/), which provides the class `SQLDataSource`. 

`SQLDataSource` is unusual among data sources in that:

- A single instance is created (versus a new instance for each request).
- It does caching only, not batching.

It also:

- recommends using a single class for the whole database, instead of a class per table as we did with `MongoDataSource`
- uses a specific libraryâ€”Knex! 

Letâ€™s start by creating our data source class:

[`src/data-sources/SQL.js`](https://github.com/GraphQLGuide/guide-api/blob/sql2_0.2.0/src/data-sources/SQL.js)

```js
import { SQLDataSource } from 'datasource-sql'

class SQL extends SQLDataSource {
  // TODO
}

export default SQL
```

Our job will be to fill in the class with methods our resolvers need. To know what those methods are, letâ€™s go at it from the other direction: creating and using the data source as if it were complete. 

First letâ€™s create it. Instead of our current data sources file:

[`src/data-sources/index.js`](https://github.com/GraphQLGuide/guide-api/compare/sql_0.2.0...sql2_0.2.0)

```js
import Reviews from './Reviews'
import Users from './Users'
import Github from './Github'
import { db } from '../db'

export default () => ({
  reviews: new Reviews(db.collection('reviews')),
  users: new Users(db.collection('users'))
})

export { Reviews, Users, Github }
```

we do:

```js
import Github from './Github'
import SQL from './SQL'

export const knexConfig = {
  client: 'sqlite3',
  connection: {
    filename: './sql/dev.sqlite3'
  },
  useNullAsDefault: true
}

export const db = new SQL(knexConfig)

export default () => ({ db })

export { Github }
```

The `SQLDataSource` constructor takes the same config we have in our `sql/knexfile.js`. Since we only want a single instance, we move the creation (`new SQL(knexConfig)`) outside of the exported function. Instead of the data source instances being named `reviews` and `users`, itâ€™s named `db` (because it is the way to access the whole SQL database).

Now in resolvers, we can use functions like `context.dataSources.db.getReviews()` instead of `context.dataSources.reviews.all()`. And we also need to replace `camelCase` fields with `snake_case`, like `deletedAt -> deleted_at`.

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/sql_0.2.0...sql2_0.2.0)

```js
export default {
  Query: {
    me: ...,
    user: (_, { id }, { dataSources: { db } }) => db.getUser({ id }),
    searchUsers: (_, { term }, { dataSources: { db } }) => db.searchUsers(term)
  },
  UserResult: {
    __resolveType: result => {
      if (result.deleted_at) {
        return 'DeletedUser'
      } else if (result.suspended_at) {
        return 'SuspendedUser'
      } else {
        return 'User'
      }
    }
  },
  SuspendedUser: {
    daysLeft: user => {
      const end = addDays(user.suspended_at, user.duration_in_days)
      return differenceInDays(end, new Date())
    }
  },
  User: {
    firstName: user => user.first_name,
    lastName: user => user.last_name,
    email: ...,
    photo(user) {
      // user.auth_id: 'github|1615'
      const githubId = user.auth_id.split('|')[1]
      return `https://avatars.githubusercontent.com/u/${githubId}`
    },
    createdAt: user => user.created_at,
    updatedAt: user => user.updated_at
  },
  Mutation: {
    createUser(_, { user, secretKey }, { dataSources: { db } }) {
      if (secretKey !== process.env.SECRET_KEY) {
        throw new AuthenticationError('wrong secretKey')
      }

      return db.createUser(user)
    }
  }
}
```

So the `db.*` methods we needed and named are:

```js
db.getUser()
db.searchUsers()
db.createUser()
```

Note that we needed to add resolvers for `firstName`, `lastName`, and `updatedAt`, because we no longer have database fields with those exact names (instead we have `first_name`, `last_name`, and `updated_at`).

Next letâ€™s update our Review resolvers:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/sql_0.2.0...sql2_0.2.0)

```js
export default {
  Query: {
    reviews: (_, __, { dataSources: { db } }) => db.getReviews()
  },
  Review: {
    author: (review, _, { dataSources: { db } }) =>
      db.getUser({ id: review.author_id }),
    fullReview: async (review, _, { dataSources: { db } }) => {
      const author = await db.getUser({ id: review.author_id })
      return `${author.first_name} ${author.last_name} gave ${review.stars} stars, saying: "${review.text}"`
    },
    createdAt: review => review.created_at,
    updatedAt: review => review.updated_at
  },
  Mutation: {
    createReview: (_, { review }, { dataSources: { db }, user }) => {
      ...

      const newReview = db.createReview(review)

      pubsub.publish('reviewCreated', {
        reviewCreated: newReview
      })

      return newReview
    }
  },
  Subscription: {
    reviewCreated: ...
  }
}
```

We reused the `db.getUser()` function and used two new ones:

```js
db.getReviews()
db.createReview()
```

The Users and Reviews resolvers were the only place we used `context.dataSources`, but we can do a workspace text search for `db.collection` to find any other uses of our MongoDB database. The only match is from our context function in `src/context.js`:

```js
  const user = await db.collection('users').findOne({ authId })
```

To update this, we need access to our SQL data source. In `src/data-sources/index.js`, we have this line:

```js
export const db = new SQL(knexConfig)
```

So we can import our new `db` from there.

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/compare/sql_0.2.0...sql2_0.2.0)

```js
import { db } from './data-sources/'

export default async ({ req }) => {
  ...

    const user = await db.getUser({ auth_id: authId })

  ...
  
  return context
}
```

Now we can implement all the data source methods weâ€™re using:

```js
db.getReviews()
db.createReview()
db.createUser()
db.getUser()
db.searchUsers()
```

Inside methods we have access to `this.context`, which has the current user, and `this.knex`, our [Knex instance](http://knexjs.org/#Builder), which we use to construct SQL statements. For example, hereâ€™s `SELECT * FROM reviews;`:

[`src/data-sources/SQL.js`](https://github.com/GraphQLGuide/guide-api/blob/sql2_0.2.0/src/data-sources/SQL.js)

```js
import { SQLDataSource } from 'datasource-sql'

const REVIEW_TTL = 60 // minute

class SQL extends SQLDataSource {
  getReviews() {
    return this.knex
      .select('*')
      .from('reviews')
      .cache(REVIEW_TTL)
  }

  async createReview(review) { ... }
  async createUser(user) { ... }
  async getUser(where) { ... }
  searchUsers(term) { ... }
}

export default SQL
```

The added `.cache()` tells `SQLDataSource` to cache the response for the provided number of seconds.

Next up is `createReview()`, where we get a review from the client and need to add the current userâ€™s ID as well as timestamps:

```js
class SQL extends SQLDataSource {
  getReviews() { ... }

  async createReview(review) {
    review.author_id = this.context.user.id
    review.created_at = Date.now()
    review.updated_at = Date.now()
    const [id] = await this.knex
      .returning('id')
      .insert(review)
      .into('reviews')
    review.id = id
    return review
  }

  async createUser(user) { ... }
  async getUser(where) { ... }
  searchUsers(term) { ... }
}
```

We tell Knex to return the inserted ID (`.returning('id')`) so that we can add it to the review object and return it. We didnâ€™t do this before because MongoDBâ€™s `collection.insertOne(review)` automatically added an `_id` to `review`. We do the same for `createUser()`:

```js
class SQL extends SQLDataSource {
  getReviews() { ... }
  async createReview() { ... }

  async createUser(user) {
    const newUser = {
      first_name: user.firstName,
      last_name: user.lastName,
      username: user.username,
      email: user.email,
      auth_id: user.authId,
      created_at: Date.now(),
      updated_at: Date.now()
    }

    const [id] = await this.knex
      .returning('id')
      .insert(newUser)
      .into('users')
    newUser.id = id

    return newUser
  }

  async getUser(where) { ... }
  searchUsers(term) { ... }
}
```

Here we just take the fields out of the user argument (which matches the GraphQL schema) and put them into a `newUser` object that matches the SQL `users` table schema. 

Lastly, we have `getUser()` and `searchUser()`. The `getUser()` function receives an object like `{id: 1}` or `{auth_id: 'github|1615'}`, which can be passed directly to Knexâ€™s `.where()`:

```js
const REVIEW_TTL = 60 // minute
const USER_TTL = 60 * 60 // hour

class SQL extends SQLDataSource {
  getReviews() { ... }
  async createReview(review) { ... }
  async createUser(user) { ... }

  async getUser(where) {
    const [user] = await this.knex
      .select('*')
      .from('users')
      .where(where)
      .cache(USER_TTL)

    return user
  }

  searchUsers(term) {
    return this.knex
      .select('*')
      .from('users')
      .where('first_name', 'like', `%${term}%`)
      .orWhere('last_name', 'like', `%${term}%`)
      .orWhere('username', 'like', `%${term}%`)
      .cache(USER_TTL)
  }
}
```

We use a longer TTL for users with the idea theyâ€™ll change less often than reviews will. We could also have different TTLs for different types of queries. For instance, we could use 60 seconds for selecting a single review but only 5 seconds for selecting all reviews. Then we wouldnâ€™t have to wait more than 5 seconds to see a new review appear on the reviews page.

SQLâ€™s `like` syntax is followed by a search pattern that can include the `%` wildcard, which takes the place of zero or more characters.

Now letâ€™s see if it works by running `npm run dev` and making queries in Playground:

![successful reviews query](img/sql-reviews.png)

ðŸ˜ƒ Looks like itâ€™s working! ...but not if we select a Date field:

![error requesting reviews.createdAt](img/sql-date-error.png)

ðŸ˜ž The stacktrace points to this part of `src/resolvers/Date.js`:

```js
  serialize(date) {
    if (!(date instanceof Date)) {
      throw new Error(
        'Resolvers for Date scalars must return JavaScript Date objects'
      )
    }

    if (!isValid(date)) {
      throw new Error('Invalid Date scalar')
    }

    return date.getTime()
  }
```

Remember when we [wrote that](#custom-scalars)? A custom scalarâ€™s `serialize()` function is called when a value is returned from a resolver, and it formats the value for being sent to the client. When we were querying MongoDB, our resultsâ€”for instance `review.createdAt`â€”were JavaScript Date objects, and we formatted them as integers. But when we query SQL datetime fields, we get them as integers, so we donâ€™t need to format them differently for sending to the client. Similarly, when we receive values from the client, we donâ€™t need to convert them to Date objects in `parseValue()` and `parseLiteral()`. However, we can still check to make sure theyâ€™re valid date integers:

[`src/resolvers.Date.js`](https://github.com/GraphQLGuide/guide-api/compare/sql_0.2.0...sql2_0.2.0)

```js
import { GraphQLScalarType } from 'graphql'
import { Kind } from 'graphql/language'

const isValid = date => !isNaN(date.getTime())

export default {
  Date: new GraphQLScalarType({
    name: 'Date',
    description:
      'The `Date` scalar type represents a single moment in time. It is serialized as an integer, equal to the number of milliseconds since the Unix epoch.',

    parseValue(value) {
      if (!Number.isInteger(value)) {
        throw new Error('Date values must be integers')
      }

      const date = new Date(value)
      if (!isValid(date)) {
        throw new Error('Invalid Date value')
      }

      return value
    },

    parseLiteral(ast) {
      if (ast.kind !== Kind.INT) {
        throw new Error('Date literals must be integers')
      }

      const dateInt = parseInt(ast.value)
      const date = new Date(dateInt)
      if (!isValid(date)) {
        throw new Error('Invalid Date literal')
      }

      return dateInt
    },

    serialize(date) {
      if (!Number.isInteger(date)) {
        throw new Error('Resolvers for Date scalars must return integers')
      }

      if (!isValid(new Date(date))) {
        throw new Error('Invalid Date scalar')
      }

      return date
    }
  })
}
```

For `parseValue()`, the value is already an integer. For `parseLiteral()`, we get a string, so we use `parseInt()`. 

The last thing we need to update is our root query field `isoString(date: Date)`:

```js
    isoString: (_, { date }) => date.toISOString()
```

`date` used to be a Date instance, but now itâ€™s an integer, so we canâ€™t call `toISOString()` until we create a Date object. But strangely enough, we canâ€™t create a Date object because the `Date` identifier is being used later in the file:

```js
import Date from './Date'
```

So we also need to change what we call the Date resolvers weâ€™re importing:

[`src/resolvers/index.js](https://github.com/GraphQLGuide/guide-api/compare/sql_0.2.0...sql2_0.2.0)

```js
const resolvers = {
  Query: {
    hello: () => 'ðŸŒðŸŒðŸŒŽ',
    isoString: (_, { date }) => new Date(date).toISOString()
  }
}

import Review from './Review'
import User from './User'
import DateResolvers from './Date'
import Github from './Github'

export default [resolvers, Review, User, DateResolvers, Github]
```

Now all our dates are working:

![isoString query and reviews.createdAt working](img/sql-time-working.png)

### SQL testing

> If youâ€™re jumping in here, `git checkout sql2_0.1.0` (tag [sql2_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/sql2_0.1.0), or compare [sql2...sql3](https://github.com/GraphQLGuide/guide-api/compare/sql2_0.1.0...sql3_0.1.0))

In the last section we implemented and used (okay, more like used then implemented ðŸ˜„) our SQL data source. We also made a couple of queries to see if it worked, and the queries did work (eventually), but it wasnâ€™t a comprehensive test. Letâ€™s update our automated tests (which are currently broken) so we can have a higher level of confidence in our codeâ€™s correctness. 

The place to start updating is in the code at the base of all our tests, `test/guide-test-utils.js`. We need to:

- Update mocked data field names (`_id -> id` and `firstName -> first_name`) and values.
- Mock our new SQL data source.
- Remove our old data sources and database connection code.

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/compare/sql2_0.2.0...sql3_0.2.0)

```js
import { ApolloServer } from 'apollo-server'
import { promisify } from 'util'
import { HttpLink } from 'apollo-link-http'
import fetch from 'node-fetch'
import { execute, toPromise } from 'apollo-link'

import {
  server,
  typeDefs,
  resolvers,
  context as defaultContext,
  formatError
} from '../src/'

const created_at = new Date('2020-01-01').getTime()
const updated_at = created_at

export const mockUser = {
  id: 1,
  first_name: 'First',
  last_name: 'Last',
  username: 'mockA',
  auth_id: 'mockA|1',
  email: 'mockA@gmail.com',
  created_at,
  updated_at
}

const mockUsers = [mockUser]

const reviewA = {
  id: 1,
  text: 'A+',
  stars: 5,
  created_at,
  updated_at,
  author_id: mockUser.id
}

const reviewB = {
  id: 2,
  text: 'Passable',
  stars: 3,
  created_at,
  updated_at,
  author_id: mockUser._id
}

const mockReviews = [reviewA, reviewB]

class SQL {
  getReviews() {
    return mockReviews
  }
  createReview() {
    return reviewA
  }
  createUser() {
    return mockUser
  }
  getUser() {
    return mockUser
  }
  searchUsers() {
    return mockUsers
  }
}

export const db = new SQL()

export const createTestServer = ({ context = defaultContext } = {}) => {
  const server = new ApolloServer({
    typeDefs,
    resolvers,
    dataSources: () => ({ db }),
    context,
    formatError,
    engine: false
  })

  return { server, dataSources: { db } }
}

export const startE2EServer = async () => {
  const e2eServer = await server.listen({ port: 0 })

  const stopServer = promisify(e2eServer.server.close.bind(e2eServer.server))

  const link = new HttpLink({
    uri: e2eServer.url,
    fetch
  })

  return {
    stop: stopServer,
    request: operation => toPromise(execute(link, operation))
  }
}

export { createTestClient } from 'apollo-server-testing'
export { default as gql } from 'graphql-tag'
```

In our User resolver tests, we also need to update field names:

[`src/resolvers/User.test.js`](https://github.com/GraphQLGuide/guide-api/compare/sql2_0.2.0...sql3_0.2.0)

```js
import {
  createTestServer,
  createTestClient,
  gql,
  mockUser
} from 'guide-test-utils'

const ME = gql`
  query {
    me {
      id
    }
  }
`

test('me', async () => {
  const { server } = createTestServer({
    context: () => ({ user: { id: 'itme' } })
  })
  const { query } = createTestClient(server)

  const result = await query({ query: ME })
  expect(result.data.me.id).toEqual('itme')
})

const USER = gql`
  query User($id: ID!) {
    user(id: $id) {
      id
    }
  }
`

test('user', async () => {
  const { server } = createTestServer()
  const { query } = createTestClient(server)

  const id = mockUser.id
  const result = await query({
    query: USER,
    variables: { id }
  })
  expect(result.data.user.id).toEqual(id.toString())
})

const CREATE_USER = gql`
  mutation CreateUser($user: CreateUserInput!, $secretKey: String!) {
    createUser(user: $user, secretKey: $secretKey) {
      id
    }
  }
`

test('createUser', async () => {
  const { server } = createTestServer()
  const { mutate } = createTestClient(server)

  const user = {
    firstName: mockUser.first_name,
    lastName: mockUser.last_name,
    username: mockUser.username,
    email: mockUser.email,
    authId: mockUser.auth_id
  }

  const result = await mutate({
    mutation: CREATE_USER,
    variables: {
      user,
      secretKey: process.env.SECRET_KEY
    }
  })

  expect(result).toMatchSnapshot()
})
```

Now if we run `npm test`, we see tests fail due to mismatching snapshots, which we can update with `npx jest -u`. 

One thing we updated in the last section that we donâ€™t have a test for is the context function:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/blob/sql2_0.2.0/src/context.js)

```js
import { AuthenticationError } from 'apollo-server'

import { getAuthIdFromJWT } from './util/auth'
import { db } from './data-sources/'

export default async ({ req }) => {
  const context = {}

  const jwt = req && req.headers.authorization
  let authId

  if (jwt) {
    try {
      authId = await getAuthIdFromJWT(jwt)
    } catch (e) {
      let message
      if (e.message.includes('jwt expired')) {
        message = 'jwt expired'
      } else {
        message = 'malformed jwt in authorization header'
      }
      throw new AuthenticationError(message)
    }

    const user = await db.getUser({ auth_id: authId })
    if (user) {
      context.user = user
    } else {
      throw new AuthenticationError('no such user')
    }
  }

  return context
}
```

Letâ€™s write a test for it! In order to test it, we have two options:

- Using an authorization header that successfully decodes to our mock `auth_id`: `mockA|1`. We canâ€™t create such a JWT, and, even if we could, it would expire. And then our test would fail.
- Make it a unit test and mock all the functions it callsâ€”in this case `getAuthIdFromJWT()` and `db.getUser()`.

Letâ€™s do the second. To mock an import, we need to call `jest.mock(file)`:

[`src/context.test.js`](https://github.com/GraphQLGuide/guide-api/blob/sql3_0.2.0/src/context.test.js)

```js
import { mockUser } from 'guide-test-utils'

jest.mock('./util/auth', () => ({
  getAuthIdFromJWT: jest.fn(jwt => (jwt === 'valid' ? mockUser.auth_id : null))
}))

jest.mock('./data-sources/', () => ({
  db: {
    getUser: ({ auth_id }) => (auth_id === mockUser.auth_id ? mockUser : null)
  }
}))
```

Now when any code weâ€™re testing does the below imports, it will get our mock implementations.

```js
import { getAuthIdFromJWT } from './util/auth'
import { db } from './data-sources/'
```

Letâ€™s test the success case first:

```js
import getContext from './context'
import { getAuthIdFromJWT } from './util/auth'

describe('context', () => {
  it('finds a user given a valid jwt', async () => {
    const context = await getContext({
      req: { headers: { authorization: 'valid' } }
    })

    expect(getAuthIdFromJWT.mock.calls.length).toBe(1)
    expect(context.user).toMatchSnapshot()
  })
})
```

We can check our snapshot:

`src/__snapshots__/context.test.js.snap`

```js
// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`context finds a user given a valid jwt 1`] = `
Object {
  "auth_id": "mockA|1",
  "created_at": 1577836800000,
  "email": "mockA@gmail.com",
  "first_name": "First",
  "id": 1,
  "last_name": "Last",
  "updated_at": 1577836800000,
  "username": "mockA",
}
`;
```

âœ… Looks good! Next letâ€™s make sure that giving an invalid JWT throws an error:

[`src/context.test.js`](https://github.com/GraphQLGuide/guide-api/blob/sql3_0.2.0/src/context.test.js)

```js
import { AuthenticationError } from 'apollo-serverâ€™
describe('context', () => {
  it('finds a user given a valid jwt', async () => { ... }

  it('throws error on invalid jwt', async () => {
    const promise = getContext({
      req: { headers: { authorization: 'invalid' } }
    })

    expect(getAuthIdFromJWT.mock.calls.length).toBe(1)
    expect(promise).rejects.toThrow(AuthenticationError)
  })
})
```

We see with `npx jest context` that the test fails, saying that the `getAuthIdFromJWT` mock was called twice.

> Adding `context` after `npx jest` limits testing to files with â€œcontextâ€ in their names.

![Invalid jwt test fails, receiving 2 instead of 1](img/invalid-jwt-test-failure.png)

The mock calls are cumulative until we clear the mock. Letâ€™s do that after each test:

```js
describe('context', () => {
  afterEach(() => {
    getAuthIdFromJWT.mockClear()
  })
  
  it('finds a user given a valid jwt', async () => { ... }

  it('throws error on invalid jwt', async () => {
    const promise = getContext({
      req: { headers: { authorization: 'invalid' } }
    })

    expect(getAuthIdFromJWT.mock.calls.length).toBe(1)
    expect(promise).rejects.toThrow(AuthenticationError)
  })
})
```

âœ… And weâ€™re back to green. Lastly, letâ€™s test a blank auth header:

```js
describe('context', () => {
  afterEach(() => {
    getAuthIdFromJWT.mockClear()
  })
  
  it('finds a user given a valid jwt', async () => { ... }
  it('throws error on invalid jwt', async () => { ... }

  it('is empty without jwt', async () => {
    const context = await getContext({
      req: { headers: {} }
    })

    expect(getAuthIdFromJWT.mock.calls.length).toBe(0)
    expect(context).toEqual({})
  })
})
```

âœ… And still green! ðŸ’ƒ All together, thatâ€™s:

[`src/context.test.js`](https://github.com/GraphQLGuide/guide-api/blob/sql3_0.2.0/src/context.test.js)

```js
import { AuthenticationError } from 'apollo-server'
import { mockUser } from 'guide-test-utils'

import getContext from './context'
import { getAuthIdFromJWT } from './util/auth'

jest.mock('./util/auth', () => ({
  getAuthIdFromJWT: jest.fn(jwt => (jwt === 'valid' ? mockUser.auth_id : null))
}))

jest.mock('./data-sources/', () => ({
  db: {
    getUser: ({ auth_id }) => (auth_id === mockUser.auth_id ? mockUser : null)
  }
}))

describe('context', () => {
  afterEach(() => {
    getAuthIdFromJWT.mockClear()
  })

  it('finds a user given a valid jwt', async () => {
    const context = await getContext({
      req: { headers: { authorization: 'valid' } }
    })

    expect(getAuthIdFromJWT.mock.calls.length).toBe(1)
    expect(context.user).toMatchSnapshot()
  })

  it('throws error on invalid jwt', async () => {
    const promise = getContext({
      req: { headers: { authorization: 'invalid' } }
    })

    expect(getAuthIdFromJWT.mock.calls.length).toBe(1)
    expect(promise).rejects.toThrow(AuthenticationError)
  })

  it('is empty without jwt', async () => {
    const context = await getContext({
      req: { headers: {} }
    })

    expect(getAuthIdFromJWT.mock.calls.length).toBe(0)
    expect(context).toEqual({})
  })
})
```

Unfortunately if we run `npm test`, we see our coverage is down to 40%. And if we look at the coverage report (`npm run open-coverage`), we see not much of our SQL data source is covered:

![Coverage web page with 12.5% statement coverage of src/data-sources/SQL.js](img/sql-coverage-report.png)

Our old `Users.js` and `Reviews.js` files were 100% covered:

![Coverage web page with 100% coverage of Users.js, Reviews.js, and index.js](img/data-sources-full-coverage.png)

The issue is that before, we were mocking the `.find()` and `.insertOne()` methods of MongoDB collections, and currently, weâ€™re mocking the data source methods:

[`test/guide-test-utils.js`](https://github.com/GraphQLGuide/guide-api/compare/sql2_0.2.0...sql3_0.2.0)

```js
class SQL {
  getReviews() {
    return mockReviews
  }
  createReview() {
    return reviewA
  }
  createUser() {
    return mockUser
  }
  getUser() {
    return mockUser
  }
  searchUsers() {
    return mockUsers
  }
}
```

If we wanted to cover `SQL.js`, we would need to run the actual methods, which means we would need to instead mock the `this.knex` used by the methods.

### SQL performance

> If youâ€™re jumping in here, `git checkout sql3_0.1.0` (tag [sql3_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/sql3_0.1.0), or compare [sql3...sql4](https://github.com/GraphQLGuide/guide-api/compare/sql3_0.1.0...sql4_0.1.0))

The two main performance factors when it comes to database querying are latency and load. Latency is how quickly we get all the data we need, and load is how much work the database is doing. Latency usually wonâ€™t be an issue unless we have complex queries or a lot of data. Load wonâ€™t be an issue unless we have a lot of clients simultaneously using our API. 

When neither latency nor load is an issue for our app, we donâ€™t need to concern ourselves with performance, and our current implementation is fine. If either becomes an issue (or if weâ€™re certain that it will be when our API is completed and released), then we have different ways we can improve performance. This section is mainly about using SQL JOIN statements, which weâ€™re currently not using. We discuss more performance topics in the [Performance section](#performance) later in the chapter.

Letâ€™s consider this GraphQL query:

```gql
{
  reviews {
    id
    text
    author {
      firstName
    }
  }
}
```

If we were writing an efficient SQL statement to fetch that data, weâ€™d write:

```sql
SELECT reviews.id, reviews.text, users.first_name
FROM reviews 
LEFT JOIN users 
ON reviews.author_id = users.id
```

Letâ€™s compare this statement to what happens with our current server. We can have Knex print out statements it sends by adding a `DEBUG=knex:query` env var. When we do that and make the above GraphQL query, we see these three SQL statements:

```sh
$ DEBUG=knex:query npm run dev
GraphQL server running at http://localhost:4000/
SQL (1.437 ms) select * from `reviews`
SQL (0.364 ms) select * from `users` where `id` = 1
SQL (0.377 ms) select * from `users` where `id` = 1
```

There are a few issues with this:

- There are 3 queries instead of 1. (And more generally, there are `N+1` queries, where `N` is the number of reviews.)
- They all select `*` instead of just the fields needed.
- The second two are redundant (they occur because SQLDataSource doesnâ€™t do batching).

This probably will result in a higher load on the SQL server than the single efficient statement we wrote. It also has a higher latency, since not all of the three statements are sent at the same timeâ€”first the reviews are fetched, then the `author_id`s are used to create the rest of the statements. Thatâ€™s two round trips over the network from the API server to the database instead of the one trip our efficient statement took. 

Letâ€™s change our code to use a JOIN like the efficient statement did. Currently, the `reviews` root Query field calls the `getReviews()` data source method:

[`src/data-sources/SQL.js`](https://github.com/GraphQLGuide/guide-api/compare/sql3_0.2.0...sql4_0.2.0)

```js
class SQL extends SQLDataSource {
  getReviews() {
    return this.knex
      .select('*')
      .from('reviews')
      .cache(REVIEW_TTL)
  }

  ...
}
```

We can add a `.leftJoin()`:

```js
import { pick } from 'lodash'

class SQL extends SQLDataSource {
  async getReviews() {
    const reviews = await this.knex
      .select(
        'users.*',
        'users.created_at as users__created_at',
        'users.updated_at as users__updated_at',
        'reviews.*'
      )
      .from('reviews')
      .leftJoin('users', 'users.id', 'reviews.author_id')
      .cache(REVIEW_TTL)

    return reviews.map(review => ({
      ...review,
      author: {
        id: review.author_id,
        created_at: review.users__created_at,
        updated_at: review.users__updated_at,
        ...pick(review, 'first_name', 'last_name', 'email', 'photo')
      }
    }))
  }

  ...
}
```

We needed to change our `.select('*')` because both users and reviews have `created_at` and `updated_at` columns. We also needed to use `.map()` to extract out the user fields into an `author` object. 

Finally, we need to stop the `Review.author` resolver from querying the database. We can do so by checking if the `author` object is already present on the review object:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/sql3_0.2.0...sql4_0.2.0)

```js
export default {
  Query: ...
  Review: {
    author: (review, _, { dataSources: { db } }) =>
      review.author || db.getUser({ id: review.author_id }),
    ...
}
```

Now when we run the same GraphQL query in Playground, we see this SQL statement is executed:

```
SQL (1.873 ms) select `reviews`.*, `users`.`created_at` as `users__created_at`, `users`.`updated_at` as `users__updated_at` from `reviews` left join `users` on `users`.`id` = `reviews`.`author_id`
```

Success! We went from three statements down to one. However, there are still inefficiencies. The SQL statement is overfetching in two ways:

- Itâ€™s selecting all fields, whereas the GraphQL query only needed `id`, `text`, and `author.firstName`.
- It always does a JOIN, even when the GraphQL query doesnâ€™t select `Review.author`.

We can write code to address both these thingsâ€”by looking through the fourth argument to resolvers, `info`, which contains information about the current GraphQL query, and seeing which fields are selected. However, it would be easier to use the [Join Monster](https://join-monster.readthedocs.io/en/latest/) library, which does this for us.

To set it up, we create a new file to add the following information to our schema:

[`src/joinMonsterAdapter.js`](https://github.com/GraphQLGuide/guide-api/blob/sql4_0.2.0/src/joinMonsterAdapter.js)

```js
import joinMonsterAdapt from 'join-monster-graphql-tools-adapter'

export default schema =>
  joinMonsterAdapt(schema, {
    Query: {
      fields: {
        user: {
          where: (users, args) => `${users}.id = ${args.id}`
        }
      }
    },
    Review: {
      sqlTable: 'reviews',
      uniqueKey: 'id',
      fields: {
        author: {
          sqlJoin: (reviews, users) =>
            `${reviews}.author_id = ${users}.id`
        },
        text: { sqlColumn: 'text' },
        stars: { sqlColumn: 'stars' },
        fullReview: { sqlDeps: ['text', 'stars', 'author_id'] },
        createdAt: { sqlColumn: 'created_at' },
        updatedAt: { sqlColumn: 'updated_at' }
      }
    },
    User: {
      sqlTable: 'users',
      uniqueKey: 'id',
      fields: {
        firstName: { sqlColumn: 'first_name' },
        lastName: { sqlColumn: 'last_name' },
        createdAt: { sqlColumn: 'created_at' },
        updatedAt: { sqlColumn: 'updated_at' },
        photo: { sqlDeps: ['auth_id'] }
      }
    }
  })
```

> Weâ€™re using the [`join-monster-graphql-tools-adapter`](https://github.com/join-monster/join-monster-graphql-tools-adapter) package, which we need when defining our schema in SDL format via [`graphql-tools`](https://www.apollographql.com/docs/graphql-tools/generate-schema/) or Apollo Server. (We wouldnâ€™t need an adapter if we defined our schema in code with [`graphql-js`](https://github.com/graphql/graphql-js).)

We tell Join Monster:

- Which table each type corresponds to.
- Which column each field corresponds to.
- Query information for fields that involve SQL statements. For example, `Query.user`â€™s WHERE clause matches the `id` argument with the `id` field in the users table, and `Review.author` can be fetched with a JOIN on the users table.
- When we need it to fetch fields that arenâ€™t in the GraphQL query. For example, if `User.firstName` is in the query, it knows to fetch and return `first_name`:

```js
  firstName: { sqlColumn: 'first_name' },
```

But for `User.photo`, thereâ€™s no photo column in the users table. So our `User.photo` resolver will run, but it needs access to the userâ€™s `auth_id` field. We need to tell Join Monster when `User.photo` is in the query, it needs to fetch `auth_id` from the database:

```js
  photo: { sqlDeps: ['auth_id'] }
```

We call our configuration function with a schema created by `makeExecutableSchema`, and then we pass the schema to `ApolloServer()` (whereas before we were passing `typeDefs` and `resolvers`):

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/sql3_0.2.0...sql4_0.2.0)

```js
import { makeExecutableSchema } from 'graphql-tools'

import joinMonsterAdapter from './joinMonsterAdapter'

export const schema = makeExecutableSchema({
  typeDefs,
  resolvers
})

joinMonsterAdapter(schema)

const server = new ApolloServer({
  schema,
  dataSources,
  context,
  formatError
})

...
```

Weâ€™re also going to need a Knex instance, which weâ€™ll add here:

[`src/data-sources/index.js`](https://github.com/GraphQLGuide/guide-api/compare/sql3_0.2.0...sql4_0.2.0)

```js
import Knex from 'knex'

const knexConfig = {
  client: 'sqlite3',
  connection: {
    filename: './sql/dev.sqlite3'
  },
  useNullAsDefault: true
}

export const knex = Knex(knexConfig)
```

And lastly, we update our `Query.user` and `Query.review` resolvers:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/sql3_0.2.0...sql4_0.2.0)

```js
import joinMonster from 'join-monster'

import { knex } from '../data-sources/'

export default {
  Query: {
    me: ...
    user: (_, __, context, info) =>
      joinMonster(info, context, sql => knex.raw(sql), {
        dialect: 'sqlite3'
      }),
    ...
  }
  ...
}
```

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/sql3_0.2.0...sql4_0.2.0)

```js
import joinMonster from 'join-monster'

import { knex } from '../data-sources/'

export default {
  Query: {
    reviews: (_, __, context, info) =>
      joinMonster(info, context, sql => knex.raw(sql), {
        dialect: 'sqlite3'
      })
  },
  ...
}
```

That was certainly simpler than the long `getReviews()` method we wrote! Instead, we give `joinMonster()` the `info` and `context`, and it gives us a SQL statement to run. 

We also get to remove some resolvers that will be taken care of by Join Monster: 

```
User.firstName
User.lastName
User.createdAt
User.updatedAt
Review.author
Review.createdAt
Review.updatedAt
```

Now when we query for a user and select `firstName`, `createdAt`, and `photo`:

![user query in Playground with 3 fields selected](img/user-query-3-fields.png)

this SELECT statement gets run:

```
GraphQL server running at http://localhost:4000/
  knex:query SELECT
  knex:query   "user"."id" AS "id",
  knex:query   "user"."first_name" AS "firstName",
  knex:query   "user"."created_at" AS "createdAt",
  knex:query   "user"."auth_id" AS "auth_id"
  knex:query FROM users "user"
  knex:query WHERE "user".id = 1 +16s
```

Join Monster knows to get `1` from the query argument to use in the WHERE clause, it knows to look in the users table, and it knows exactly which fields to fetch, even `auth_id`. 

Hereâ€™s another example of `sqlDeps` working. From the config:

```
  fullReview: { sqlDeps: ['text', 'stars', 'author_id'] },
```

When we send this query:

```gql
{
  reviews {
    fullReview
  }
}
```

all three deps are selected:

```
  knex:query SELECT
  knex:query   "reviews"."id" AS "id",
  knex:query   "reviews"."text" AS "text",
  knex:query   "reviews"."stars" AS "stars",
  knex:query   "reviews"."author_id" AS "author_id"
  knex:query FROM reviews "reviews" +0ms
SQL (0.980 ms) select * from `users` where `id` = 1
SQL (0.367 ms) select * from `users` where `id` = 1
```

Join Monster [doesnâ€™t yet support](https://github.com/acarl005/join-monster/issues/398) a joined object type as a field dependency, which is why we list `author_id` instead of `author` in `sqlDeps`, and why the `Review.fullReview` resolver still has  to call `db.getUser()`.

Lastly, letâ€™s see how it handles a reviews query with `author` selected:

```gql
{
  reviews {
    author {
      lastName
    }
  }
}
```

```
  knex:query SELECT
  knex:query   "reviews"."id" AS "id",
  knex:query   "author"."id" AS "author__id",
  knex:query   "author"."last_name" AS "author__lastName"
  knex:query FROM reviews "reviews"
  knex:query LEFT JOIN users "author" ON "reviews".author_id = "author".id +3m
```

âœ¨ Perfect! It only fetched the fields needed and used a single statement.

## REST

> If youâ€™re jumping in here, `git checkout 25_0.1.0` (tag [25_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/25_0.1.0), or compare [25...rest](https://github.com/GraphQLGuide/guide-api/compare/25_0.1.0...rest_0.1.0))

Instead of fetching our data directly from the database, we may want to make use of our companyâ€™s legacy REST services (yes, any service that doesnâ€™t speak GraphQL and support [Apollo Federation](#apollo-federation) is now a *legacy* service ðŸ˜‰ðŸ˜„). Or we may want to use data from third-party REST APIs. In either case, we use `RESTDataSource` to create a data source that makes REST requests.

Users of the Guide site need to be able to purchase the book, so we need to display the price to them. And letâ€™s say we wanted to make the book more affordable in locations outside of the United States where it was originally priced. [Purchasing power parity](https://en.wikipedia.org/wiki/Purchasing_power_parity) (PPP) produces a conversion factor based on the actual purchasing power in a different location. For example, if the book is $100 in the U.S., and the conversion factor for India is 0.26, then charging `100 * 0.26 = $26` for the book to customers in India would make it equivalently affordable for them.

Letâ€™s add a root query field `costInCents` that returns the PPP-adjusted cost of the book. To do that, weâ€™ll need to query a PPP API. `ppp.graphql.guide` is a REST API that returns PPP information when given a country code (for example, `/?country=IN` for India). We can try it out in the browser:

[ppp.graphql.guide/?country=IN](https://ppp.graphql.guide/?country=IN)

![Chrome showing the JSON response with PPP info](img/ppp-in-browser.png)

The response JSON includes `pppConversionFactor`, which combines the `ppp` value and exchange rate into a number we multiply the USD price by. 

The other thing we need to figure out is how to get the country code of the client. We could look at the IP address (which is either `req.headers['x-forwarded-for'] || req.socket.remoteAddress`) and use a GeoIP lookup API (where we send the IP address and get back a location), but the easier way is to use the Cloudflare CDN, which adds a fairly accurate [`cf-ipcountry` HTTP header](https://support.cloudflare.com/hc/en-us/articles/200168236-What-does-Cloudflare-IP-Geolocation-do-) to all incoming requests. We can emulate this by setting the `cf-ipcountry` header in Playground.

We can check the header in our context function, and add the country code to our context object:

[`src/context.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...rest_0.2.0)

```js
export default async ({ req }) => {
  const context = {}
  
  ...

  const countryCode = req && req.headers['cf-ipcountry']
  const invalidCode = ['XX', 'T1'].includes(countryCode)
  if (countryCode && !invalidCode) {
    context.countryCode = countryCode
  }

  return context
}
```

Weâ€™ll then be able to access the code from our data source, which we create by extending `RESTDataSource` from [`apollo-datasource-rest`](https://www.npmjs.com/package/apollo-datasource-rest). There are five main things to know about `RESTDataSource`:

- Set `this.baseURL` to the REST APIâ€™s URL in the constructor.
- Use HTTP verb methods like `this.get(path, queryParams, options)`, `this.post()`, etc.
- It [deduplicates](https://khalilstemmler.com/blogs/graphql/how-apollo-rest-data-source-caches-api-calls/) REST requests.
- It caches responses from the REST API based on the responsesâ€™ cache headers.
- Define a `willSendRequest()` method if you want to modify all outgoing requestsâ€”for instance, by adding an auth header:

```js
class SomePrivateAPI extends RESTDataSource {
  ...
  
  willSendRequest(request) {
    request.headers.set('Authorization', this.context.token);
  }
}
```

Hereâ€™s our implementation, using `this.baseURL`, `this.get()`, and `this.context`:

[`src/data-sources/PPP.js`](https://github.com/GraphQLGuide/guide-api/blob/rest_0.2.0/src/data-sources/PPP.js)

```js
import { RESTDataSource } from 'apollo-datasource-rest'

export default class PPP extends RESTDataSource {
  constructor() {
    super()
    this.baseURL = `https://ppp.graphql.guide`
  }

  async getConversionFactor() {
    const { countryCode } = this.context
    if (!countryCode) {
      return 1
    }

    const data = await this.get('/', { country: countryCode })
    return data.pppConversionFactor || 1
  }
}
```

We donâ€™t need to define `willSendRequest()` because itâ€™s a public API. We only need a single method `getConversionFactor()`, which makes a GET request of the form `/?country=[countryCode]`. It defaults to a factor of 1, which results in the full price.

Next we need to add this to our `dataSources` so we can access it from our resolvers:

[`src/data-sources/index.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...rest_0.2.0)

```js
import PPP from './PPP'

export default () => ({
  reviews: new Reviews(db.collection('reviews')),
  users: new Users(db.collection('users')),
  ppp: new PPP()
})

export { Reviews, Users, Github, PPP }
```

And now adding our resolver:

[`src/resolvers/PPP.js`](https://github.com/GraphQLGuide/guide-api/blob/rest_0.2.0/src/resolvers/PPP.js)

```js
const BOOK_PRICE = 3900

export default {
  Query: {
    costInCents: async (_, __, { dataSources }) =>
      Math.round((await dataSources.ppp.getConversionFactor()) * BOOK_PRICE)
  }
}
```

[`src/resolvers/index.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...rest_0.2.0)

```js
import PPP from './PPP'

const resolversByType = [Review, User, Date, Github, PPP]

...
```

Lastly, we add the `costInCents` root Query field:

[`src/schema/PPP.graphql`](https://github.com/GraphQLGuide/guide-api/blob/rest_0.2.0/src/schema/PPP.graphql)

```gql
extend type Query {
  costInCents: Int!
}
```

[`src/schema/schema.graphql`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...rest_0.2.0)

```gql
...
#import 'PPP.graphql'
```

Now we should be able to get 3900 in response to a `{ costInCents }` query:

![costInCents query in Playground](img/costInCents.png)

This is defaulting to the US price, since thereâ€™s no header. When we add a country header, weâ€™ll see a different result:

```json
{
  "cf-ipcountry": "IN"
}
```

![costInCents query with cf-ipcountry header](img/costInCents-with-header.png)

It works! ðŸ’ƒ The only thing left to check is caching. `RESTDataSource` only caches responses that contain a `Cache-Control` header. To see whether `ppp.graphql.guide` uses cache headers, we can use a command-line tool called [httpie](https://httpie.org/) (a modern alternative to `wget`):

```sh
brew install httpie
```

```
$ http https://ppp.graphql.guide/?country=IN
HTTP/1.1 200 OK
Connection: keep-alive
Content-Type: application/json; charset=utf-8
cache-control: max-age=604800, public
content-length: 278
date: Wed, 05 Feb 2020 07:27:47 GMT
etag: W/"116-6RgJXuLuRrGbBbX6QFViYUXAREs"
server: now
strict-transport-security: max-age=63072000
x-now-cache: MISS
x-now-id: iad1:sfo1:bxvvv-1580887666054-bbdc016271ef
x-now-trace: iad1

{
    "countryCode": "IN",
    "currency": {
        "code": "INR",
        "exchangeRate": 71.295489,
        "name": "Indian rupee",
        "symbol": "â‚¹"
    },
    "ppp": 18.553,
    "pppConversionFactor": 0.2602268426828519
}
```

At the top of a list of headers, which includes a `cache-control` header (HTTP headers arenâ€™t case-sensitive) instructing the recipient to cache the response for 604800 seconds (one week). So now our data source *should* be saving responses to the cache, but how can we check? If we were still using [Redis as a cache](#redis-caching), we could check Redis, but instead the data source is using the default in-memory cache. Without Redis, we can run [tcpdump](https://en.wikipedia.org/wiki/Tcpdump) to see when our development machine makes requests to `ppp.graphql.guide`. When a country is already cached, we shouldnâ€™t see the request. In one terminal, we run this command:

```
$ sudo tcpdump "tcp[tcpflags] & (tcp-syn) != 0 and dst ppp.graphql.guide"
tcpdump: data link type PKTAP
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on pktap, link-type PKTAP (Apple DLT_PKTAP), capture size 262144 bytes
```

And then we change the country header in Playground to one we havenâ€™t used, e.g., `CN` for China. On the first query, we should see this line printed:

```
04:30:18.705846 IP macbook.fios-router.home.52591 > ec2-3-210-90-207.compute-1.amazonaws.com.https: Flags [S], seq 995289110, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1101427783 ecr 0,sackOK,eol], length 0
```

which signifies a new request to `ppp.graphql.guide`. If we continue to re-issue the Playground query with the same country header, no more lines should be printed, which means the data source used the in-memory cache instead of making a request.

## GraphQL

If thereâ€™s a GraphQL API that we want to use data from, we have a few options:

- If we want to include parts of the APIâ€™s schema in our schema:
  - If it supports [federation](#schema-federation), we should use that. For example, FaunaDB is [working on support](https://fauna.com/blog/fauna-engineering-looking-back-at-2019), and some third-party services we use might have a GraphQL API that supports federation. And if we have control over the API (e.g., if itâ€™s one of our services), we can add support for federation.
  - We can use [schema stitching](https://www.apollographql.com/docs/graphql-tools/schema-stitching/) if the API doesnâ€™t support federation. But unless we want a significant part of the APIâ€™s schema, it may be easier to use one of the below methods instead.
- If we just want to use data from the API in our resolvers:
  - Use `GraphQLDataSource` from [`apollo-datasource-graphql`](https://github.com/poetic/apollo-datasource-graphql#readme) to create a data source class. Similarly to `RESTDataSource`, we can define a `willSendRequest` method that adds an authorization header to all requests. But in our data fetching methods, instead of `this.get('path')`, we use `this.query(QUERY_DOCUMENT)`.
  - Use `graphql-request` in our resolvers to fetch data from the data source (similar to our [`githubStars`](#githubstars) subscription where we fetch data from GitHubâ€™s GraphQL API). While `graphql-request` is nice for extremely simple uses like `githubStars`, usually `GraphQLDataSource` is a better choice, as itâ€™s a data source class.

## Custom data source

When weâ€™ve been talking about data sources, sometimes weâ€™re referring to the classes we create (`PPP` in the below snippet), and sometimes weâ€™re referring to the parent classes that we get from an npm library and extend (`RESTDataSource`).

```js
import { RESTDataSource } from 'apollo-datasource-rest'

class PPP extends RESTDataSource {
  ...
}
```

If thereâ€™s a type of database or API for which we canâ€™t find an existing library and parent class, we can write our own! A data source parent class has most or all of the following pieces:

- Extends the `DataSource` class from the `apollo-datasource` library
- Some way of receiving information about the database or API (either a constructor parameter or an instance variable like `this.baseURL` in `RESTDataSource`)
- An `initialize()` method that receives the context and an optional cache
- Calls lifecycle methods that can be defined by the child class, like `willSendRequest()` and `didEncounterError()` in `RESTDataSource`
- Methods for fetching data, which use DataLoader and/or the cache
- Methods for changing data, which might invalidate cached data

Letâ€™s see all these in a parent class called `FooDataSource` for an imaginary Foo document database. Itâ€™s passed a Foo database client `dbClient`, which has these fields:

- `dbClient.connectionURI`: the URI of the database server
- `dbClient.getByIds(ids)`: given an array of IDs, returns the associated documents from the database
- `dbClient.update(id, newDoc)`: updates the document with the given `id` to the `newDoc`

```js
import { DataSource } from 'apollo-datasource'
import { InMemoryLRUCache } from 'apollo-server-caching'
import DataLoader from 'dataloader'

class FooDataSource extends DataSource {
  constructor(dbClient) {
    super()
    this.db = dbClient
    this.loader = new DataLoader(ids => dbClient.getByIds(ids))
  }

  initialize({ context, cache } = {}) {
    this.context = context
    this.cache = cache || new InMemoryLRUCache()
  }

  didEncounterError(error) {
    throw error
  }

  cacheKey(id) {
    return `foo-${this.db.connectionURI}-${id}`
  }

  async get(id, { ttlInSeconds } = {}) {
    const cacheDoc = await cache.get(this.cacheKey(id))
    if (cacheDoc) {
      return JSON.parse(cacheDoc)
    }

    const doc = await this.loader.load(id)

    if (ttlInSeconds) {
      cache.set(this.cacheKey(id), JSON.stringify(doc), { ttl: ttlInSeconds })
    }

    return doc
  }

  async update(id, newDoc) {
    try {
      await this.db.update(id, newDoc)
      this.cache.delete(this.cacheKey(id))
    } catch (error) {
      this.didEncounterError(error)
    }
  }
}
```

Letâ€™s look at each part:

```js
  constructor(dbClient) {
    super()
    this.db = dbClient
    this.loader = new DataLoader(ids => dbClient.getByIds(ids))
  }
```

The constructor saves the db client as an instance variable to be used later. It also creates an instance of `DataLoader` to use for this request (a new data source object will be created for each GraphQL request). DataLoader needs to know how to fetch a list of documents by their IDs. Here weâ€™re assuming the array of documents that `getByIds()` returns is in the same order and has the same length as `ids` (a requirement of DataLoader); otherwise, weâ€™d need to reorder them.

[DataLoader](https://github.com/graphql/dataloader) is a library that does batching and memoization caching for the queries our data source makes within a single GraphQL request. **Batching** converts multiple database requests for individual documents into a single request for all the documents, and **memoization caching** deduplicates multiple requests for the same document.

```js
  initialize({ context, cache } = {}) {
    this.context = context
    this.cache = cache || new InMemoryLRUCache()
  }
```

`initialize()` is called automatically by Apollo Server. If Apollo Server has been configured with a global cache, we use that; otherwise, we create an in-memory cache.

```js
  didEncounterError(error) {
    throw error
  }
```

When an error occurs, we call `this.didEncounterError()`, which a child class can override.

```js
  cacheKey(id) {
    return `foo-${this.db.connectionURI}-${id}`
  }
```

We use the `connectionURI` in the cache key to avoid collisions. A collision could occur if there were a global cache and multiple Foo data sources connected to different Foo databases, and one database had a document with the same ID as a document in another database.

```js
  async get(id, { ttlInSeconds } = {}) {
    const cacheDoc = await cache.get(this.cacheKey(id))
    if (cacheDoc) {
      return JSON.parse(cacheDoc)
    }

    const doc = await this.loader.load(id)

    if (ttlInSeconds) {
      cache.set(this.cacheKey(id), JSON.stringify(doc), { ttl: ttlInSeconds })
    }

    return doc
  }
```

We provide a `get(id)` method to be used in resolvers, with an optional `ttlInSeconds` if the caller wants the result to be cached. First, we check if the doc is already in the cache. If it is, we parse it (cache values are always strings) and return it. Then we ask DataLoader to get the document. It will: 

- Take all the calls to `.load(id)`. (The resolverâ€”or other resolversâ€”might be calling `.get()` around the same time as this is running.)
- Deduplicate them (when `.get()` is called multiple times with the same ID).
- Put all the distinct IDs into an array for a batch request (the call to `dbClient.getByIds()` in the constructor).

Once the batch request completes, DataLoader returns on this line the one document we need:

```js
    const doc = await this.loader.load(id)
```

Then if `ttlInSeconds` was provided, we cache the document for that length of time. And finally, we return it!

```js
  async update(id, newDoc) {
    try {
      await this.db.update(id, newDoc)
      this.cache.delete(this.cacheKey(id))
    } catch (error) {
      this.didEncounterError(error)
    }
  }
```

We provide an `update(id, newDoc)` method to be used in resolvers. After a successful update, it deletes the old document from the cache. Another possible implementation would be to overwrite the previous cache entry with `newDoc`â€”in this case, weâ€™d need a value for `ttl` and could add a third argument to `update()` with a `ttlInSeconds`.

Once we have the parent class complete, we can use it by creating one or more child classes. In the case of Foo, weâ€™d create one for each database, but with some data sources we might do one for each table or collection. 

Hereâ€™s an example child class:

```js
import FooDataSource from './FooDataSource'
import { reportError } from './utils'

export default class MyFooDB extends FooDataSource {
  async updateFields(id, fields) {
    const doc = await this.get(id)
    return this.update(id, {
      ...doc,
      ...fields
    })
  }
  
  didEncounterError(error) {
    reportError(error)
  }
}
```

The child class overrides `didEncounterError` to use its own error reporting service instead of throwing. It adds a new method that calls the parentâ€™s `.get()` and `.update()`. When we create the data source, we give the database client to the constructor:

```js
import FooClient from 'imaginary-foo-library'

import MyFooDB from './MyFooDB'

const fooClient = new FooClient({ uri: 'https://foo.graphql.guide:9001' })

const dataSources = () => ({
  myFoos: new MyFooDB(fooClient)
})
```

And now inside our resolvers, we can use `context.dataSources.myFoos` and all the methods defined in the parent class (`FooDataSource`) and child class (`MyFooDB`):

```js
const resolvers = {
  Query: {
    getFoo: (_, { id }, context) => 
      context.dataSources.myFoos.get(id, { ttlInSeconds: 60 })
  },
  Mutation: {
    updateFoo: async (_, { id, fields }, context) => {
      if (context.isAdmin) {
        context.dataSources.myFoos.updateFields(id, fields)
      }
    }
  }
}
```

These example resolvers use `.get()` from `FooDataSource` and `.updateFields()` from `MyFooDB`.

## Prisma

This section will be written after the release of [Prisma 2](https://www.notion.so/Is-Prisma-2-Ready-8b3fba3eaf5b4bf3ab7102fd94f56148).

# Extended topics

* [Mocking](11.md#mocking)
* [Pagination](11.md#pagination)
* [File uploads](11.md#file-uploads)
* [Schema design](11.md#schema-design)
* [Apollo federation](11.md#apollo-federation)
* [Schema change validation](11.md#schema-change-validation)
* [Subscription design](11.md#subscription-design)
* [Auth options](11.md#auth-options)
* [Security](11.md#security)
* [Caching](11.md#caching)
* [Custom schema directives](11.md#custom-schema-directives)
* [Performance](11.md#performance)
* [Future](11.md#future)

This section includes miscellaneous server topics that we didnâ€™t get to in the main-line [Building](11.md#building) tutorial, the [Testing](11.md#testing) sequence, the [Production](11.md#production) section, or the [data sources](#more-data-sources) section. Some topics are short, and some are long (yes, we knowâ€”the length of this chapter is ridiculous ðŸ˜†). Most of the code will be branched off of 25, the end of the Testing sequence.

## Mocking

> If youâ€™re jumping in here, `git checkout 25_0.1.0` (tag [25_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/25_0.1.0), or compare [25...mocking](https://github.com/GraphQLGuide/guide-api/compare/25_0.1.0...mocking_0.1.0))

Mocking API responsesâ€”providing the client with fake (mock) dataâ€”is easy in GraphQL because we have a schema that tells us the structure of the data and the type of each field. And itâ€™s super easy with Apollo Serverâ€”we just add `mock: true`:

```js
const server = new ApolloServer({
  typeDefs,
  resolvers,
  mock: true
})
```

Apollo needs to know how to mock custom types, so we need a mock `Date` for our app:

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...mocking_0.2.0)

```js
const mocks = {
  Date: () => new Date()
}

const server = new ApolloServer({
  typeDefs,
  resolvers,
  dataSources,
  context,
  formatError,
  mocks
})
```

Now when we make a `reviews` query, all the fields we select get returned with mock data:

![reviews query with mock strings and numbers](img/mocking-default.png)

If we want them to look more like real data, we can use the [`casual`](https://github.com/boo1ean/casual) library for fake data generation:

```js
import casual from 'casual'

const mocks = {
  Date: () => new Date(),
  Review: () => ({
    text: casual.sentence,
    stars: () => casual.integer(0, 5)
  }),
  User: () => ({
    firstName: casual.first_name,
    lastName: casual.last_name,
    username: casual.username,
    email: casual.email,
    photo: `https://placekitten.com/100/100`
  })
}
```

![reviews query with data generated by casual](img/mocking-casual.png)

To make the results array have a variable number of results (the default is two items for all lists), we could add this to make it return between 0 and 3 items:

```js
import { ApolloServer, MockList } from 'apollo-server'

const mocks = {
  ...
  Query: () => ({
    reviews: () => new MockList([0, 3])
  })
```

If we created a new app with mocking, and then we wanted to start writing real resolvers, we could add `resolvers` and `mockEntireSchema: false`:

```js
const server = new ApolloServer({
  typeDefs,
  mocks,
  resolvers,
  mockEntireSchema: false
})
```

Then our resolvers would be used first, and mocks would be used for all the fields for which we hadnâ€™t yet written resolvers.

We can also mock a schema written in a different language than JavaScript or a schema from a third-party GraphQL API. First we download `graphql-cli`, and then we use it to download the target APIâ€™s schema:

```sh
$ npm i -g graphql-cli
$ graphql get-schema -e https://api.spacex.land/graphql -o schema.json
```

Then we start a simple Apollo Server:

```js
const { buildClientSchema } = require('graphql')
const introspectionResult = require('./schema.json')
const { ApolloServer } = require('apollo-server')

const schema = buildClientSchema(introspectionResult.data)

const server = new ApolloServer({
  schema,
  mocks: true
})

server.listen().then(({ url }) => {
  console.log(`Server ready at ${url}`)
})
```

To test it, we do:

```sh
$ git clone https://github.com/GraphQLGuide/mock-external-schema.git
$ cd mock-external-schema
$ npm install
$ npm start
```

And we open [localhost:4000](http://localhost:4000) to issue a query:

![SpaceX query with mocked results](img/mocking-external.png)

## Pagination

* [Offset-based](#offset-based)
* [Cursors](#cursors)
  * [after an ID](#after-an-id)
  * [Relay cursor connections](#relay-cursor-connections)

Pagination is the general term for requesting chunks of a list of data instead of the whole list, because requesting the whole list would take too much time or resources. In [Chapter 6: Paginating](6.md#paginating), we covered different types of pagination from the clientâ€™s perspective. In this section, weâ€™ll cover them from the serverâ€™s perspective: Defining the schema and writing code that fetches the requested chunk of data from the database.

These are the main types of pagination:

- *Offset-based*: Request a chunk at an offset from the beginning of the list.
  - *Pages*: Request Nth page of a certain size. For instance, `page: 3, size: 10` would be items 21-30.
  - *Skip & limit*: Request *limit* items after skipping *skip* items. For instance `skip: 40, limit: 20` would be items 41-60.
- *Cursor-based*: Request a chunk before or after a *cursor*. Conceptually, a cursor is a pointer to a location in a queryâ€™s result set. Thereâ€™s a range of ways to implement it, both in terms of what arguments are used and how the schema looks. Here are a couple options:
  - *after an ID*: Request *limit* items *after* some sortable field, like `id`â€”in MongoDB, ObjectIds sort by the time they were created, like a `createdAt` timestamp. This is the simplified, cursor-like system used in [Chapter 6: Cursors](6.md#cursors). For instance `after: '5d3202c4a044280cac1e2f60', limit: 10` would be the 10 items after that `id`.
  - *Relay cursor connections*: Request the *first* N items *after* an opaque cursor (or *last* N items *before* a cursor). For instance, `first: 10, after: 'abcabcabc'`, where `'abcabcabc'` contains an encoded result set location.

> In Chapter 6, we used `[id]:[sort order]` as the cursor format (like `'100:createdAt_DESC'`). However, itâ€™s best practice for the client to treat cursors as opaque strings, and thatâ€™s usually facilitated by the server Base64-encoding the string. So the server would return `'MTAwOmNyZWF0ZWRBdF9ERVND'` as the cursor instead of `'100:createdAt_DESC'`.

The downsides to offset-based are:

- When the result set changes (items added or removed), we might miss or get duplicate results. (We discuss this scenario in [Chapter 6: skip & limit](6.md#skip-&-limit).)
- The performance of a `LIMIT x OFFSET y` query does not scale well for large data sets in many databases, including PostgreSQL, MySQL, and MongoDB. (Note that depending on the flexibility of our collection structure, we might be able to use [the bucket pattern](https://www.mongodb.com/blog/post/paging-with-the-bucket-pattern--part-1) in MongoDB to scale this query well.)

The downsides to cursor-based are:

- We canâ€™t jump ahead, for example, from page 1 to page 5.
- The implementation is a little more complex.

In [Offset-based](#offset-based), weâ€™ll implement skip & limit. Then in [Cursor-based](#cursor-based), weâ€™ll implement [after an ID](#after-an-id) and [Relay cursor connections](#relay-cursor-connections).

### Offset-based

> If youâ€™re jumping in here, `git checkout 25_0.1.0` (tag [25_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/25_0.1.0), or compare [25...pagination](https://github.com/GraphQLGuide/guide-api/compare/25_0.1.0...pagination_0.1.0))

In skip & limit, we have three arguments: `skip`, `limit`, and `orderBy`. Letâ€™s update the schema first, then the resolver, and lastly the data sources.

For `orderBy`, we need a new enum type. The `skip` and `limit` arguments are integers. We can set default values for each so that we can make each argument nullable.

Hereâ€™s the current `reviews` Query:

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...pagination_0.2.0)

```gql
extend type Query {
  reviews: [Review!]!
}
```

Here we add the arguments:

```gql
enum ReviewOrderBy {
  createdAt_ASC 
  createdAt_DESC
}

extend type Query {
  reviews(skip: Int, limit: Int, orderBy: ReviewOrderBy): [Review!]!
}
```

The convention for enum values is `ALL_CAPS`, but `createdAt_ASC` makes it more clear than `CREATED_AT_ASC` that itâ€™s sorting by the `Review.createdAt` field. The subsequent underscore and all-caps `ASC/DESC` still demonstrate theyâ€™re enum values.

> Learn the rules so you know how to break them properly.
> â€”The Dalai Lamaâ€™s Fifth Rule of Living

Our resolver is currently very simple:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...pagination_0.2.0)

```js
export default {
  Query: {
    reviews: (_, __, { dataSources }) => dataSources.reviews.all()
  },
  ...
}
```

We need to add the arguments and check them. GraphQL execution adequately checks `orderBy` (so we know it will either be the string `'createdAt_DESC'` or `'createdAt_ASC'`), but it only checks that `skip` and `limit` are integers. We also need to make sure theyâ€™re not invalid or restricted values. It doesnâ€™t make sense for `skip` to be less than 0, nor for `limit` to be less than 1. Weâ€™ll also prevent large values of `limit` to protect against [denial of service attacks](11.md#denial-of-service).

```js
const MAX_PAGE_SIZE = 100

export default {
  Query: {
    reviews: (
      _,
      { skip = 0, limit = 10, orderBy = 'createdAt_DESC' },
      { dataSources }
    ) => {
      const errors = {}

      if (skip < 0) {
        errors.skip = `must be non-negative`
      }

      if (limit < 1) {
        errors.limit = `must be positive`
      }

      if (limit > MAX_PAGE_SIZE) {
        errors.limit = `cannot be greater than ${MAX_PAGE_SIZE}`
      }

      if (!isEmpty(errors)) {
        throw new InputError({ review: errors })
      }

      return dataSources.reviews.getPage({ skip, limit, orderBy })
    }
  },
  ...
}
```

Lastly, call a new data source method `getPage`, which weâ€™ll define next. Hereâ€™s our old `.all()` method:

[`src/data-sources/Reviews.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...pagination_0.2.0)

```js
export default class Reviews extends MongoDataSource {
  all() {
    return this.collection.find().toArray()
  }
  ...
}
```

We replace it with:

```js
export default class Reviews extends MongoDataSource {
  getPage({ skip, limit, orderBy }) {
    return this.collection
      .find()
      .sort({ _id: orderBy === 'createdAt_DESC' ? -1 : 1 })
      .skip(skip)
      .limit(limit)
      .toArray()
  }
  
  ...
}
```

> `_id` is an ObjectId, so sorting by `_id` is equivalent to sorting by a `createdAt` timestamp.

Letâ€™s first test the error case in Playground:

![Errors with skip: -1 and limit: 101](img/skiplimit-invalid-args.png)

And with default arguments, we see the most recent 10 reviews:

![reviews query response has reviews #12 through #3](img/skiplimit-default-args.png)

And with `skip: 5, limit: 3, orderBy: createdAt_ASC`, we see the 6th through 8th reviews: 

![reviews query response has reviews #6, #7, and #8](img/skiplimit-all-args.png)

### Cursors

There are a number of ways to do cursor-based pagination:

- [`after` an ID](#after-an-id): Use three arguments to support cursor-like pagination for queries sorted by a single field (`createdAt`).
- `first/after & last/before`: `first` and `last` are equivalent to `limit`, and `after/before` is the cursor. These are added as arguments, but the client has to get the cursor from the server, which requires adding a `cursor` field to the schema. We can do this a few ways:
  1. Add `cursor` to each object.
  2. Have each paginated query return a `startCursor`, an `endCursor`, and `nodes`.
  3. Use Relay cursor connections, where the paginated query returns edges, which each contain a `cursor` and a `node`.

In this section, we will implement `after` an ID and Relay cursor connections.

\#1 would have `Review.cursor`:

```gql
type Review {
  id: ID!
  author: User!
  text: String!
  stars: Int
  fullReview: String!
  createdAt: Date!
  updatedAt: Date!
  cursor: String
}

enum ReviewOrderBy {
  createdAt_ASC
  createdAt_DESC
}

extend type Query {
  reviews(first: Int, after: String): [Review!]!
  get(id: ID!): Review
}
```

One downside to this approach is the cursor isnâ€™t really part of a Reviewâ€™s data. For instance, itâ€™s not applicable when we do a `get` Query to fetch a single Review by ID.

\#2 would fix that issue, since the cursor is no longer a Review field:

```gql
type ReviewsResult {
  nodes: [Review!]!
  startCursor: String!
  endCursor: String!
}

extend type Query {
  reviews(first: Int, after: String, last: Int, before: String): ReviewsResult!
  get(id: ID!): Review
}
```

We could also add information about the data setâ€”the total number of items and whether there are more items available to query:

```gql
type ReviewsResult {
  nodes: [Review!]!
  startCursor: String!
  endCursor: String!
  totalCount: Int!
  hasNextPage: Boolean!
  hasPreviousPage: Boolean!
}
```

\#3 has the most involved schema, which weâ€™ll go over in [the last section](#relay-cursor-connections):

```gql
type ReviewEdge {
  cursor: String!
  node: Review
}

type PageInfo {
  startCursor: String!
  endCursor: String!
  hasNextPage: Boolean!
  hasPreviousPage: Boolean!
}

type ReviewsConnection {
  edges: [ReviewEdge]
  pageInfo: PageInfo!
  totalCount: Int!
}

extend type Query {
  reviews(first: Int, after: String, last: Int, before: String): ReviewsConnection!
  get(id: ID!): Review
}
```

The main two benefits to #3 over #2 are:

- We have the cursor of every objectâ€”not just the start and end cursorsâ€”so we can request the next page starting at any location in the list.
- We can add more information to the edge. For instance if we had a social platform with a paginated `User.friends` field returning a `FriendsConnection` with `edges: [FriendEdge]`, a `FriendEdge` could include:

```gql
type FriendEdge {
  cursor: String!
  node: Friend
  becameFriendsOn: Date
  mutualFriends: [Friends]
  photosInCommon: [Photo]
}
```

#### after an ID

> If youâ€™re jumping in here, `git checkout pagination_0.1.0` (tag [pagination_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/pagination_0.1.0), or compare [pagination...pagination2](https://github.com/GraphQLGuide/guide-api/compare/pagination_0.1.0...pagination2_0.1.0))

In this section weâ€™ll do a limited cursor-like pagination with these three arguments:

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/compare/pagination_0.2.0...pagination2_0.2.0)

```gql
extend type Query {
  reviews(after: ID, limit: Int, orderBy: ReviewOrderBy): [Review!]!
}
```

The only change from [skip & limit](#skip-&-limit) is instead of *skip*ing a number of results, we return those *after* an ID. In our resolver, we change `skip -> after` and remove `skip`â€™s error checking:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/pagination_0.2.0...pagination2_0.2.0)

```js
export default {
  Query: {
    reviews: (
      _,
      { after, limit = 10, orderBy = 'createdAt_DESC' },
      { dataSources }
    ) => {
      const errors = {}

      if (limit < 0) {
        errors.limit = `must be non-negative`
      }

      if (limit > MAX_PAGE_SIZE) {
        errors.limit = `cannot be greater than ${MAX_PAGE_SIZE}`
      }

      if (!isEmpty(errors)) {
        throw new InputError({ review: errors })
      }

      return dataSources.reviews.getPage({ after, limit, orderBy })
    }
  },
  ...
}
```

> We could also check whether `after` is a valid `ObjectId` (as we do in the `Query.user` resolver).

In the data source, if `after` is provided (itâ€™s optional), we filter using either `$lt` or `$gt` (less than / greater than):

[`src/data-sources/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/pagination_0.2.0...pagination2_0.2.0)

```js
import { ObjectId } from 'mongodb'

export default class Reviews extends MongoDataSource {
  getPage({ after, limit, orderBy }) {
    const filter = {}
    if (after) {
      const afterId = ObjectId(after)
      filter._id =
        orderBy === 'createdAt_DESC' ? { $lt: afterId } : { $gt: afterId }
    }

    return this.collection
      .find(filter)
      .sort({ _id: orderBy === 'createdAt_DESC' ? -1 : 1 })
      .limit(limit)
      .toArray()
  }

  ...
}
```

To test, first letâ€™s get the first 5 reviews with their IDs:

![reviews query with limit: 5, showing reviews #12 â€“ #8](img/afterlimit-initial.png)

Then we take the last ID and use it for the `after` argument:

![reviews query with after, showing reviews #7 â€” #3](img/afterlimit-after.png)

It works! If we wanted to paginate the other way from review #7, we would switch the `orderBy`:

![reviews query with after and orderBy, showing reviews #8 â€” #12](img/afterlimit-after-orderby.png)

#### Relay cursor connections

> If youâ€™re jumping in here, `git checkout pagination2_0.1.0` (tag [pagination2_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/pagination2_0.1.0), or compare [pagination2...pagination3](https://github.com/GraphQLGuide/guide-api/compare/pagination2_0.1.0...pagination3_0.1.0))

Relay cursor connections are defined by the [Relay Cursor Connections spec](https://facebook.github.io/relay/graphql/connections.htm). It specifies a standard way of implementing cursor pagination so that different clients and tools (like the Relay client library) can depend on that specific schema structure. Its benefits over other cursor structures are listed at the end of the [Cursors](#cursors) section above. Its cost is a more complex schema, like this one:

[`src/schema/Review.graphql`](https://github.com/GraphQLGuide/guide-api/compare/pagination2_0.2.0...pagination3_0.2.0)

```gql
type ReviewEdge {
  cursor: String!
  node: Review
}

type PageInfo {
  startCursor: String!
  endCursor: String!
  hasNextPage: Boolean!
  hasPreviousPage: Boolean!
}

type ReviewsConnection {
  edges: [ReviewEdge]
  pageInfo: PageInfo!
  totalCount: Int!
}

extend type Query {
  reviews(first: Int, after: String, last: Int, before: String): ReviewsConnection!
}
```

Including both `first/after` and `last/before` is optionalâ€”according to the spec, only one is required. Also, we can add fieldsâ€”for instance, `totalCount` isnâ€™t in the specâ€”and add arguments to `Query.reviews` (for instance, filtering and sorting arguments). Common added arguments include a `filterBy` object type and `orderBy`, which can be an `enum` as weâ€™ve been doing or a list (for example `orderBy: [stars_DESC, createdAt_ASC]`). Letâ€™s do just `first/after`, `orderBy`, and a single filter fieldâ€”`stars`:

```gql
extend type Query {
  reviews(first: Int, after: String, orderBy: ReviewOrderBy, stars: Int): ReviewsConnection!
}
```

For implementing the resolver, first we check arguments:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/pagination2_0.2.0...pagination3_0.2.0)

```js
export default {
  Query: {
    reviews: async (
      _,
      { first = 10, after, orderBy = 'createdAt_DESC', stars },
      { dataSources }
    ) => {
      const errors = {}

      if (first !== undefined && first < 1) {
        errors.first = `must be non-negative`
      }

      if (first > MAX_PAGE_SIZE) {
        errors.first = `cannot be greater than ${MAX_PAGE_SIZE}`
      }

      if (stars !== undefined && ![0, 1, 2, 3, 4, 5].includes(stars)) {
        errors.stars = `must be an integer between 0 and 5, inclusive`
      }

      if (!isEmpty(errors)) {
        throw new InputError({ review: errors })
      }

      // â€¦ TODO

      return {
        edges,
        pageInfo: {
          startCursor,
          endCursor,
          hasNextPage,
          hasPreviousPage
        },
        totalCount
      }
    }
  },
  ...
}
```

Then, after some work (which will include one or more calls to `dataSources.reviews.*`), we return an object matching the `ReviewsConnection` in our schema:

```gql
type ReviewsConnection {
  edges: [ReviewEdge]
  pageInfo: PageInfo!
  totalCount: Int!
}
```

Hereâ€™s how to construct that object:

```js
import { encodeCursor } from '../util/pagination'

export default {
  Query: {
    reviews: async (
      _,
      { first = 10, after, orderBy = 'createdAt_DESC', stars },
      { dataSources }
    ) => {
      ...

      const {
        reviews,
        hasNextPage,
        hasPreviousPagePromise
      } = await dataSources.reviews.getPage({ first, after, orderBy, stars })

      const edges = reviews.map(review => ({
        cursor: encodeCursor(review),
        node: review
      }))

      return {
        edges,
        pageInfo: {
          startCursor: encodeCursor(reviews[0]),
          endCursor: encodeCursor(reviews[reviews.length - 1]),
          hasNextPage,
          hasPreviousPage: hasPreviousPagePromise
        },
        totalCount: dataSources.reviews.getCount({ stars })
      }
    }
  },
```

`dataSources.reviews.getPage()` returns an object with three things. We use `reviews` to create the edges and cursors. Each field returned from a resolver can either be a value or a Promise that resolves to a value (Apollo Server will resolve the Promise for us if that field is selected in the query). Instead of a boolean for `hasPreviousPage`, we return a Promise. And for `totalCount`, we call a new data source method `getCount()`:

[`src/data-sources/Reviews.js`](https://github.com/GraphQLGuide/guide-api/compare/pagination2_0.2.0...pagination3_0.2.0)

```js
export default class Reviews extends MongoDataSource {
  getCount(filter) {
    return this.collection.find(filter).count()
  }

  ...
}
```

The code for `getPage()` is a bit complex. Weâ€™ll make three database queries to fetch the list of reviews and determine whether there are next and previous pages:

```js
import { decodeCursor } from '../util/pagination'

export default class Reviews extends MongoDataSource {
  getPage({ first, after, orderBy, stars }) {
    const isDescending = orderBy === 'createdAt_DESC'
    const filter = {}
    const prevFilter = {}

    if (after) {
      const afterId = decodeCursor(after)
      filter._id = isDescending ? { $lt: afterId } : { $gt: afterId }
      prevFilter._id = isDescending ? { $gte: afterId } : { $lte: afterId }
    }

    if (stars) {
      filter.stars = stars
    }

    const sort = { _id: isDescending ? -1 : 1 }

    const reviewsPromise = this.collection
      .find(filter)
      .sort(sort)
      .limit(first)
      .toArray()

    const hasNextPagePromise = this.collection
      .find(filter)
      .sort(sort)
      .skip(first)
      .hasNext()

    const hasPreviousPagePromise =
      !!after &&
      this.collection
        .find(prevFilter)
        .sort(sort)
        .hasNext()

    return { reviewsPromise, hasNextPagePromise, hasPreviousPagePromise }
  }
  
  ...
}
```

The reviews query has:

```js
  .limit(first)
  .toArray()
```

Whereas to see if thereâ€™s a next item, we do:

```js
  .skip(first)
  .hasNext()
```

And to check if thereâ€™s a previous item, we use the opposite `filter` (`$gte` and `$lte` are greater/less than or equal to) and `hasNext()`:

```js
  prevFilter._id = isDescending ? { $gte: afterId } : { $lte: afterId }
  ...
  this.collection
    .find(prevFilter)
    .sort(sort)
    .hasNext()
```

If the number of database queries became a performance problem, we could remove the need for the second by changing `.limit(first)` in the reviews query to `.limit(first + 1)`. Then, if we receive `first + 1` results, we know thereâ€™s a next page:

```js
    ...

    const reviews = await this.collection
      .find(filter)
      .sort(sort)
      .limit(first + 1)
      .toArray()

    const hasNextPage = reviews.length > first
    if (hasNextPage) {
      reviews.pop()
    }

    const hasPreviousPagePromise =
      !!after &&
      this.collection
        .find(prevFilter)
        .sort(sort)
        .hasNext()

    return { reviews, hasNextPage, hasPreviousPagePromise }
  }
```

We do `reviews.pop()` to take the extra last review (which the client didnâ€™t request) off the list. 

Now we have a new issue: Our latency has gone up, since weâ€™re making two database queries in serial (`await`ing one before starting the other) instead of three queries in parallel (initiating them all at the same time). To fix this, we can create the `hasPreviousPagePromise` before the `await`:

```js
    const hasPreviousPagePromise =
      !!after &&
      this.collection
        .find(prevFilter)
        .sort(sort)
        .hasNext()

    const reviews = await this.collection
      .find(filter)
      .sort(sort)
      .limit(first + 1)
      .toArray()

    const hasNextPage = reviews.length > first
    if (hasNextPage) {
      reviews.pop()
    }

    return { reviews, hasNextPage, hasPreviousPagePromise }
  }
```

If, however, we were more concerned with database load than latency, and clients frequently made reviews queries without selecting `Query.reviews.pageInfo.hasPreviousPage`, then we could make those queries only trigger a single database query. We can do this by moving `hasPreviousPage` from a property in an object returned by the `Query.reviews` resolver (what weâ€™re currently doing) to a `PageInfo.hasPreviousPage` resolver:

```js
    â€¦

    const getHasPreviousPage = () =>
      !!after &&
      this.collection
        .find(prevFilter)
        .sort(sort)
        .hasNext()

    return { reviews, hasNextPage, getHasPreviousPage }
  }
```

And then we update the resolvers:

[`src/resolvers/Review.js`](https://github.com/GraphQLGuide/guide-api/compare/pagination2_0.2.0...pagination3_0.2.0)

```js
export default {
  Query: {
    reviews: async (
      _,
      { first = 10, after, orderBy = 'createdAt_DESC', stars },
      { dataSources }
    ) => {
      ...

      const {
        reviews,
        hasNextPage,
        getHasPreviousPage
      } = await dataSources.reviews.getPage({ first, after, orderBy, stars })

      const edges = reviews.map(review => ({
        cursor: encodeCursor(review),
        node: review
      }))

      return {
        edges,
        pageInfo: {
          startCursor: encodeCursor(reviews[0]),
          endCursor: encodeCursor(reviews[reviews.length - 1]),
          hasNextPage,
          getHasPreviousPage
        },
        totalCount: dataSources.reviews.getCount({ stars })
      }
    }
  },
  PageInfo: {
    hasPreviousPage: ({ getHasPreviousPage }) => getHasPreviousPage()
  },
  ...
}
```

Apollo Server first calls the `Query.reviews` resolver, which returns a `ReviewsConnection` that includes a `PageInfo` object without a `hasPreviousPage` property. Instead, Apollo Server will call the `PageInfo.hasPreviousPage` resolver. This resolver receives as its first argument the `pageInfo` sub-object that the resolver above returned, so it can call the `getHasPreviousPage()` function, which either immediately returns a boolean (when thereâ€™s no `after` argument) or initiates a database query and returns a Promise. If the `hasPreviousPage` field isnâ€™t selected in the GraphQL query, the resolver wonâ€™t be called, and the database query wonâ€™t be sent.

Letâ€™s try out a query:

![reviews query with first: 3, stars: 5](img/connections-initial.png)

We see there are 11 total reviews with 5 stars, starting with review #2, and there are no previous pages (`pageInfo.hasPreviousPage` is false). If we want to request the next 3 reviews after review #4, we use `pageInfo.endCursor` as the next queryâ€™s `after`:

![reviews query with first, after, and stars](img/connections-after.png)

And we get reviews #5â€“7 ðŸ’ƒâ˜ºï¸.

Lastly, letâ€™s look at the cursor creating and decoding:

[`src/util/pagination.js`](https://github.com/GraphQLGuide/guide-api/compare/pagination2_0.2.0...pagination3_0.2.0)

```js
import { ObjectId } from 'mongodb'

export const encodeCursor = review =>
  Buffer.from(review._id.toString()).toString('base64')

export const decodeCursor = cursor =>
  ObjectId(Buffer.from(cursor, 'base64').toString('ascii'))
```

We take the reviewâ€™s `_id` property and base64-encode it, and then decode it back to an [ASCII](https://en.wikipedia.org/wiki/ASCII) string, which we convert to an ObjectId. 

Using `_id` works because we only support ordering by createdAt. If we had `orderBy: updatedAt_DESC`, then the cursor would need to contain the reviewâ€™s `updatedAt` property. To differentiate between the two, we could encode an object instead of just an ID string:

```js
export const encodeCursor = (review, orderBy) => {
  const cursorData = ['updatedAt_DESC', 'updatedAt_ASC'].includes(orderBy)
    ? { updatedAt: review.updatedAt }
    : { _id: review._id }

  return Buffer.from(JSON.stringify(cursorData)).toString('base64')
}

export const decodeCursor = cursor =>
  JSON.parse(Buffer.from(cursor, 'base64').toString('ascii'))
```

Also, for either of our encoding systems to work, the client has to continue sending the `orderBy` and `stars` arguments (so that the server knows what MongoDB query filter and sort to use). If we wanted the client to be able to just send `first` and `after`, then we would need to encode the ordering and filtering arguments in cursors. Then the server could decode the information later when receiving a cursor as an `after` argument:

```js
export const encodeCursor = (review, orderBy, stars) => {
  const cursorData = {
    _id: review._id,
    updatedAt: review.updatedAt,
    orderBy,
    stars
  }

  return Buffer.from(JSON.stringify(cursorData)).toString('base64')
}
```

## File uploads

Originally, web servers saved files to their hard drives or to colocated file servers. Most modern web servers use a third-party file-storage service like Amazon S3 or Cloudinary. When a user wants to upload a file, there are a few different ways the client can get it to a storage service:

- [Client-side](#client-side): The client sends the file directly to the storage service.
  - Signed: Our API server gives a signature to the client to give to the storage service along with the file. If our API server doesnâ€™t give the client a signature (for any reasonâ€”for example the client isnâ€™t logged in, or the logged-in user doesnâ€™t have upload permissions), then the storage service wonâ€™t accept the file.
  - Unsigned: Our server is not involved, and the storage service accepts any file from any client.
- [Server-side](#server-side): The client sends the file to our server, and we forward it to the storage service.
  - Through GraphQL: The file goes through our GraphQL endpoint.
  - Outside GraphQL: We create a separate endpoint or server for the file to go through.

We recommend unsigned client-side file uploads unless the lack of signatures becomes a problem. If it does, we suggest switching to signed client-side. We prefer unsigned file uploads because theyâ€™re the easiest to set up. And the client-side upload process is faster than server-side and reduces load on the GraphQL server.

Not all storage services support client-side uploads, and among those that do, only some support unsigned uploads. S3, for instance, doesnâ€™t really support it (we can configure an S3 bucket for public write access, but then anyone can delete user uploads). Cloudinary not only supports unsigned uploads, but they also take security measures to prevent abuse.

In the first section weâ€™ll go over client-side uploads, and in [the second](#server-side) weâ€™ll do server-side through GraphQL.

### Client-side

> If youâ€™re jumping in here, `git checkout 25_0.1.0` (tag [25_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/25_0.1.0), or compare [25...files](https://github.com/GraphQLGuide/guide-api/compare/25_0.1.0...files_0.1.0))

In this section weâ€™ll add the server code to support an unsigned client-side uploadâ€”and at the end, weâ€™ll show the additional code needed for a signed upload. All we need is a mutation for the client to tell the server the filename, ID, or path, depending on which file-storage service weâ€™re using. If we wanted to make it general-purpose, we could use the fileâ€™s full URL instead. For the Guide, weâ€™ll use Cloudinary, which gives the client the fileâ€™s path after the upload is complete (the client-side upload process is [described in Chapter 6](6.md#client-side)). The server then combines the pathâ€”for example `v1551850855/jeresig.jpg`â€”with our account URL (`https://res.cloudinary.com/graphql/`) to form the full URL: 

[https://res.cloudinary.com/graphql/v1551850855/jeresig.jpg](https://res.cloudinary.com/graphql/v1551850855/jeresig.jpg)

Weâ€™ll use the file-upload feature to allow users to add a profile photo (instead of using their current GitHub photo), so weâ€™ll call the mutation `setMyPhoto` and add it to `User.graphql`:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...files_0.2.0)

```gql
extend type Mutation {
  ...
  setMyPhoto(path: String!): User!
}
```

Since `setMyPhoto` will be changing a `User` field, we return the modified `User` object.

In the resolver, we check if the client is logged in and call a new data source method `setPhoto()`:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...files_0.2.0)

```js
export default {
  ...
  Mutation: {
    createUser: ...,
    setMyPhoto(_, { path }, { user, dataSources }) {
      if (!user) {
        throw new ForbiddenError('must be logged in')
      }

      return dataSources.users.setPhoto(path)
    }
  }
}
```

The method constructs the full photo URL, saves it to the database, and returns the updated user object: 

[`src/data-sources/Users.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...files_0.2.0)

```js
export default class Users extends MongoDataSource {
  ...

  async setPhoto(path) {
    const { user } = this.context
    const photo = `https://res.cloudinary.com/graphql/${path}`
    await this.collection.updateOne({ _id: user._id }, { $set: { photo } })
    return {
      ...user,
      photo
    }
  }
}
```

Now that some user documents will contain a `photo` field, we need to update our resolver:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...files_0.2.0)

```js
export default {
  ...
  User: {
    id: ...,
    email: ...,
    photo(user) {
      if (user.photo) {
        return user.photo
      }

      // user.authId: 'github|1615'
      const githubId = user.authId.split('|')[1]
      return `https://avatars.githubusercontent.com/u/${githubId}`
    },
    createdAt: ...
  },
  Mutation: {
    createUser: ...,
    setMyPhoto: ...
  }
}
```

We return early if the `user` object fetched from the database has a `photo` property. 

We can test out the mutation in Playground with either a valid Authorization header or by hard coding the `authId` in `src/context.js`:

![setMyPhoto mutation in Playground](img/setMyPhoto.png)

If we wanted to do signed client-side upload, weâ€™d need to make a Query for the client to fetch the signature. Our resolver would call [cloudinary.utils.api_sign_request()](https://cloudinary.com/documentation/upload_images#using_cloudinary_server_side_sdks_to_generate_authentication_signatures) like this:


```js
export default {
  Query: {
    ...
    uploadSignature(_, { uploadParams }, { user }) {
      if (!user) {
        throw new ForbiddenError('must be logged in')
      }

      return cloudinary.utils.api_sign_request(uploadParams, CLOUDINARY_API_SECRET)
    }
  }
}
```

Then the client would send the signature along with the file to Cloudinaryâ€™s servers (and we would disable unsigned uploads in our Cloudinary account settings). 

If we were using Amazon S3, then weâ€™d use the [`s3.createPresignedPost()`](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createPresignedPost-property) function to create the signature.

### Server-side

> If youâ€™re jumping in here, `git checkout files_0.1.0` (tag [files_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/files_0.1.0), or compare [files...files2](https://github.com/GraphQLGuide/guide-api/compare/files_0.1.0...files2_0.1.0))

We go over the differences between client-side and server-side [above](#file-uploads). In this section, weâ€™ll do server-side file uploads, where the client sends the file to the GraphQL server, which sends it to the storage service (we could send to Cloudinary again, but weâ€™ll use Amazon S3 this time for diversity). There are different methods for the client to send the file, and the most common is a multipart HTTP request, which works through:

- an [`Upload`](https://www.apollographql.com/docs/apollo-server/data/file-uploads/) scalar provided by Apollo Server
- the Apollo Link [`apollo-upload-client`](https://github.com/jaydenseric/apollo-upload-client) on the client side

We create a mutation with an argument of type `Upload`:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/files_0.2.0...files2_0.2.0)

```gql
extend type Mutation {
  createUser(user: CreateUserInput!, secretKey: String!): User
  setMyPhoto(path: String!): User!
  uploadMyPhoto(file: Upload!): User!
}
```

Weâ€™ll need an instance of the AWS S3 client library ([`aws-sdk`](https://aws.amazon.com/sdk-for-node-js/)) to upload to S3:

[`src/util/s3.js`](https://github.com/GraphQLGuide/guide-api/blob/files2_0.2.0/src/util/s3.js)

```js
import AWS from 'aws-sdk'

export default new AWS.S3()
```

Weâ€™ll import and use it in the resolver:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/files_0.2.0...files2_0.2.0)

```js
import s3 from '../util/s3'

const IMAGE_MIME_TYPES = ['image/jpeg', 'image/png', 'image/gif', 'image/webp']

export default {
  ...
  Mutation: {
    ...
    uploadMyPhoto: async (_, { file }, { user, dataSources }) => {
      if (!user) {
        throw new ForbiddenError('must be logged in')
      }

      const { createReadStream, filename, mimetype } = await file

      if (!IMAGE_MIME_TYPES.includes(mimetype)) {
        throw new InputError({ file: 'must be an image file' })
      }

      const stream = createReadStream()
      const { Location: fileUrl } = await s3
        .upload({
          Bucket: 'guide-user-photos',
          Key: filename,
          Body: stream
        })
        .promise()

      return dataSources.users.setPhoto(fileUrl)
    }
  }
}
```

We first check if the user is logged in, then we check the file type (valid values taken from a [list of MIME types](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types)), and then we create a Node.js file stream, which we pass to [`s3.upload()`](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#upload-property) along with the filename and S3 *bucket* (the top-level folder in S3, and the subdomain of the fileâ€™s URL). Finally, we call the data source `setPhoto()` method, which used to take a path, but letâ€™s refactor it to take a full URL:

[`src/data-sources/Users.js`](https://github.com/GraphQLGuide/guide-api/compare/files_0.2.0...files2_0.2.0)

```js
export default class Users extends MongoDataSource {
  ...
  
  async setPhoto(photo) {
    const { user } = this.context
    await this.collection.updateOne({ _id: user._id }, { $set: { photo } })
    return {
      ...user,
      photo
    }
  }
}
```

Changing the parameter means we need to update where we used it previously:

[`src/resolvers/User.js`](https://github.com/GraphQLGuide/guide-api/compare/files_0.2.0...files2_0.2.0)

```js
export default {
  ...
  Mutation: {
    createUser...
    setMyPhoto(_, { path }, { user, dataSources }) {
      if (!user) {
        throw new ForbiddenError('must be logged in')
      }

      return dataSources.users.setPhoto(
        `https://res.cloudinary.com/graphql/${path}`
      )
    },
    uploadMyPhoto...
  }
}
```

We pass the full cloudinary URL instead of just the path.

In order for the AWS SDK to authenticate our account, we need to add `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to our `.env`.

> To test this section yourself, you need an AWS account, a bucket created in the [S3 management console](https://s3.console.aws.amazon.com/s3/home), and access keys created in the [Identity and Access Management console](https://console.aws.amazon.com/iam/home). Youâ€™d replace `'guide-user-photos'` in `src/resolvers/User.js` with your bucket name, and youâ€™d put your own access keys in `.env`. Then youâ€™d write [a test like this](https://github.com/jaydenseric/graphql-upload/blob/b70a67dd4d0aee4eeccbd261ae6105a2bace418e/test/lib/graphqlUploadExpress.test.js#L37-L64) or create a small web app that used [`apollo-upload-client`](https://github.com/jaydenseric/apollo-upload-client) to send a file in an `uploadMyPhoto` Mutation.

When the `uploadMyPhoto` Mutation is run, the upload is successful, and the server saves a URL like this in the `photo` field of the current userâ€™s MongoDB document:

`https://guide-user-photos.s3.amazonaws.com/filename.jpg`

## Schema validation

In this section weâ€™ll go over schema validation and how to set it up using [Apollo Graph Manager](https://www.apollographql.com/docs/graph-manager/). 

There are three places where our server is currently doing things that we might call schema validation: 

- `gql` parses our [SDL](https://www.apollographql.com/docs/apollo-server/schema/schema/#the-schema-definition-language) strings and throws errors when theyâ€™re invalid.
- On startup, `ApolloServer` checks the `typeDefs` it receives to see if our whole schema is valid, according to the GraphQL spec. 
- While running, `ApolloServer` validates queries against the schema.

However, usually the term *schema validation* refers to schema-change validation: i.e., ascertaining whether a *change* to a schema is valid. When we deploy a schema and clients use it, and we then change the schema and want to re-deploy, we can first use schema validation to check if the change is valid. â€œValidâ€ in this context can have different meanings. We could say itâ€™s invalid if any of the changes are backward incompatible. However, sometimes we want to make backward-incompatible changes. So, often â€œvalidâ€ means the changes will work with X% of queries in the last N days. The default for Apollo Graph Manager is 100% of queries in the last seven days. This way, backward-incompatible changes can be made as long as no clients have selected the changed field within the past week.

`graphql-inspector` is a command-line tool for [finding breaking or dangerous changes](https://graphql-inspector.com/docs/essentials/diff), and [GraphQL Doctor](https://github.com/cap-collectif/graphql-doctor) is a GitHub app that does the same for pull requests, comparing the PRâ€™s schema against the schema in `master`. However, we recommend using Graph Manager if you can (the validation feature requires a paid plan). Its method of validating against the query patterns of our clients is more broadly useful, and itâ€™s easy to use from the command line, in continuous integration, and in GitHub PRs.

The first step to setting up Graph Manager is setting the env var `ENGINE_API_KEY` to the value we get from our [Graph Manager account](https://engine.apollographql.com/). We already added it to our `.env` in the [Analytics](#analytics) section. Having `ENGINE_API_KEY` configures the `apollo` command-line tool, which we use for schema registration and validation, and it enables metrics reporting (which we need for validation, because validation is based on clientsâ€™ queries, which are collected metrics).

The second step we also did in the Analytics section: Registering our schema with Graph Manager. Letâ€™s assume we have our app running in production at `api.graphql.guide`. We would register the production schema with:

```
$ npx apollo service:push --endpoint="https://api.graphql.guide/graphql" --tag=prod
```

We use `--tag` to denote the *variant*. Graph Manager tracks variants of schemas, each with their own metrics and schema history. So the above command says to Apollo: â€œIntrospect the schema at `api.graphql.guide` and save it as the latest version of our 'prod' schema variant.â€ 

> Registration has other uses beyond validationâ€”it also powers the [Apollo VS Code extension](https://marketplace.visualstudio.com/items?itemName=apollographql.vscode-apollo) and Graph Managerâ€™s schema history and analytics.

Then, when we make changes to our schema, before we push to production, we check to see whether the change is valid by running `npm run dev` in one terminal and the following in another:

```
$ npx apollo service:check --endpoint="http://localhost:4000/graphql" --tag=prod
```

This says, â€œIntrospect the schema of the server running on port 4000 of my machine and validate it against the latest production schema.â€ It will output either success or a list of which changes fail validation, like this:

```
$ npx apollo service:check ...
  âœ” Loading Apollo Project
  âœ” Validated local schema against tag prod on service engine
  âœ” Compared 8 schema changes against 110 operations over the last 7 days
  âœ– Found 2 breaking changes and 3 compatible changes
    â†’ breaking changes found

FAIL    ARG_REMOVED                `Query.searchUsers` arg `term` was removed
FAIL    FIELD_REMOVED              `Review.stars` was removed

PASS    FIELD_ADDED                `Review.starCount` was added
PASS    ARG_ADDED                  `Query.searchUsers` arg `partialName` was added
PASS    TYPE_REMOVED               `ReviewComment` removed
PASS    FIELD_DEPRECATION_REMOVED  `Review.text` is no longer deprecated

View full details at: https://engine.apollographql.com/service/example-123/check/foo
```

Given the validation failure, we would know to not push to production. 

We can save ourselves time and the risk of forgetting to run the validation command by automating itâ€”for instance, with the [Apollo Engine GitHub App](https://github.com/apps/apollo-engine) or with a continuous integration service like CircleCI:

`.circleci/config.yml`

```yml
version: 2

jobs:
  validate_against_production:
    docker:
      - image: circleci/node:8

    steps:
      - checkout

      - run: npm install

      - run:
          name: Starting server
          command: npm start
          background: true

      # Wait for server to start up
      - run: sleep 5

      - run: npx apollo service:check --endpoint="http://localhost/graphql" --serviceName=users --tag=prod
```

Validating Apollo federation services is similar, and weâ€™ll see how in the [Managed federation](#managed-federation) section below.

## Apollo federation

- [Federated service](11.md#federated-service)
- [Federated gateway](11.md#federated-gateway)
- [Extending entities](11.md#extending-entities)
- [Managed federation](11.md#managed-federation)
- [Deploying federation](11.md#deploying-federation)

In the [Introduction](#introduction) to this chapter, we talk about microservices versus monoliths. If we go down the microservice route, then the best way to do it is with Apollo federation. 

Apollo federation is a specification for how to divide our schema across different services. Each service describes which parts of the schema it implements, and a gateway combines all the parts into one larger schema. The gateway stands between the client and the services, receiving requests from the client and automatically resolving them through one or more requests to services.

The Apollo federation specification can be implemented in any language and has been added to many [existing GraphQL server libraries](https://www.apollographql.com/docs/apollo-server/federation/other-servers/). Those servers that follow the specification are the services, and the gateway is a special instance of Apollo Server that uses the [`@apollo/gateway`](https://www.apollographql.com/docs/apollo-server/api/apollo-gateway/) library.

In the first three sections, weâ€™ll rebuild our Guide server monolith using federation: Weâ€™ll start with a users service, then the gateway, and then the reviews service. Then in [Managed federation](11.md#managed-federation), weâ€™ll see how we can benefit from Apolloâ€™s Graph Manager SaaS product, and finally in [Deploying federation](11.md#deploying-federation), weâ€™ll discuss the deployment of the gateway and services.

### Federated service

> If youâ€™re jumping in here, `git checkout federation_0.1.0` (tag [federation_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/federation_0.1.0), or compare [federation...federation2](https://github.com/GraphQLGuide/guide-api/compare/federation_0.1.0...federation2_0.1.0))

In this section weâ€™ll build a users service: A GraphQL server that supports Apollo federation and handles queries related to the `User` type. Weâ€™ll start from a new tag:

```sh
$ git checkout federation_0.1.0
```
Here is our starting file structure:

```sh
$ tree -L 3
.
â”œâ”€â”€ babel.config.json
â”œâ”€â”€ lerna.json
â”œâ”€â”€ lib
â”‚   â”œâ”€â”€ Date.js
â”‚   â”œâ”€â”€ auth.js
â”‚   â”œâ”€â”€ db.js
â”‚   â””â”€â”€ errors.js
â”œâ”€â”€ package.json
â””â”€â”€ services
    â”œâ”€â”€ reviews
    â”‚   â””â”€â”€ package.json
    â””â”€â”€ users
        â””â”€â”€ package.json
```

The two services will go in the `services/` folder, and `lib/` contains code to share between the services (taken from the monolith we built earlier). Letâ€™s install all the modules we need:

```sh
$ npm install
```

This creates a `node_modules/` at the rootâ€”which has modules for the gateway code that weâ€™ll place at the rootâ€”and it also creates `node_modules/` folders inside `services/reviews/` and `services/users/` thanks to the [Lerna library](https://lerna.js.org/), which we configure in `lerna.json` and use in a `postinstall` script in [`package.json`](https://github.com/GraphQLGuide/guide-api/blob/federation_0.2.0/package.json):

```json
{
  "name": "guide-api",
  "version": "0.1.0",
  "scripts": {
    "start": "babel-watch gateway.js",
    "start-service-users": "babel-watch services/users/index.js",
    "start-service-reviews": "babel-watch services/reviews/index.js",
    "start-services": "concurrently \"npm:start-service-*\"",
    "postinstall": "lerna bootstrap"
  },
  ...
}
```

We also see from the scripts where weâ€™ll locate the main server files: 

```
gateway.js
services/users/index.js
services/reviews/index.js
```

`concurrently` runs multiple other scripts in the same terminalâ€”in this case, both `start-service-users` and `start-service-reviews`.

In this section, weâ€™ll be filling in `services/users/*`. There are three main parts to a federated service:

- `buildFederatedSchema()`: Instead of passing `typeDefs` and `resolvers` directly to `ApolloServer()`, we give them to the `buildFederatedSchema()` from the `@apollo/federation` library.
- *Entities*: Types defined in one service that can be referenced or extended by other services.
  - `@key` directive: Each entity requires a `@key` directive denoting the primary key.
  - `__resolveReference()`: For each entity, we must write a reference resolver, which fetches an entity object by its `@key` field(s).

As usual, letâ€™s start with the schema:

[`services/users/schema.js`](https://github.com/GraphQLGuide/guide-api/blob/federation2_0.2.0/services/users/schema.js)

```js
import { gql } from 'apollo-server'

export default gql`
  scalar Date

  extend type Query {
    me: User
    user(id: ID!): User
  }

  type User @key(fields: "id") {
    id: ID!
    firstName: String!
    lastName: String!
    username: String!
    email: String
    photo: String!
    createdAt: Date!
    updatedAt: Date!
  }
`
```

We include shared types like custom scalars in the schema of each service. Also, the `Query` and `Mutation` types will be initially defined in the gateway, so the services `extend` them. Finally, our `User` type has this directive: `@key(fields: "id")`, which tells the gateway that the `User` type is a federation entity and the `id` field is its primary key.

We copy the below from our monolithâ€™s `src/resolvers/User.js` with a couple of additions:

- Adding the `Date` resolvers, imported from `lib/Date.js`
- Adding `User.__resolveReference`

[`services/users/resolvers.js`](https://github.com/GraphQLGuide/guide-api/blob/federation2_0.2.0/services/users/resolvers.js)

```js
import { ForbiddenError } from 'apollo-server'
import { ObjectId } from 'mongodb'

import { InputError } from '../../lib/errors'
import Date from '../../lib/Date'

const OBJECT_ID_ERROR =
  'Argument passed in must be a single String of 12 bytes or a string of 24 hex characters'

export default {
  ...Date,
  Query: {
    me: (_, __, context) => context.user,
    user: (_, { id }, { dataSources }) => {
      try {
        return dataSources.users.findOneById(ObjectId(id))
      } catch (error) {
        if (error.message === OBJECT_ID_ERROR) {
          throw new InputError({ id: 'not a valid Mongo ObjectId' })
        } else {
          throw error
        }
      }
    }
  },
  User: {
    __resolveReference: (reference, { dataSources }) =>
      dataSources.users.findOneById(ObjectId(reference.id)),
    id: ({ _id }) => _id,
    email(user, _, { user: currentUser }) {
      if (!currentUser || !user._id.equals(currentUser._id)) {
        throw new ForbiddenError(`cannot access others' emails`)
      }

      return user.email
    },
    photo(user) {
      // user.authId: 'github|1615'
      const githubId = user.authId.split('|')[1]
      return `https://avatars.githubusercontent.com/u/${githubId}`
    },
    createdAt: user => user._id.getTimestamp()
  }
}
```

The first argument to `__resolveReference` is the reference: An object containing the primary key field(s)â€”in this case, just the `id`â€”which we resolve to the user object.

Now we put the resolvers and schema together to create the server:

[`services/users/index.js`](https://github.com/GraphQLGuide/guide-api/blob/federation2_0.2.0/services/users/index.js)

```js
import { ApolloServer } from 'apollo-server'
import { buildFederatedSchema } from '@apollo/federation'
import { MongoDataSource } from 'apollo-datasource-mongodb'

import resolvers from './resolvers'
import typeDefs from './schema'
import { mongoClient } from '../../lib/db'
import context from '../../lib/userContext'

const server = new ApolloServer({
  schema: buildFederatedSchema([
    {
      typeDefs,
      resolvers
    }
  ]),
  dataSources: () => ({
    users: new MongoDataSource(mongoClient.db().collection('users'))
  }),
  context
})

mongoClient.connect()

server.listen({ port: 4001 }).then(({ url }) => {
  console.log(`Users service ready at ${url}`)
})
```

Here we see the use of `buildFederatedSchema()`. Also, the only data source method we use is `.findOneById()`, so we can use `MongoDataSource` directly instead of defining a subclass. `mongoClient` we get from `db.js`:

[`lib/db.js`](https://github.com/GraphQLGuide/guide-api/blob/federation_0.2.0/lib/db.js)

```js
import { MongoClient } from 'mongodb'

const URL = 'mongodb://localhost:27017/guide'

export const mongoClient = new MongoClient(URL)
```

Finally, our `context` function needs to provide a `user` object for the `Query.me` resolver. Our monolith context function looked at the `authorization` header, decoded the `authId`, and fetched the user object. Instead of having each of our services repeat this process, we can have our gateway do part or all of it. We can either do:

1. Gateway decodes `authId` and passes it to services as an `auth-id` header. Services read the header and fetch the user document.
2. Gateway decodes `authId`, connects to the user database to fetch the user document, and passes it to services as a `user` header. 
3. The JWT thatâ€™s sent in the authorization header from the client can be created to contain the whole user document, so that when itâ€™s decoded, no database query is required.

Our JWTs donâ€™t have the whole user document, so we canâ€™t do #3. Between #1 and #2, #2 is more efficient, as it reduces the number of database calls. Note that #2 isnâ€™t possible when the user document is large. The maximum header size is set by the receiving server, for instance Nginx has a maximum 4KB, which is ~4,000 ASCII characters. (We can check the length of a user document by doing `JSON.stringify(user).length`.) Here is the service side of #2:

[`lib/userContext.js`](https://github.com/GraphQLGuide/guide-api/blob/federation2_0.2.0/lib/userContext.js)

```js
module.exports = async ({ req }) => {
  const context = {}

  const userDocString = req && req.headers['user']
  if (userDocString) {
    context.user = JSON.parse(userDocString)
  }

  return context
}
```

Now we can set the `user` HTTP header and both `Query.user` and `Query.me` work:

```
$ npm run start-service-users

> guide-api@0.1.0 start-service-users /guide-api
> babel-watch services/users/index.js

Users service ready at http://localhost:4001/
```

![user and me queries with user HTTP header](img/user-service.png)

### Federated gateway

> If youâ€™re jumping in here, `git checkout federation2_0.1.0` (tag [federation2_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/federation2_0.1.0), or compare [federation2...federation3](https://github.com/GraphQLGuide/guide-api/compare/federation2_0.1.0...federation3_0.1.0))

In the last section we implemented the users service. In this section, weâ€™ll implement the gateway. The basic process is creating an `ApolloGateway()` that points to a list of the services, and then giving that to `ApolloServer()`:

[`gateway.js`](https://github.com/GraphQLGuide/guide-api/blob/federation3_0.2.0/gateway.js)

```js
import { ApolloServer } from 'apollo-server'
import { ApolloGateway } from '@apollo/gateway'

const gateway = new ApolloGateway({
  serviceList: [
    { name: 'users', url: 'http://localhost:4001/graphql' },
  ]
})

const server = new ApolloServer({
  gateway,
  subscriptions: false
})

server.listen().then(({ url }) => {
  console.log(`Gateway ready at ${url}`)
})
```

We disable subscriptions because they donâ€™t yet work with `ApolloGateway`. This works, but itâ€™s not yet sending the `user` HTTP header our `users` service expects. This takes two steps: copying our monolithâ€™s context function to give to `ApolloServer()` and defining a `buildService()` function to add the header in requests to services:

```js
import { ApolloServer } from 'apollo-server'
import { ApolloGateway, RemoteGraphQLDataSource } from '@apollo/gateway'

import context from './context'
import { mongoClient } from './lib/db'

class AuthenticatedDataSource extends RemoteGraphQLDataSource {
  willSendRequest({ request, context }) {
    request.http.headers.set('user', JSON.stringify(context && context.user))
  }
}

const gateway = new ApolloGateway({
  serviceList: [
    { name: 'users', url: 'http://localhost:4001/graphql' },
    { name: 'reviews', url: 'http://localhost:4002/graphql' }
  ],
  buildService({ url }) {
    return new AuthenticatedDataSource({ url })
  }
})

const server = new ApolloServer({
  gateway,
  context,
  subscriptions: false
})

mongoClient.connect()

server.listen().then(({ url }) => {
  console.log(`Gateway ready at ${url}`)
})
```

`buildService()` returns an `AuthenticatedDataSource` which sets the stringified user doc from the context as a header. `willSendRequest()` is then called for each request from the gateway to the services. We also import `mongoClient` in order to initiate the connection and import context from:

[`context.js`](https://github.com/GraphQLGuide/guide-api/blob/federation3_0.2.0/context.js)

```js
import { AuthenticationError } from 'apollo-server'

import { getAuthIdFromJWT } from './lib/auth'
import { mongoClient } from './lib/db'

export default async ({ req }) => {
  const context = {}

  const jwt = req && req.headers.authorization
  let authId

  if (jwt) {
    try {
      authId = await getAuthIdFromJWT(jwt)
    } catch (e) {
      let message
      if (e.message.includes('jwt expired')) {
        message = 'jwt expired'
      } else {
        message = 'malformed jwt in authorization header'
      }
      throw new AuthenticationError(message)
    }

    const user = await mongoClient
      .db()
      .collection('users')
      .findOne({ authId })
    if (user) {
      context.user = user
    } else {
      throw new AuthenticationError('no such user')
    }
  }

  return context
}
```

The only difference between this and the monolithâ€™s version is importing `mongoClient` instead of the `db` directly.

We can now run our `users` service and gateway in two different terminals:

```sh
$ npm run start-service-users

> guide-api@0.1.0 start-service-users /guide-api
> babel-watch services/users/index.js

Users service ready at http://localhost:4001/
```

```sh
$ npm start

> guide-api@0.1.0 start /guide-api
> babel-watch gateway.js

Gateway ready at http://localhost:4000/
[INFO] Wed Mar 1 2020 04:55:43 GMT-0400 (EST) apollo-gateway: Gateway successfully loaded schema.
        * Mode: unmanaged
```

When we open the gateway URL, set our authorization header, and query, it works! ðŸ’ƒ

![user and me queries with authorization header](img/user-through-gateway.png)

### Extending entities

> If youâ€™re jumping in here, `git checkout federation3_0.1.0` (tag [federation3_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/federation3_0.1.0), or compare [federation3...federation4](https://github.com/GraphQLGuide/guide-api/compare/federation2_0.1.0...federation4_0.1.0))

In this section, weâ€™ll build another serviceâ€”this one for reviewsâ€”and weâ€™ll see how to extend entities created by other services. Then, weâ€™ll add the reviews service to the gateway and see how the gateway resolves queries involving both services.

Letâ€™s start with the schema. First, we take the `Review` type and `reviews` query from our monolith for our new schema, and then we add a few things:

[`services/reviews/schema.js`](https://github.com/GraphQLGuide/guide-api/blob/federation4_0.2.0/services/reviews/schema.js)

```js
import { gql } from 'apollo-server'

export default gql`
  scalar Date

  type Review @key(fields: "id") {
    id: ID!
    text: String!
    stars: Int
    author: User!
    createdAt: Date!
    updatedAt: Date!
  }

  extend type Query {
    reviews: [Review!]!
  }

  extend type User @key(fields: "id") {
    id: ID! @external
    reviews: [Review!]!
  }
`
```

- `scalar Date`, as we did in the `users` service
- `@key` directive for `type Review`, to declare it as a federation entity
- `extend type User`: Here weâ€™re extending the `User` type originally defined externally. We have to include both the `@key` directive as well as the primary key fieldsâ€”in this case just `User.id`â€”with the `@external` directive (signifying that this field was originally defined in another service). The `reviews` field doesnâ€™t have `@external`, which means itâ€™s being added to the `User` type, and weâ€™ll need to write a resolver for it:

[`services/reviews/resolvers.js`](https://github.com/GraphQLGuide/guide-api/blob/federation4_0.2.0/services/reviews/resolvers.js)

```js
import { ObjectId } from 'mongodb'

import Date from '../../lib/Date'

export default {
  ...Date,
  Query: {
    reviews: (_, __, { dataSources }) => dataSources.reviews.all()
  },
  Review: {
    __resolveReference: (reference, { dataSources }) =>
      dataSources.reviews.findOneById(ObjectId(reference.id)),
    id: review => review._id,
    author: review => ({ id: review.authorId }),
    createdAt: review => review._id.getTimestamp()
  },
  User: {
    reviews: (user, _, { dataSources }) =>
      dataSources.reviews.all({ authorId: ObjectId(user.id) })
  }
}
```

These resolvers are taken from our monolith with four additions:

- The `Date` custom scalar resolver.
- The `Review.__resolveReference` resolver, required because this service is the origin of the `Review` entity.
- The `Review.author` resolver, which returns a `reference` (the same reference passed to `__resolveReference` above)â€”an object with an entityâ€™s primary key. The gateway takes this reference and provides it to the `User.__resolveReference` resolver to get the user object.
- The `User.reviews` resolver, which uses the data source `review.all()` method with a MongoDB selector. Speaking of which, we need a `Reviews` data source with a `.all()` method:

[`services/reviews/Reviews.js`](https://github.com/GraphQLGuide/guide-api/blob/federation4_0.2.0/services/reviews/Reviews.js)

```js
import { MongoDataSource } from 'apollo-datasource-mongodb'

export default class Reviews extends MongoDataSource {
  all(query) {
    return this.collection.find(query).toArray()
  }
}
```

Weâ€™ll include this, along with our schema and resolvers, when creating the server:

[`services/reviews/index.js`](https://github.com/GraphQLGuide/guide-api/blob/federation4_0.2.0/services/reviews/index.js)

```js
import { ApolloServer } from 'apollo-server'
import { buildFederatedSchema } from '@apollo/federation'

import resolvers from './resolvers'
import typeDefs from './schema'
import Reviews from './Reviews'
import { mongoClient } from '../../lib/db'
import context from '../../lib/userContext'

const server = new ApolloServer({
  schema: buildFederatedSchema([
    {
      typeDefs,
      resolvers
    }
  ]),
  dataSources: () => ({
    reviews: new Reviews(mongoClient.db().collection('reviews'))
  }),
  context
})

mongoClient.connect()

server.listen({ port: 4002 }).then(({ url }) => {
  console.log(`Reviews service ready at ${url}`)
})
```

We use the same context function as the `users` service and a new port (4002, versus 4001 for the `users` service and the default 4000 for the gateway).

One piece of our old schema that weâ€™re missing is `Review.fullReview`. Since it involves the authorâ€™s name, we need to query the users collection. And the service that is responsible for querying the users collection is the `users` service. So letâ€™s add the field to the `users` service:

[`services/users/schema.js`](https://github.com/GraphQLGuide/guide-api/compare/federation3_0.2.0...federation4_0.2.0)

```js
export default gql`
  ...

  extend type Review @key(fields: "id") {
    id: ID! @external
    fullReview: String!
  }
`
```

Like with `extend type User`, when we `extend type Review`, we repeat the directive and include the primary key field. However, we have an issue: The `fullReview` resolver needs data from the review document (`authorId`, `text`, and `stars`). By default, the resolver will only receive an object with the reviewâ€™s `id` field. 

We can solve this issue with the `@requires` directive:

```js
export default gql`
  ...

  extend type Review @key(fields: "id") {
    id: ID! @external
    text: String! @external
    stars: Int @external
    authorId: ID! @external
    fullReview: String! @requires(fields: "authorId text stars")
  }
`
```

We list the fields we require in order to resolve `fullReview` using `@requires`, and we list those fields above with `@external`. The last issue is that `authorId` isnâ€™t currently part of the `Review` type, so letâ€™s add it to the `reviews` service schema:

[`services/reviews/schema.js`](https://github.com/GraphQLGuide/guide-api/blob/federation4_0.2.0/services/reviews/schema.js)

```js
export default gql`
  scalar Date

  type Review @key(fields: "id") {
    id: ID!
    text: String!
    stars: Int
    authorId: ID!
    author: User!
    createdAt: Date!
    updatedAt: Date!
  }

  ...
`
```

This makes `authorId` appear in the public gateway schema as well, which isnâ€™t ideal, as it unnecessarily clutters the schema, but the ability to define a private, internal field is [a planned addition](https://github.com/apollographql/apollo-server/issues/2812) to the federation spec.

Finally, we can implement the `fullReview` resolver back in the `users` service:

[`services/users/resolvers.js`](https://github.com/GraphQLGuide/guide-api/compare/federation3_0.2.0...federation4_0.2.0)

```js
export default {
  ...
  Review: {
    fullReview: async (review, _, { dataSources }) => {
      const author = await dataSources.users.findOneById(
        ObjectId(review.authorId)
      )
      return `${author.firstName} ${author.lastName} gave ${review.stars} stars, saying: "${review.text}"`
    }
  }
}
```

We add the `reviews` service to our gateway by simply adding it to our `serviceList`:

[`gateway.js`](https://github.com/GraphQLGuide/guide-api/compare/federation3_0.2.0...federation4_0.2.0)

```js
const gateway = new ApolloGateway({
  serviceList: [
    { name: 'users', url: 'http://localhost:4001/graphql' },
    { name: 'reviews', url: 'http://localhost:4002/graphql' }
  ],
  buildService({ url }) {
    return new AuthenticatedDataSource({ url })
  }
})
```

We can run both services with:

```sh
$ npm run start-services
```

And in another terminal run the gateway:

```sh
$ npm start
```

And test! ðŸ™

![reviews query with author.firstName and fullReview](img/reviews-through-gateway.png)

âœ… Here we see both of the jumps from the `reviews` service to the `users` service working: The `reviews` service resolves `Query.reviews` and the `Review.author` reference, and the `users` service resolves the reference into a user, as well as `User.firstName` and `Review.fullReview`.

Next, we can see that going from the `users` service to the `reviews` service works. First the `users` service resolves `Query.user`, and then the `reviews` service resolves `User.reviews`.

![user query with User.reviews selected](img/user-reviews-through-gateway.png)

To see a more detailed explanation of the *query plan*â€”the process by which the gateway determines how to get all the data it needs from the servicesâ€”we can add this last argument to `ApolloGateway()`:

```js
const gateway = new ApolloGateway({
  serviceList...  
  buildService...
  __exposeQueryPlanExperimental: true
})
```

Now inside Playground, we can open the QUERY PLAN tab on the bottom-right:

![Query plan tab in Playground](img/user-reviews-through-gateway.png)

```gql
{
  user(id: "5d24f846d2f8635086e55ed3") {
    id
    firstName
    reviews {
      stars
      text
    }
  }
}
```

The above query results in the below query plan:

```gql
QueryPlan {
  Sequence {
    Fetch(service: "users") {
      {
        user(id: "5d24f846d2f8635086e55ed3") {
          id
          firstName
          __typename
        }
      }
    },
    Flatten(path: "user") {
      Fetch(service: "reviews") {
        {
          ... on User {
            __typename
            id
          }
        } =>
        {
          ... on User {
            reviews {
              stars
              text
            }
          }
        }
      },
    },
  },
}
```

`Sequence` means the following queries are done in sequenceâ€”one after the other. So first it does a `Fetch` from the `users` service, and then a fetch from the `reviews` service.

Our first query involves a `Parallel` in addition to a `Sequence`:

```gql
{
  reviews {
    author {
      firstName
    }
    fullReview
  }
}
```

```gql
QueryPlan {
  Sequence {
    Fetch(service: "reviews") {
      {
        reviews {
          author {
            __typename
            id
          }
          __typename
          id
          authorId
          text
          stars
        }
      }
    },
    Parallel {
      Flatten(path: "reviews.@") {
        Fetch(service: "users") {
          {
            ... on Review {
              __typename
              id
              authorId
              text
              stars
            }
          } =>
          {
            ... on Review {
              fullReview
            }
          }
        },
      },
      Flatten(path: "reviews.@.author") {
        Fetch(service: "users") {
          {
            ... on User {
              __typename
              id
            }
          } =>
          {
            ... on User {
              firstName
            }
          }
        },
      },
    },
  },
}
```

The gateway first fetches from the `reviews` service and then does two fetches from the `users` service for each review, all in parallel. 

We can look at the query plan to diagnose performance issuesâ€”itâ€™s possible that the query plan will show a lot of fetches in series, which increases latency. A fetch in seriesâ€”where the second fetch happens after the first is completeâ€”is denoted by `Sequence`. In the case of bugs, the query plan might also help us discover why the gateway is not working as we expect. 

Another tool we have for diagnosing bugs is our gatewayâ€™s `RemoteGraphQLDataSource`, to which we can add the `didReceiveResponse` method, where we can log responses from the services:

```js
class AuthenticatedDataSource extends RemoteGraphQLDataSource {
  willSendRequest...

  didReceiveResponse({ response, request, context }) {
    console.log('response data:', response.data)
    return response
  }
}
```

Here are further capabilities we arenâ€™t using:

- Having [multiple primary keys](https://www.apollographql.com/docs/apollo-server/federation/entities/#defining-multiple-primary-keys) or [compound primary keys](https://www.apollographql.com/docs/apollo-server/federation/entities/#defining-a-compound-primary-key)
- Resolving other servicesâ€™ fields with the [`@provides`](https://www.apollographql.com/docs/apollo-server/federation/entities/#resolving-another-services-field-advanced) directive
- [Modifying the gatewayâ€™s response](https://www.apollographql.com/docs/apollo-server/federation/implementing/#customizing-outgoing-responses)
- Using [custom directives](https://www.apollographql.com/docs/apollo-server/federation/implementing/#implementing-custom-directives)

### Managed federation

As weâ€™ve been running the gateway, weâ€™ve been seeing the output:

```
        * Mode: unmanaged
```

The default gateway mode is unmanaged. A gateway is *managed* when itâ€™s connected to Apollo Graph Manager, the SaaS tool weâ€™ve used previously for [Analytics](#analytics) and [Schema validation](#schema-validation). `ApolloGateway` will connect to Graph Manager if we set `ENGINE_API_KEY` and make one change to the codeâ€”remove the `serviceList` argument in the constructor:

`gateway.js`

```js
const gateway = new ApolloGateway({
  serviceList: [
    { name: 'users', url: 'http://localhost:4001/graphql' },
    { name: 'reviews', url: 'http://localhost:4002/graphql' }
  ],
  buildService({ url }) {
    return new AuthenticatedDataSource({ url })
  },
  __exposeQueryPlanExperimental: true
})
```

In managed federation, instead of listing the service URLs in the gateway, we register each service with Graph Manager, and the gateway gets the service info from Graph Manager. This has two main benefits: 

1. When we add services, change service URLs, or change service schemas, we donâ€™t need to redeploy the gateway.
2. When thereâ€™s an error with one of the changes in #1, the gateway can automatically fall back to the last working configuration.

We register a service in the same way we registered our monolithâ€™s schema in [Analytics](#analytics) and [Schema validation](#schema-validation)â€”with the `apollo service:push` command:

```sh
$ npx apollo service:push \
    --serviceName=users \
    --serviceURL="http://users.svc.cluster.local:4001/" \
    --endpoint="http://localhost:4001/"
```

We can view the list of services weâ€™ve pushed:

```sh
$ npx apollo service:list
  âœ” Loading Apollo Project
  âœ” Fetching list of services for graph guide-api

name       URL                                      last updated
â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users      http://users.svc.cluster.local:4001/    5 March 2020 (5 days ago)
Reviews    http://reviews.svc.cluster.local:4002/  5 March 2020 (5 days ago)

View full details at: https://engine.apollographql.com/graph/guide-api/service-list
```

To validate the service, we use `--serviceName` with the `apollo service:check` command we used in the [Schema validation](#schema-validation) section:

```sh
$ npx apollo service:check \
    --serviceName=users \
    --endpoint="http://localhost:4001/" \
    --tag=prod \
```

> Just as monolith schemas can have multiple *variants*, denoted by the `--tag` option, so can federated schemas.

This command not only validates the serviceâ€™s schema against recent usage data, but it also checks failed compositionâ€”that is, a failure in the ability to compose the whole federated schema. 

Now we know how to set up Graph Manager with federation and to validate changes to services to make sure they continue to fit into the whole data graph and donâ€™t break clients.

### Deploying federation

The gateway and our services are all just Node.js servers, so we can use any of the deployment options we discussed in the main [Deployment section](#deployment). And Apollo gateway doesnâ€™t yet support subscriptions, so FaaS websocket support isnâ€™t an issue like it was before. One new issue is the recommendation that services not be publicly accessible. Federation services need to expose extra information to work with the gateway (note the added `_service` and `_entities` root query fields), and we might not want people to be able to access it. 

There are a number of different options for deploying services privately, including:

- IaaS or Faas: Amazonâ€™s VPC ([Virtual Private Cloud](https://aws.amazon.com/vpc/)) with either EC2 or Lambda
- PaaS: Herokuâ€™s [Private Spaces](https://www.heroku.com/private-spaces) (requires an Enterprise account)
- Kubernetes [private clusters](https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept)

And if we didnâ€™t care about the information exposure, we could use public-only options like Vercel Now.

There are three steps we usually do around deployment:

- Schema validation (`apollo service:check`)
- Code deployment (various)
- Push new service information to Graph Manager (`apollo service:push`)

Normally, itâ€™s best to do them in the order listedâ€”first checking if the serviceâ€™s schema will fit in the graph and not break queries, then deploying the code, and finally, once the production servers are ready to receive requests, telling the gateway about the updated service. In CircleCI, it would look something like this:

`.circleci/config.yml`

```yml
version: 2

jobs:
  deploy_to_prod:
    docker:
      - image: circleci/node:8

    steps:
      - checkout

      - run: npm install

      - run:
          name: Starting server
          command: npm start
          background: true

      # Wait for server to start up
      - run: sleep 5

      - run: npx apollo service:check --serviceName=users --endpoint="http://localhost/graphql" --tag=prod

      - run: npm run deploy

      # Wait for production servers to restart
      - run: sleep 5

      - run: npx apollo service:push --serviceName=users --endpoint="http://localhost/graphql/" --tag=prod
```

If the `service:check` command fails, the CircleCI build will fail, and `npm run deploy` and subsequent commands wonâ€™t get run.

When a `service:push` is not backward compatible with our gatewayâ€™s query planner (for instance when we change `@key @requires @provides` directives), then we should do the `service:push` *before* deploying. And generally, when we make modifications that affect the query planner, we need to take the steps listed in [Apollo Docs: Modifying query-planning logic](https://www.apollographql.com/docs/graph-manager/managed-federation/advanced-topics/). The article has different instructions for *in-place* versus *atomic* changes. In-place is when we deploy a service to the same domain, whereas atomic is when we deploy a service to a new domain and `service:push` to point the gateway at the new domain. Letâ€™s look at the difference using Vercel Now, which creates a unique URL with every deployment. 

In-place, deploying to the existing `serviceUrl`:

```sh
$ apollo service:push \
    --tag=prod 
    --serviceName=users 
    --endpoint="http://localhost:4001"
$ now --prod
> https://users.api.graphql.guide
> Success! Deployment ready
```

Atomic, changing the `serviceUrl`:

```sh
$ now
> https://users-61h1hvwis.now.sh/
> Success! Deployment ready
$ apollo service:push \
    --tag=prod \
    --serviceName=users \
    --endpoint="http://localhost:4001" \
    --serviceUrl="https://users-61h1hvwis.now.sh/"
```

-------

In summary, we started out this Apollo federation section by building a [users service](11.md#federated-service) and connecting it to [a gateway](11.md#federated-gateway). Then we built a [second service](#extending-entities) for reviews and extended entities. Finally, we learned how to set up [managed federation](11.md#managed-federation) and [how to deploy](11.md#deploying-federation). ðŸš€

## Hasura

Background: [Databases](bg.md#databases), [SQL](bg.md#sql)

[Hasura](https://hasura.io/?ref=guide) is a GraphQL BaaS ([Backend as a Service](https://www.cloudflare.com/learning/serverless/glossary/backend-as-a-service-baas/)). In [Deployment > Options](#options) we covered IaaS, PaaS, and FaaS, which are different ways we can host our code. In BaaS, we donâ€™t have to write codeâ€”the server and database (PostgreSQL in the case of Hasura) are automatically set up based on our configuration.

> While itâ€™s true we donâ€™t *have* to write code, many apps need at least a little custom logic, so there are various ways to write our own code or SQL statements and integrate them into our Hasura serverâ€™s functioning. These waysâ€”which weâ€™ll get to later in this sectionâ€”include actions, triggers, functions, and remote schemas.

*[Note: the rest of this section is forthcoming]*

## Schema design

* [One schema](11.md#one-schema)
* [User-centric](11.md#user-centric)
* [Easy to understand](11.md#easy-to-understand)
* [Easy to use](11.md#easy-to-use)
* [Mutations](11.md#mutations)
  * [Arguments](11.md#arguments)
  * [Payloads](11.md#payloads)
* [Versioning](11.md#versioning)

### One schema

> Ash graph durbatulÃ»k,
> ash graph gimbatul,
> ash graph thrakatulÃ»k,
> agh gateway-ishi krimpatul.
> 
> Inscription upon the Ring of Byron, written in Black Speech. Translates as:
> 
> One graph to rule them all, 
> one graph to find them,
> one graph to bring them all, 
> and in the gateway bind them.

The first principle of schema design is there should only be one schema! While we can *implement* it as smaller schemas and a [federation gateway](11.md#apollo-federation), from the perspective of the client, there should only be one schema (or *data graph*). And while this may seem obvious, there are many large companies whose GraphQL adoption began by independent teams creating their own GraphQL APIs. This results in a lot of duplication of effortâ€”not only duplicated resolvers where the schemas overlap, but also management of the APIs. We also might wind up with clients that need to make requests from two separate endpoints, which our frontend devs might find... inconvenient ðŸ˜„. Which brings us to the first principle of design in general, which is:

### User-centric

**Design things for the people who will be using them.**

The people who will be using our schema are primarily our frontend devs (or, in the case of a public API, the worldâ€™s frontend devs ðŸ˜Š), so we want to design the schema for them. We want our API to be:

- Easy to understand. 
- Easy to use. 
- Hard for devs to make mistakes or create bugs when querying.

Secondarily, our schema is used by our end users (the people using the software written by the frontend devs) and ourselves (the backend devs). For our end users, we take into consideration things like latency (maybe having a single mutation that did two things would get results to the user faster than two mutations that had to be executed serially) or the clarity of error types. For ourselves, we take into consideration how difficult our schema will be to run, secure, and update. For instance, we might decide not to include a query field that would take too much server resources to resolve. Or we might structure parts of the schema to make it easier to add fields later on.

Once weâ€™ve read this section, we can have a meeting with our frontend devs, UX designers, product managers, etc., to create:

- The core types and queries, based on what data the frontend needs.
- Mutations, based on the user action flows. 

We do *not* want to start writing the schema based on backend implementation / naming / structure / tech details. It shouldnâ€™t look like our REST APIs or mirror our database tables.

> One good option for how to structure your schema creation meeting is event storming, a process from [domain-driven design](https://en.wikipedia.org/wiki/Domain-driven_design) described in [this article](https://khalilstemmler.com/articles/graphql/ddd/schema-design/).

Our schema also shouldnâ€™t be perfect or comprehensive. It should only cover the use cases for which itâ€™s needed right nowâ€”we shouldnâ€™t design it based on hypothetical future requirements:

> Fields shouldnâ€™t be added to the schema speculatively. Ideally, each field should be added only in response to a concrete need by a consumer for additional functionality, while being designed for maximum reuse by other consumers that have similar needs.
>
> Updating the graph should be a continuous process. Rather than releasing a new â€œversionâ€ of the graph periodically, such as every 6 or 12 months, it should be possible to change the graph many times a day if necessary. New fields can be added at any time. To remove a field, it is first deprecated, and then removed when no consumers use it.
> â€”[Principled GraphQL](https://principledgraphql.com/agility)

### Easy to understand

We want others to be able to understand our schema just by reading it. We donâ€™t want them to read it, not fully get it, and then have to talk to us or learn through trial and error. Ideally we donâ€™t even want them to have to read schema descriptionsâ€”just the types themselves. Itâ€™s the same reason why itâ€™s easier to understand readable code than commented code. For example:

```js
const resolvers = {
  Mutation: {
    addWineToCart(_, { wineId }, { user }) {
      // first check if user is allowed to drink
      if (new Date(Date.now() - user.dateOfBirth.getTime()).getUTCFullYear() - 1970 < 21) {
        throw new ForbiddenError()
      }

      ...
    }
  }
}
```

The `if` statement condition is complicated and not *readable* (i.e., we donâ€™t immediately understand what it means by glancing at it), so we read the comment above it to learn what the `if` statement does. In the below code, however, we can just read it:

```js
const US_DRINKING_AGE = 21

const context = async ({ req }) => {
  const user = await getUser(req.headers.authorization)

  user.age = function() {
    const millisecondsSinceBirth = Date.now() - this.dateOfBirth.getTime()
    return new Date(millisecondsSinceBirth).getUTCFullYear() - 1970
  }
  user.isAllowedToDrink = function() {
    return user.age() >= US_DRINKING_AGE
  }

  return { user }
}

const resolvers = {
  Mutation: {
    addWineToCart(_, { wineId }, { user }) {
      if (!user.isAllowedToDrink()) {
        throw new ForbiddenError()
      }

      ...
    }
  }
}
```

While this is many more lines of code, thatâ€™s not as important as readability. And all we need to read now is `if (!user.isAllowedToDrink())`, which is readily understandable. At most, we may need to mentally move the location of the â€œnotâ€ from â€œif not user is allowed to drinkâ€ to â€œif user is not allowed to drink.â€œ

For a schema example of this concept, letâ€™s imagine we were building an online store, and we had this mutation:

```gql
type Mutation {
  add(productId: ID!): Cart
  checkout: Order
}
```

Then we realized that while people could probably infer the `add` mutation meant add a product to the cart (given the argument name and return type), it would be clearer if we added a field description:

```gql
type Mutation {
  # add product to cart
  add(productId: ID!): Cart
  checkout: Order
}
```

While the new â€œadd product to cartâ€ description now appears in Playground autocomplete (and in the DOCS tab after clicking `add`), it has a couple downsides:

- It takes us another step to look for and read the description, versus just reading the field name.
- When we read a query document in the client code, we only see the mutation nameâ€”not the description.

We can remove the need for a comment by making the mutation name clearer:

```gql
type Mutation {
  addProductToCart(productId: ID!): Cart
  checkout: Order
}
```

Readability starts with giving clear names to things. In this case, it was giving a full, specific nameâ€”not just `add` or `addProduct`, but `addProductToCart`. Here are a few more examples of specificity:

- Instead of just a `Review` type, use `ProductReview`. Then schema readers know what the review is for, and in the future, we can add other review types, like `StoreReview`, without causing confusion.
- If we have two types of reviews, we shouldnâ€™t try to fit them both into a single type. Instead of `Review`, with the 3rd field for product reviews and the 4th and 5th fields for store reviews, we should have two types with different fields:

```gql
# âœ˜
type Review {
  id: ID!
  stars: Int!
  productReviewText: String
  storeDeliveryRating: Int
  storeCustomerSupportRating: Int
}

# âœ”ï¸Ž
type ProductReview {
  id: ID!
  stars: Int!
  text: String!
}

type StoreReview {
  id: ID!
  stars: Int!
  deliveryRating: Int!
  customerSupportRating: Int
}
```

And if we want to handle them together, we could have them both implement a `Review` interface and reference it:

```
type Query {
  searchReviews(term: String!): [Review!]!
}

interface Review {
  id: ID!
  stars: Int!
}

type ProductReview implements Review {
  id: ID!
  stars: Int!
  text: String!
}

type StoreReview implements Review {
  id: ID!
  stars: Int!
  deliveryRating: Int!
  customerSupportRating: Int
}
```

- Instead of a generic query with a generic argument or a list of optional arguments, make multiple specific queries with non-null arguments:

```gql
# âœ˜
type Query {
  user(fields: UserFieldInput): User!
}

input UserFieldInput {
  id: ID
  username: String
}

# âœ”ï¸Ž
type Query {
  userById(id: ID!): User!
  userByUsername(username: String!): User!
}
```

- The Guide schema uses a `Date` type for milliseconds since epoch. However, it would be more specific to call it a `DateTime`, since it includes both the date and the time. That would allow us to add `Date` (e.g., `1/1/2000`) and `Time` (e.g., `13:37`) types in the future. It would also be clearer for devs who are used to systems that handle both Dates and DateTimes.

Using specific naming is part of a broader category of being explicitâ€”we want to know what fields and types mean, how to use them, and how they behave, without guessing or trial and error. Here are a few further areas in which we can be explicit:

- Using custom scalars instead of default scalars. Instead of `createdAt: Int`, `createdAt: DateTime`. Instead of `phone: String`, `phone: PhoneNumber`. It explicitly shows what type of value it is, and we can trust that the [custom scalar code](11.md#custom-scalars) will validate `DateTime`s and `PhoneNumber`s wherever theyâ€™re used in the schema.
- Include default arguments:

```js
type Query {
  reviews(
    skip: Int = 0,
    limit: Int = 10,
    orderBy: ReviewOrderBy = createdAt_DESC
  ): [Review!]!
}

enum ReviewOrderBy {
  createdAt_ASC 
  createdAt_DESC
}
```

- Use non-null (`!`) to explicitly denote which values will always be returned, or which arguments are required. However, in some cases itâ€™s better to not use it:
  - If clients use multiple root query fields in a single document, then leave them all nullable, because if one is non-null and null is returned (e.g., due to an error), it will [null cascade](#nullability) all the way up to a `{ "data": null }` response, which will prevent the client from receiving the other root query fields.
  - If thereâ€™s any chance a field will occasionally not be available, for instance a `User.githubRepositories` field whose resolver relies on the GitHub API being accessible, make it null. We do this so that when we canâ€™t reach the GitHub API (their servers are down, or thereâ€™s a network issue, or we hit our API quota, for example), queries for user data can receive the other fields.
- Build expected errors into the schema. Then devs will know what error responses look like and will be able to handle them more easily than if they were in the `"errors"` JSON response property.
  - In the below [Mutations](#mutations) section, weâ€™ll include expected errors in the response type.
  - Earlier in the [Union errors](#union-errors) section, we included deleted and suspended users in the search results:

```gql
type Query {
  searchUsers(term: String!): [UserResult!]!
}

union UserResult = User | DeletedUser | SuspendedUser
```

  - We can also prevent errors from happening with our schema structure. For instance, if there are some queries that are public and some for which the client must be logged in, we can prevent them from them receiving unauthorized errors by having the public queries as root fields and the logged-in queries as `Viewer` fields:

```gql
# âœ˜
type Query {
  me: User
  teams: [Team]

  # must be logged in
  projects: [Project]

  # must be logged in
  reports: [Report]
}

# âœ”ï¸Ž
type Query {
  me: Viewer
  teams: [Team]
}

type Viewer {
  id: ID
  name: String
  projects: [Project]
  reports: [Report]
}
```

Only when we canâ€™t make a meaning or behavior explicit should we add a description to the schema.

Lastly, a couple more things that are helpful for readability: 

- Consistency in naming. For instance, how we name queries for a single item versus a list:

```gql
# âœ˜
type Query {
  project(id: ID): Project
  projects: [Project]

  getReport(id: ID): Report
  listReports: [Report]
}

# âœ”ï¸Ž
type Query {
  project(id: ID): Project
  projects: [Project]

  report(id: ID): Report
  reports: [Report]
}
```

Or the verbs we use with mutations:

```gql
# âœ˜
type Mutation {
  deleteProject(id: ID): DeleteProjectPayload
  removeReport(id: ID): RemoveReportPayload
}

# âœ”ï¸Ž
type Mutation {
  deleteProject(id: ID): DeleteProjectPayload
  deleteReport(id: ID): DeleteReportPayload
}
```

- Grouping fields into sub-objects: When a group of fields are related, we can create a new object type. Imagine our reviews had comments that rated the helpfulness of the review:

```gql
# âœ˜
type Review {
  id: ID!
  text: String!
  stars: Int
  commentCount: Int!
  averageCommentRating: Int
  averageCommentLength: Int
}

# âœ”ï¸Ž
type Review {
  id: ID!
  text: String!
  stars: Int
  commentStats: CommentStats!
}

type CommentStats {
  count: Int!
  averageRating: Int
  averageLength: Int
}
```

### Easy to use

While ease of use is determined largely by ease of understanding, there are other factors that can contribute:

- Include fields that save the client from having to go through computation, logic, or other processing. For instance, we provide Review.fullReview:

```js
const resolvers = {
  Review: {
    fullReview: async (review, _, { dataSources }) => {
      const author = await dataSources.users.findOneById(
        review.authorId,
        USER_TTL
      )
      return `${author.firstName} ${author.lastName} gave ${review.stars} stars, saying: "${review.text}"`
    },
  }
}
```

If the client wants the whole review text in a sentence like that, they could construct it themselves by querying for all the pieces of information and putting it together. Instead, we do it for them, saving them the effort. Similarly, if our clients often want the total comment count, we can include that in the connection so they donâ€™t have to do the work of requesting all the comments and counting them:

```gql
type Review {
  id: ID!
  text: String!
  comments: CommentsConnection!
}

type CommentsConnection {
  nodes: [Comment]
  totalCount: Int!
}
```

Or, if we have a purchasing app where orders have complex states and business logic, we could include a `readyForSubmission` field so the client doesnâ€™t have to write the logic code:

```gql
type Order {
  id: ID!
  ...
  readyForSubmission: Boolean!
}
```

- Make fields easy to use. For instance when dealing with money, fractional amounts are often more difficult to work with than integers, so we can provide `Int` fields:

```gql
# âœ˜
type Charge {
  dollars: Float!
}

# âœ”ï¸Ž
type Charge {
  cents: Int!
}
```

- If we have a public API for third parties, then we can make their integration easier by supporting their preferred libraries. In the case of GraphQL, the only common library with schema requirements is Relay. The [list of requirements](https://relay.dev/docs/en/graphql-server-specification.html) includes the cursor connections we [discussed earlier](11.md#relay-cursor-connections), a particular structure to mutations, and a common `Node` interface for object types:

```gql
interface Node {
  id: ID!
}

type User implements Node {
  id: ID!
  firstName: String!
}

type Review implements Node {
  id: ID!
  text: String!
}
```

### Mutations

As with the rest of the schema, the first thing to think about for mutations is their names. While some choose to do `typeVerb` (like `reviewCreate`, `reviewUpdate`, and `reviewDelete`) so that GraphiQLâ€™s alphabetical schema docs will group mutations by type, we recommend the more readable `verbType`: `createReview`, `updateReview`, and `deleteReview`. And, as mentioned before, we recommend verb consistencyâ€”so for example, using `deleteUser` instead of `removeUser` to match `deleteReview`.

However, we donâ€™t recommend uniformly implementing `create|update|delete` mutations for each type. Instead, provide mutations according to the needs of the clientâ€”which actions will they be performing? In some cases, types are never deleted, or theyâ€™re created automatically, or the update step should be named something else or should happen in stages. For instance, imagine a store checkout process in which the server needs to do something (save data, validate, talk to an API, etc.) for each of these steps:

- Create a cart.
- Add products to the cart.
- Apply a coupon code.
- Add shipping address.
- Add payment information.
- Submit order.

We could have the client use `createCart` for the first step and a single generic `updateCart` mutation for each of the rest. (First theyâ€™d call `updateCart(productId)`, and then `updateCart(couponCode)`, etc.) However, it would require a large amount of optional arguments, and we would have to write a long field description telling the dev which arguments to use in which order. Instead, we should write multiple mutations with specific names:

```gql
type Mutation {
  createCart: Cart!
  addProductsToCart(input: AddProductsToCartInput): Cart!
  applyCoupon(input: ApplyCouponInput): Cart!
  addShippingAddressToCart(input: AddShippingAddressToCartInput): Cart!
  addPaymentToCart(input: AddPaymentToCartInput): Cart!
  createOrder(cartId: ID!): Order!
}

input AddProductsToCartInput {
  cartId: ID!
  productIds: [ID!]!
}

input ApplyCouponInput {
  cartId: ID!
  code: String!
}

input AddShippingAddressToCartInput {
  cartId: ID!
  address: AddressInput!
}

input AddPaymentToCartInput {
  cartId: ID!
  payment: PaymentMethodInput!
}
```

- For most of the mutations, we end with `ToCart` to be specific. Just `addProducts` could be adding them to a wishlist, or `addPayment` could be adding a payment method to your account. And if thereâ€™s anything besides a cart to which a coupon might be applied in the future, we should change `applyCoupon` to `applyCouponToCart`!
- We do `addProductsToCart` instead of the singular `addProductToCart` in case the client might want to add multiple products at a time (itâ€™s easier to send a single mutation with an array of IDs than a single-ID mutation many times).

#### Arguments

The most common pattern for mutation arguments is a single input object type. Some people choose to instead have a two-argument limit, when one argument is an ID, like this:

```gql
type Mutation {
  applyCoupon(cartId: ID!, coupon: String!): Cart!
  addShippingAddressToCart(cartId: ID!, address: AddressInput!): Cart!
}
```

A couple benefits of a single argument are:

- The mutation is more readable with a single input object than with a long list of scalars and input objects.
- The input object is more evolvable (we canâ€™t deprecate an argument, but we can deprecate an input object field).

Here are a few more considerations when it comes to mutation arguments:

- Earlier we recommended creating specific scalar types over using built-in generics, but we may want to avoid that for mutation arguments. If we use our own scalar types, then the client may have to go through two requests to discover all the errors. If there are errors in both the scalar validation (for instance, an invalid phone number) and in the business logic (for instance, the order size is too large), then the clientâ€™s first request will only receive the validation error. When they send a second request with a fixed phone number, theyâ€™ll receive the business logic error. We can improve the clientâ€™s experience by allowing them to receive all errors at once, which we do by using `String` instead of our own `PhoneNumber` scalar, and doing both the phone number validation and the business logic checks in our resolver code. Then our resolver can return all the errors together. We also have more flexibility on how we return the errorâ€”a scalar validation error shows up in the `"errors"` attribute of the JSON response, whereas in our resolver, we can either throw an error *or* return an errorâ€”an option weâ€™ll see in the next section.
- The client can generate and provide a unique `clientMutationId` for mutations they want to make sure are *idempotent*â€”that donâ€™t get executed multiple times. For instance, if the client sent the below mutation and then lost internet connection and resent, the server could receive the mutation a second time once the connection is back. To avoid this issue, our server code could check to see if the `clientMutationId` on the second mutation matches the first. If it does, our code wonâ€™t process the second mutation.

```gql
mutation { 
  buyStock(input: { ticker: "TSLA", shares: 10, clientMutationId: "mvvAb9sDGnPYNtZm" }) { 
    id 
  } 
}
```

```gql
type Mutation {
  buyStock(input: BuyStockInput): Order
}

input BuyStockInput {
  ticker: String!
  shares: Int!
  clientMutationId: ID!
}
```

- While itâ€™s tempting to [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) our code by sharing input types between create and update mutations, we donâ€™t recommend it. We have to use at least one non-null field for the ID (since itâ€™s not used during creation), and we have to make all fields non-null if we want to be able to provide the update mutation with just the fields we want to change. However, doing that removes the clarity around which fields are required when creating.

```gql
# âœ˜
mutation {
  createReview(input: ReviewInput!): Review!
  updateReview(input: ReviewInput!): Review!
}

input ReviewInput {
  # only provide when updating
  id: ID
  # required when creating
  text: String
  stars: Int
}

# âœ”ï¸Ž
mutation {
  createReview(input: CreateReviewInput!): Review!
  updateReview(input: UpdateReviewInput!): Review!
}

input CreateReviewInput {
  text: String!
  stars: Int
}

input UpdateReviewInput! {
  id: ID!
  text: String
  stars: Int
}
```

#### Payloads

So far our mutations have been returning the object they alter or throwing errors. For instance, `createReview` might return a `Review` object or throw an `InputError` thatâ€™s serialized in the response JSONâ€™s `"errors"` attribute. However, there are a couple issues with this:

- Returning a single type is inflexibleâ€”what if multiple types are altered during the mutation, or we want to provide the client with more information about how the mutation went?
- As we discussed in [Union errors](11.md#union-errors), itâ€™s better to return expected errors than to throw them: Itâ€™s easier for client code to handle, and it documents the possible errors and their associated data (whereas thrown errors like the [`InputError` we created](#custom-errors) are undocumented / do not appear in the schema).

We solve both of these issues by returning a payload type:

```gql
type Mutation {
  createReview(input: CreateReviewInput): CreateReviewPayload
}

type CreateReviewPayload {
  review: Review
  user: User
  errors: [Error!]!
}

type Error {
  message: String!
  code: ErrorCode
  field: Field
}
```

When we create a review, our `User.reviews` changes. We can include the user in the payload so that the client can easily update their cached user object. We make both the `review` and `user` optional because we might instead return `errors`. The clientâ€™s operation would look like:

```gql
mutation {
  createReview(input: { text: "", stars: 6 }) {
    review {
      id
      text
      stars
      createdAt
    }
    user {
      reviews {
        id
      }
    }
    errors {
      message
      code
      field
    }
  }
}
```

And the response would be:

```json
{
  "data": {
    "createReview": {
      "errors": [{
        "message": "Text cannot be empty",
        "code": 105,
        "field": "input.text"
      }, {
        "message": "Stars must be an integer between 0 and 5, inclusive",
        "code": 106,
        "field": "input.stars"
      }]
    }
  }
}
```

In cases when the mutation alters an unknown set of types, we can use the Query type to allow the client to get back whatever data theyâ€™d like after the mutation is complete:

```gql
type Mutation {
  performArbitraryOperation(operation: ArbitraryOperation): PerformArbitraryOperationPayload
}

type CreateReviewPayload {
  query: Query
  errors: [Error!]!
}
```

### Versioning

Most APIs change over time. We can deploy *backward-compatible* changes at any time. We usually try to avoid making *breaking* changes, i.e., changes that may break client code using that part of the API. However, sometimes we want to make a breaking change because it would be a significant improvement. If our API is only used by our clients, and all our clients are web apps, then we can publish a new version of the client at the same time as a breaking API change, and we can force all the currently loaded webpages (now out of date) to reload, and nothing will be broken. However, if we donâ€™t want to force-reload our web app, or if we have mobile apps (which we canâ€™t force-reload), or if we have a public API (which is used by third parties, whose code we donâ€™t have control over), then we have two options:

- **Global versioning**. Publish a new version of the API at a different URL, like `api.graphql.guide/v2/`. Then clients using the original URL will continue to work.
- **Deprecation**: 
  - Add a deprecation notice so that, going forward, devs donâ€™t use the field.
  - Notify existing API consumers of the deprecation so they can change their code.
  - Monitor the usage of the field.
  - When the field usage falls under a tolerable threshold (number of will-be-broken requests), remove it.

Here are a couple examples of deprecation:

```gql
type User {
  id: ID!
  name: String @deprecated(
    reason: "Replaced by field `fullName`"
  )
  fullName: String
}

type Mutation {
  createReview(text: String!, stars: Int): Review @deprecated(
    reason: "Replaced by field `createReviewV2`"
  )
  createReviewV2(input: CreateReviewInput): CreateReviewPayload  
}
```

While only the deprecation option includes making the breaking change as a step, it usually eventually happens for global versioning as well. There is always a cost of maintaining the old codeâ€”whether the code is backing an earlier global version or a deprecated fieldâ€”and at some point, that cost outweighs the cost of breaking old clients. For instance, we could have a globally versioned API thatâ€™s currently on version 5, and almost all of the clients are using v2â€“v5, and we decide that weâ€™d rather break the few clients still using v1 than continue maintaining it.

We recommend using the deprecation process (also called **continuous evolution**) in lieu of versioning. The downside of deprecating is the schema can get cluttered with deprecated fields. The downside of versioning is the large cost of maintaining old server versions and the increased time it takes to make changes. Given the complexity of deploying and maintaining a new version of the API, we batch changes and create new versions infrequently, whereas we can deprecate at any time.

There are a few reasons why continuous evolution is the better practice compared to versioning, which was common with REST APIs:

- Adding is backward compatible. With REST APIs that donâ€™t have control over what data is returned from an endpoint, any changes, even returning more data than the client expects, can be breaking. With GraphQL APIs, adding a new field doesnâ€™t affect current clientsâ€”they only receive the fields specified in their query document.
- Deprecation is built into the GraphQL spec, and GraphQL tooling will show developers when theyâ€™re using a deprecated field, so clients will update their code more easily and sooner.
- Since all the fields requested are in the query document, we can know how many clients are using deprecated fields. If we added a `fullName` field to the user REST endpoint, we wouldnâ€™t know how many clients were still using the `name` field. With GraphQL, we know!

We can currently deprecate fields and enum values, and deprecating arguments and input fields will likely be added to the spec in the near future.

We deprecate a field instead of removing it because removing a field is a breaking change. But there are other breaking changes to watch out for as well:

- Removing fields, enum values, union members, or interfaces.
- Changing the type of a field.
- Making an argument or input field non-null.
- Adding a new non-null argument or input field.
- Making a non-null argument nullable.
- Changing a field from non-null to nullable isnâ€™t automatically breaking, but if the server ever does return null for that field, the client can break.

Finally, itâ€™s possible to break clients by adding new enum values, union members, and interface implementations if the client logic depends on all the data they receive fitting their (outdated) set of values/members/implementations. Ideally, clients will always leave open the possibility that those things could be added.

## Custom schema directives

Background: [Directives](2.md#directives)

> If youâ€™re jumping in here, `git checkout 25_0.1.0` (tag [25_0.1.0](https://github.com/GraphQLGuide/guide-api/tree/25_0.1.0), or compare [25...directives](https://github.com/GraphQLGuide/guide-api/compare/25_0.1.0...directives_0.1.0))

Apollo Server includes the [default directives](2.md#directives) `@deprecated`, `@skip`, and  `@include`. `@skip` and `@include` are *query directives*, so they donâ€™t appear in our schema; instead, theyâ€™re included in query documents and can be used on any field. `@deprecated` is a *schema directive*, and when we add it after a field or enum value in our schema, the directive will be included in responses to introspection queries. 

We can make our own schema directives in Apollo Server. When we add them to specific places in our schema, those parts of the schema are modified or evaluated differently when resolving requests. Three examples weâ€™ll code are `@tshirt`, which modifies an enum valueâ€™s description; `@upper`, which takes the result of a field resolver and returns the uppercase version instead; and `@auth`, which throws an error if the user isnâ€™t authorized to view that object or field.

- [@tshirt](#@tshirt)
- [@upper](#@upper)
- [@auth](#@auth)

### @tshirt

Schema directives are implemented by subclassing `SchemaDirectiveVisitor` and overriding one or more methods of the format `visitFoo()`, where `Foo` is the part of the schema to which the directive is applied. Possible parts of the schema are:

- Whole schema
- Scalar
- Object
- Field definition
- Argument definition
- Interface
- Union
- Enum
- Enum value
- Input object
- Input field definition

For example, if it were applied to an enum value:

[`src/schema/schema.graphql`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...directives_0.2.0)

```gql
directive @tshirt on ENUM_VALUE

enum Package {
  BASIC
  PRO 
  FULL @tshirt
  TRAINING @tshirt

  # Group license.
  TEAM @tshirt
}
```

Then our subclass would override `visitEnumValue()`:

[`src/directives/TshirtDirective.js`](https://github.com/GraphQLGuide/guide-api/blob/directives_0.2.0/src/directives/TshirtDirective.js)

```js
import { SchemaDirectiveVisitor } from 'apollo-server'

class TshirtDirective extends SchemaDirectiveVisitor {
  visitEnumValue(value) {
    ...
    return value
  }
}
```

To determine the structure of `value`, we can either use `console.log()` or look up the type definition of an enum value in the `graphql-js` library. All type definitions are in [`src/type/definition.js`](https://github.com/graphql/graphql-js/blob/688f93c9153c1b69d522c130200373e75d0cfc7e/src/type/definition.js#L1419-L1427), where we can find:

```js
export type GraphQLEnumValue /* <T> */ = {|
  name: string,
  description: ?string,
  value: any /* T */,
  isDeprecated: boolean,
  deprecationReason: ?string,
  extensions: ?ReadOnlyObjMap<mixed>,
  astNode: ?EnumValueDefinitionNode,
|};
```

> `isDeprecated` and `deprecationReason` are the fields that are used by the `@deprecated` directive.

It has an optional `description` field, to which we can add a note about T-shirts ðŸ˜„:

[`src/directives/TshirtDirective.js`](https://github.com/GraphQLGuide/guide-api/blob/directives_0.2.0/src/directives/TshirtDirective.js)

```js
import { SchemaDirectiveVisitor } from 'apollo-server'

export default class TshirtDirective extends SchemaDirectiveVisitor {
  visitEnumValue(value) {
    value.description += ' Includes a T-shirt.'
    return value
  }
}
```

Then we need to get it to `ApolloServer()`:

[`src/directives/index.js`](https://github.com/GraphQLGuide/guide-api/blob/directives_0.2.0/src/directives/index.js)

```js
import TshirtDirective from './TshirtDirective'

export default {
  tshirt: TshirtDirective
}
```

[`src/index.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...directives_0.2.0)

```js
import schemaDirectives from './directives'

const server = new ApolloServer({
  typeDefs,
  schemaDirectives,
  resolvers,
  dataSources,
  context,
  formatError
})
```

Now we can check the description by using the search box inside Playgroundâ€™s docs tab:

![Package enum with â€œIncludes a T-shirtâ€ descriptions](img/tshirt-directive.png)

### @upper

When weâ€™re making a directive to use on fields, oftentimes what we want to do is call the resolver and modify the result, like this:

```js
import { SchemaDirectiveVisitor } from 'apollo-server'
import { defaultFieldResolver } from 'graphql'

class MyDirective extends SchemaDirectiveVisitor {
  visitFieldDefinition(field) {
    const { resolve = defaultFieldResolver } = field
    field.resolve = async function(...args) {
      const result = await resolve.apply(null, args)
      // modify result
      // ...
      return result
    }
  }
}
```

Here we override the `visitFieldDefinition()` function, which receives a `field` object that [has a `resolve` property](https://github.com/graphql/graphql-js/blob/688f93c9153c1b69d522c130200373e75d0cfc7e/src/type/definition.js#L959-L974):

```js
export type GraphQLField<
  TSource,
  TContext,
  TArgs = { [argument: string]: any, ... },
> = {|
  name: string,
  description: ?string,
  type: GraphQLOutputType,
  args: Array<GraphQLArgument>,
  resolve?: GraphQLFieldResolver<TSource, TContext, TArgs>,
  subscribe?: GraphQLFieldResolver<TSource, TContext, TArgs>,
  isDeprecated: boolean,
  deprecationReason: ?string,
  extensions: ?ReadOnlyObjMap<mixed>,
  astNode: ?FieldDefinitionNode,
|};
```

We redefine `field.resolve`, calling the original resolve or the `defaultFieldResolver`, which resolves the field as a property on the parent object when there is no resolver function (e.g., `User: { firstName: (user, _, context) => user.firstName }`). Then we modify and return the result. 

Letâ€™s use this format to implement an `@upper` resolver, which transforms the result to uppercase:

[`src/schema/schema.graphql`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...directives_0.2.0)

```gql
directive @upper on FIELD_DEFINITION

type Query {
  hello(date: Date): String! @upper
  isoString(date: Date!): String!
}
```

And now, since we canâ€™t convert an emoji to uppercase, we need `Query.hello` to return lowercase ASCII:

[`src/resolvers/index.js`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...directives_0.2.0)

```js
const resolvers = {
  Query: {
    hello: () => 'world ',
    ...
  }
}
```

As above, we redefine the fieldâ€™s `resolve` function, calling the original. This time we check if the result is a string and call `.toUpperCase()`:

[`src/directives/UppercaseDirective.js`](https://github.com/GraphQLGuide/guide-api/blob/directives_0.2.0/src/directives/UppercaseDirective.js)

```js
import { SchemaDirectiveVisitor } from 'apollo-server'
import { defaultFieldResolver } from 'graphql'

export default class UppercaseDirective extends SchemaDirectiveVisitor {
  visitFieldDefinition(field) {
    const { resolve = defaultFieldResolver } = field
    field.resolve = async function(...args) {
      const result = await resolve.apply(this, args)
      if (typeof result === 'string') {
        return result.toUpperCase()
      }
      return result
    }
  }
}
```

We include the directive class by adding it to this object, where the key corresponds with the directive name `@upper`:

[`src/directives/index.js`](https://github.com/GraphQLGuide/guide-api/blob/directives_0.2.0/src/directives/index.js)

```js
import TshirtDirective from './TshirtDirective'
import UppercaseDirective from './UppercaseDirective'

export default {
  tshirt: TshirtDirective,
  upper: UppercaseDirective
}
```

![hello query with â€œWORLD ðŸŒðŸŒðŸŒŽâ€ result](img/upper-directive.png)

### @auth

Directives can also take arguments, which can be scalars, enums, or input object types. `@deprecated`, for instance, takes a `reason` argument of type `String`:

```gql
type User {
  firstName
  first_name: String @deprecated(reason: "Use `firstName`.")
}
```

Weâ€™ll be implementing a directive that takes an enum argument:

[`src/schema/schema.graphql`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...directives_0.2.0)

```gql
directive @auth(
  requires: Role = ADMIN,
) on OBJECT | FIELD_DEFINITION

enum Role {
  USER
  MODERATOR
  ADMIN
}
```

Our `@auth` directive is for specifying which objects or fields (`on OBJECT | FIELD_DEFINITION`) require a `Role`. If the `requires` argument isnâ€™t used, then the default `ADMIN` is used.

Our `AuthDirective` class is similar to `UppercaseDirective` in that weâ€™re wrapping the `field.resolve()` function in a new function. However, instead of modifying the result, our wrapping function throws an error if the current userâ€™s role doesnâ€™t match the required role:

[`src/directives/AuthDirective.js`](https://github.com/GraphQLGuide/guide-api/blob/directives_0.2.0/src/directives/AuthDirective.js)

```js
import { SchemaDirectiveVisitor, ForbiddenError } from 'apollo-server'
import { defaultFieldResolver } from 'graphql'

export default class AuthDirective extends SchemaDirectiveVisitor {
  visitFieldDefinition(field) {
    const { resolve = defaultFieldResolver } = field
    field.resolve = (...resolverArgs) => {
      const requiredRole = this.args.requires
      const context = resolverArgs[2]

      if (!context.user.roles.includes(requiredRole)) {
        throw new ForbiddenError(`You don't have permission to view this data.`)
      }

      return resolve.apply(null, resolverArgs)
    }
  }
}
```

The directiveâ€™s arguments are available at `this.args.*`. `resolverArgs[2]`, the third argument passed to resolvers, is always the context where we put the user doc. We assume that the userâ€™s roles are stored in the user doc as an array of strings (like `roles: ['USER']` or `roles: ['USER', 'ADMIN']`).

Since `@auth` works `on OBJECT | FIELD_DEFINITION`, we also need to implement the `visitObject()` method. It needs to go through each field in the object and wrap the `resolve()` function. We also need to mark if a field has been wrapped, so that we donâ€™t double-wrap (if we use `@auth` on both the object and field `foo` in the object, `visitObject()` will wrap all fields, and then `visitFieldDefinition()` will wrap `foo`, which has already been wrapped).

```js
import { SchemaDirectiveVisitor } from 'apollo-server'
import { defaultFieldResolver } from 'graphql'

export default class AuthDirective extends SchemaDirectiveVisitor {
  visitObject(objectType) {
    objectType._requiredRole = this.args.requires

    const fields = objectType.getFields()
    Object.keys(fields).forEach(fieldName => {
      const field = fields[fieldName]
      this._wrapResolveFn(field, objectType)
    })

    objectType._wrappedResolveFn = true
  }

  visitFieldDefinition(field, { objectType }) {
    field._requiredRole = this.args.requires

    const alreadyWrapped = objectType._wrappedResolveFn
    if (!alreadyWrapped) {
      this._wrapResolveFn(field, objectType)
    }
  }

  _wrapResolveFn(field, objectType) {
    const { resolve = defaultFieldResolver } = field

    field.resolve = (...args) => {
      const requiredRole = field._requiredRole || objectType._requiredRole
      const context = args[2]

      if (!context.user.roles.includes(requiredRole)) {
        throw new Error('not authorized')
      }

      return resolve.apply(null, args)
    }
  }
}
```

We save the required role on the field and the object so that inside the wrapper, we can determine which to use (preferencing a role saved on the field over one saved on the object):

```js
const requiredRole = field._requiredRole || objectType._requiredRole
```

We use underscores for data we save (`._requiredRole` and `._wrappedResolveFn`) and for the method we define (`._wrapResolveFn()`) to indicate theyâ€™re private (not meant to be used / called by code outside this class).

Note that `visitFieldDefinition()` receives a second argument with that fieldâ€™s object type. Here are [all the methods](https://github.com/apollographql/graphql-tools/blob/87f32f57f014715d6a311793e3929d39205e2578/src/schemaVisitor.ts#L91-L130) that have second arguments: 

- `visitFieldDefinition(field, { objectType })`
- `visitArgumentDefinition(argument, { field, objectType })`
- `visitEnumValue(value, { enumType })`
- `visitInputFieldDefinition(field, { objectType })`
- `visitSchema(schema, visitorSelector)` (see [explanation of `visitorSelector`](https://github.com/apollographql/graphql-tools/blob/87f32f57f014715d6a311793e3929d39205e2578/src/schemaVisitor.ts#L111-L130))

Finally, letâ€™s add our new directive class to our server:

[`src/directives/index.js`](https://github.com/GraphQLGuide/guide-api/blob/directives_0.2.0/src/directives/index.js)

```js
import TshirtDirective from './TshirtDirective'
import UppercaseDirective from './UppercaseDirective'
import AuthDirective from './AuthDirective'

export default {
  tshirt: TshirtDirective,
  upper: UppercaseDirective,
  auth: AuthDirective
}
```

Now we can test out the directive:

[`src/schema/User.graphql`](https://github.com/GraphQLGuide/guide-api/compare/25_0.2.0...directives_0.2.0)

```gql
type User @auth(requires: USER) {
  id: ID!
  firstName: String!
  lastName: String!
  username: String!
  email: String @auth(requires: ADMIN)
  photo: String!
  createdAt: Date!
  updatedAt: Date!
}
```

Without a `roles` field on our user doc, we get an error and null data:

![user query with error response](img/auth-directive-without-roles.png)

With `"roles": ["USER"]`, we get data and an error:

![user query with firstName and error for email](img/auth-directive-user.png)

With `"roles": ["USER", "ADMIN"]`, we get all the data:

![user query with firstName and email in response](img/auth-directive-admin.png)

## Subscriptions in depth

### Server architecture

Back in the [Deployment options](#options) section, we decided to deploy to a PaaS because our app has subscriptions, which donâ€™t work on FaaS. However, we can split our code into two servers: One that handles subscriptions and WebSockets and runs on a PaaS long-running process, and one that handles queries and mutations over HTTP and runs on a FaaS. This way, our two tasks, which have very different hosting requirements, can be maintained and scaled independently according to their needs.

Letâ€™s recall what our subscription code looks like. When the client sends this operation:

```gql
subscription {
  githubStars
}
```

Our `Subscription.githubStars.subscribe` function is called:

`src/resolvers/Github.js`

```js
import { pubsub } from '../util/pubsub'

export default {
  Subscription: {
    githubStars: {
      subscribe: () => pubsub.asyncIterator('githubStars')
    }
  }
}
```

The server now keeps the WebSocket open and sends over it anything thatâ€™s published to the `githubStars` iterator (`pubsub.publish('githubStars', foo)`).

When our server starts up, we start polling:

`src/index.js`

```js
const start = () => {
  Github.startPolling()
  ...
}
```

`src/data-sources/Github.js`

```js
export default {
  async fetchStarCount() {
    const data = await githubAPI.request(GUIDE_STARS_QUERY).catch(console.log)
    return data && data.repository.stargazers.totalCount
  },

  startPolling() {
    let lastStarCount

    setInterval(async () => {
      const starCount = await this.fetchStarCount()
      const countChanged = starCount && starCount !== lastStarCount

      if (countChanged) {
        pubsub.publish('githubStars', { githubStars: starCount })
        lastStarCount = starCount
      }
    }, 1000)
  }
}
```

When the number of stars changes, the new count is published to the `githubStars` iterator, and the server sends it out to all the clients who have subscribed. 

All the above code can be separated into a new Node server. In fact, since we switched from the default in-memory pubsub to [Redis PubSub](#redis-pubsub), the code that publishes updates doesnâ€™t need to be in the same process that receives subscriptions and handles WebSockets! So if we wanted, we could have three servers:

- Subscription server: A PaaS that supports WebSockets
- Query and mutation server: FaaS
- `githubStars` publishing server: FaaS with scheduled periodic executions

Usually, most of an appâ€™s publishing comes from the mutation server: When a mutation changes data, it publishes the change with the new data. When weâ€™re publishing data from an external source, then we need a function triggered on a schedule to check for changes or the source has to notify us when things change (a [webhook](bg.md#webhook)). When data is changed from places outside our mutation server, we can publish to our subscriptions in three different ways:

- Have those other places (for instance, a legacy application that works with the same business data) publish the changes they make to Redis.
- Have a long-running server poll the database for changes. This can take a significant amount of memory, since the process needs to keep the current state of the data in order to see what has changed. On the other hand, it scales well with high write loads (since changing data doesnâ€™t trigger anything). This is the strategy [Hasura](#hasura) uses.
- Use a special database:
  - [RethinkDB](https://rethinkdb.com/) provides *change feeds* as a way to be notified when the results of a query change (though not all possible queries are supported).
  - MongoDB provides an *oplog*â€”a log of all database operationsâ€”that we can have a server listen to (*tail*). If data changes frequently, it can take a significant amount of CPU to process the oplog, determining which operations are changes that should be published for our subscriptions.

> In the [Meteor](https://www.meteor.com/) framework, you can use a mix of oplog tailing and polling when oplog tailing is too CPU-intensive.

### Subscription design

Our `githubStars` subscription is basicâ€”just a single scalar value.

```gql
type Subscription {
  githubStars: Int
}
```

Usually subscriptions are for getting updates to an object or list of objects. For instance, our `createReview` subscription updates clients on objects being added to the list of reviews.

```gql
type Subscription {
  reviewCreated: Review!
}
```

If we wanted to get all types of updates, we have three options:

1) Adding `reviewUpdated` and `reviewDeleted`:

```gql
type Subscription {
  reviewCreated: Review!
  reviewUpdated: Review!
  reviewDeleted: ID!
}
```

2) A single `reviews` subscription:

```gql
type Subscription {
  reviews: ReviewsPayload
}

union ReviewsPayload = 
  CreateReviewPayload | 
  UpdateReviewPayload | 
  DeleteReviewPayload

type CreateReviewPayload {
  review: Review!
}

type UpdateReviewPayload {
  review: Review!
}

type DeleteReviewPayload {
  reviewId: ID!
}
```

Here we could share the same payloads as the `createReview`, `updateReview`, and `deleteReview` mutations.

3) Calling `reviewCreated` and a `review(id)` subscription for each review loaded on the page:

```gql
type Subscription {
  reviewCreated: Review!
  review(id: ID!): ReviewPayload!
}

union ReviewsPayload = 
  UpdateReviewPayload | 
  DeleteReviewPayload
```

Options #1 and #2 are similar in that the client gets updates to the entire list of reviews. In #2, they have to make fewer subscriptions. In #1, they have more flexibility if for some reason they only wanted to subscribe to `reviewCreated` and not the others. In #3, the client makes many more subscriptions, but doesnâ€™t have to deal with receiving events about reviews they donâ€™t care about. In #1 and #2, unless the user has scrolled enough to load the entire list on the page, theyâ€™re getting events about review objects that arenâ€™t on the page or in the cache, and ignoring them. Given that it takes resources to receive WebSocket messages and check to see if the review is in the cache, we may want to go with #3. In our use case, though, editing and deleting reviews happens infrequently, and even if adding reviews happens frequently, those events are usually all relevant, since the default sort order is most recent. So we might go with the simplicity of #2.

If we had a review detail page that just showed a single review, we would use the `review(id)` subscription. If the page also had a list of comments, then we might do:

```gql
type Subscription {
  reviewCreated: Review!
  review(id: ID!): ReviewPayload!
  commentsForReview(reviewId: ID!): CommentsPayload!
}

union ReviewsPayload = 
  UpdateReviewPayload | 
  DeleteReviewPayload |

union CommentsPayload = 
  CommentCreatedPayload |
  CommentUpdatedPayload |
  CommentDeletedPayload
```

> Of course, if we had (or thought we might have in the future) a different kind of comment elsewhere in our app, we would change all the instances of `Comment*` to `ReviewComment*`.

And if the client was on page `/review/123`, we would subscribe to `review(id: "123")` and `commentsForReview(id: "123")`. As before with the list of reviews, if there might be a lot of comments and comment edit/delete activity, and only some of the comments were shown on the page, we might instead subscribe to updates to each individual comment: `comment(id: "<comment id>")`.

The design of our subscriptions depends on which client views we want realtime updates for, the size of the data set, and the frequency of updates. We take into consideration how much work it takes for the client to make the subscriptions, how much work it takes them to filter out unwanted messages, and also avoiding overfetching data on the messages we do want. For instance, we return just the ID of a deleted object instead of the whole object. And if we had a granular `changeReviewStars` mutation, we could union and resolve to a `ChangeReviewStarsPayload` type. The client could then only select the `stars` field instead of the whole review:

```gql
fragment ChangeReviewStars on ChangeReviewStarsPayload {
  review {
    id
    stars
  }
}

fragment CreateReview on CreateReviewPayload {
  review {
    id
    text
    stars
    createdAt
  }
}

fragment DeleteReview on DeleteReviewPayload {
  reviewId
}

subscribe {
  reviews {
    ...ChangeReviewStars
    ...CreateReview
    ...DeleteReview
  }
}
```

## Security

Background: [HTTP](bg.md#http), [Databases](bg.md#databases), [Authentication](bg.md#authentication)

* [Auth options](11.md#auth-options)
  * [Authentication](11.md#authentication)
  * [Authorization](11.md#authorization)
* [Denial of service](11.md#denial-of-service)

In this section, weâ€™ll start out with an overview of general server-side security and then get to a few topics specific to  GraphQL. 

Computer security is protecting against:

- Unauthorized actions
- Theft or damage of data
- Disruption of service

Here are a few levels of vulnerability relevant to securing servers from the above threats, along with some methods of risk management:

- **People and their devices**: People that have access to our systems, like employees at our company, hosting companies, and service companies like Auth0.
  - Train employees on security, including avoiding the most common malware avenues: visiting websites and opening files.
  - Avoid personal use of work devices.
  - Install [antivirus](https://thewirecutter.com/blog/best-antivirus/) on work computers.
  - [Vet](https://checkr.com/) employee candidates.
  - Access production systems and data from a limited number of devices that are not used for email or web browsing.
- **Physical access**: The capability to physically get to servers that store or handle our data.
  - Make sure device hard drives are encrypted with complex login passwords, or locked away when not in use.
  - Assess risk level of our service companies (for example [AWS perimeter security](https://aws.amazon.com/compliance/data-center/perimeter-layer)).
- **Network**: Users being able to access our server over the internet or view data in transit.
  - Keep our server IP addresses private.
  - Use a DNS provider that hides our server IPs and handles DDoS attacks (like [Cloudflare](https://www.cloudflare.com/) or AWSâ€™s [Sheild Standard, CloudFront, & Route 53](https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/)).
  - Force HTTPS: When a client makes a connection to our server on port 80 (unencrypted), redirect them to port 443, which will ensure all further data sent between us and the client is encrypted.
- **Operating system**: Hackers exploiting a vulnerability in our server OS (usually Linux).
  - Apply security patches or use a PaaS or FaaS, where OS security is taken care of for us.
- **Server platform**: Node.js.
  - Apply security updates to Node.js, or use a PaaS or FaaS, where security updates are done automatically.
- **Application layer**: GraphQL execution and our code. The following sections cover this area of security.

After we implement protections, we can hire a firm to do a [security audit](https://en.wikipedia.org/wiki/Information_security_audit) and use [HackerOne](https://www.hackerone.com/) to find areas we didnâ€™t sufficiently cover. 

Any system can be hackedâ€”itâ€™s just a matter of the level of resources put into hacking. The two largest sources relevant to companies are eCrime (criminal hackingâ€”often financial or identity theft) and the Chinese government (stealing trade secrets from foreign companies). Most large companies have been hacked at some point to some degree.

After we have been hacked, itâ€™s important to be able to:

1. Figure out how it happened.
2. Ensure the attackers no longer have access.
3. Know what data was accessed.
4. Recover deleted data.

For #1 and #3, we can set up access logs for our production servers, databases, and sensitive services, and for #4, we can set up automatic database backups (MongoDB Atlas has options for either snapshots or continuous backups). Step #2 depends on #1â€”if one of our service accounts was compromised, we can change the password. If one of our API userâ€™s accounts was stolen (session token, JWT, or password), then we need to delete their session or re-deploy with code that blocks their JWT (and if weâ€™re using password authentication, delete their current password hash and send a password reset email).

One important way to mitigate the damage of a database hack is hiding sensitive database fieldsâ€”either by storing only hashes, in the case of passwords, or by storing fields encrypted (using an encryption key thatâ€™s not stored in the database). Then an attacker wonâ€™t know the userâ€™s password (which theyâ€™d likely be able to use to log in to the userâ€™s accounts on other sites), and they wonâ€™t be able to read sensitive data unless they also gain access to the encryption key.

Here are a few application-layer security risks that apply to API servers in generalâ€”not just GraphQL servers:

- Parameter manipulation: When clients alter operation arguments. We protect against this by checking arguments to ensure theyâ€™re valid, and by not trusting them (for instance, we should use the `userId` from the context instead of from an argument).
- Outdated libraries: Our code depends on a lot of libraries, any of which may have security vulnerabilities that affect our app. For Node.js, we can use `npm audit` to check for vulnerabilities in our libraries.
- Database injection like [SQL injection](https://en.wikipedia.org/wiki/SQL_injection) and [MongoDB injection](https://blog.websecurify.com/2014/08/hacking-nodejs-and-mongodb.html)
- [XSS](https://developer.mozilla.org/en-US/docs/Glossary/Cross-site_scripting): On the client, preventing XSS involves sanitizing user-provided data before itâ€™s added to the DOM, but on the server, we use a [Content-Security-Policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy) header.
- [Clickjacking](https://en.wikipedia.org/wiki/Clickjacking): Use [X-Frame-Options headers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/
- Race conditions, especially [TOCTOU](https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use): Imagine multiple of our servers are running the same mutation from the same user at the same time. We may need to use database transactions or other logic to prevent this type of attack.
- Number processing: Bugs that involve working with numbers, including conversion, rounding, and overflows.

### Auth options

*Auth* is an imprecise termâ€”sometimes itâ€™s used to mean authentication, sometimes authorization, and sometimes both. In this case, we mean both:

- [Authentication](11.md#authentication)
- [Authorization](11.md#authorization)

#### Authentication

Background: [Authentication](bg.md#authentication)

The server receives a JWT or session ID in an HTTP header, which it uses to decode or look up the user. If weâ€™re putting our GraphQL server in front of existing REST APIs, then we may want to just pass the header along to the REST APIsâ€”they can continue doing the authentication (and authorization), returning null or returning errors that we can format as GraphQL errors. 

However, usually weâ€™ll handle user decoding in the GraphQL server. In the case of federation, we decoded the user [in the gateway](11.md#federated-gateway) and passed the object in a `user` header to the services. In the case of our monolith, we decoded [in the `context` function](11.md#authenticating) and provided `context.user` to the resolvers.

But how does the client get the JWT or session ID in the first place? In our case, we used an external service: We [opened a popup](6.md#authentication) to an Auth0 site that did both signup and login and provided the client with a JWT. Other options include:

- Hosting our own identity server (for example the free, open-source [Ory server](https://www.ory.sh/kratos/)). 
- Adding HTTP endpoints to our GraphQL server (for example with the [Passport library](http://www.passportjs.org/)).
- Adding mutations to our GraphQL server (for example the [accounts-js](https://github.com/accounts-js/accounts) library adds `Mutation.register`, `Mutation.authenticate`, etc. to our schema).
- Using our hosting providerâ€™s identity service (for example [Netlify Identity](https://docs.netlify.com/functions/functions-and-identity/#access-identity-info-via-clientcontext) if our server is hosted with [Netlify Functions](https://www.apollographql.com/docs/apollo-server/deployment/netlify/), or [Amazon Cognito](https://aws.amazon.com/cognito/) with AWS Lambda).

Hosting our own separate identity server might be the most common solution.

#### Authorization

After we authenticate the client, we either have their decoded token object (in the case of JWTs) or their user object (in the case of sessions). Both the token and the user object should have the userâ€™s permissions. Permissions can be stored in different waysâ€”usually a list of roles or scopes, or, at its most simple, as an `admin` boolean field.

Once we have the userâ€™s permission info, our server has to determine which data to allow the user to query and which mutations to allow the user to call. There are a number of different places where we can make this determination:

- **REST services**: In the case of putting a GraphQL gateway in front of existing REST services that already do authorization checks, we can continue to let them do the checks.
- **Context**: If we only want logged-in users to be able to use our API, we can throw an `AuthenticationError` in our `context()` function whenever the HTTP header is missing or the decoding/session lookup fails.
- **Model**: We can do the checks in our data-fetching code. This is the best option when we have both a GraphQL and REST API, both of which call the model code. (This way, we donâ€™t have to duplicate authorization checks.)
- **Directives**: We can add directives to fields or types in our schemaâ€”for instance, `@isAuthenticated` or `@hasRoles(roles: [ADMIN])`. A library we can use that defines these directives for us is [graphql-auth-directives](https://github.com/grand-stack/graphql-auth-directives).
- **Resolvers**: In the server we built in this chapter, we did all our authorization checks in our resolver functions. The biggest downside to this approach is repetition as the schema gets largerâ€”for instance, weâ€™d probably wind up with a lot of `if (!user) { throw new ForbiddenError('must be logged in') }`. Itâ€™s also harder to get a broader sense of which parts of the schema have which authorization rules. With directives, we can easily scan through the schema, and with middleware, we can look at the below `shield({ ... })` configuration and see everything together.
- **Middleware**: We can use [`graphql-middleware`](https://github.com/prisma-labs/graphql-middleware)â€”functions that are called before our resolvers are called. In particular, we can configure the [GraphQL Shield](https://github.com/maticzav/graphql-shield) middleware library to run authorization functions before our resolvers like this:

```js
const isAuthenticated = rule({ cache: 'contextual' })(
  async (parent, args, context, info) => {
    return context.user !== null
  }
)

const isAdmin = rule({ cache: 'contextual' })(
  async (_, __, context) => {
    return context.user.roles.includes('admin')
  }
)

const isMe = rule({ cache: 'strict' })(
  async (parent, _, context) => {
    return parent._id.equals(context.user._id)
  }
)

const permissions = shield({
  Query: {
    me: isAuthenticated,
    secrets: isAdmin
  },
  Mutation: {
    createReview: isAuthenticated
  },
  User: {
    email: chain(isAuthenticated, isMe)
  },
  Secret: isAdmin
})
```

The equivalent **directives** schema would be:

```gql
type Query {
  user(id: ID!): User
  me: User @isAuthenticated  
}

type Mutation {
  createReview(review: CreateReviewInput!): Review @isAuthenticated
}

type Secret @hasRole(roles: [ADMIN]) {
  key: String
}
```

And for `User.email`, we could either do a resolver check or create a new directive.

In each of the last three authorization locationsâ€”**directives, resolvers, and middleware**â€”we have to be careful about adding rules only to our root query fields. Since our data graph is interconnected, oftentimes there will be other ways to reach a sensitive type through a connection from another field. So itâ€™s usually necessary to add rules to types, as we do with the `Secret` type above. Unfortunately, we canâ€™t do that in resolversâ€”just directives and middleware.

### Denial of service

Denial of service is a type of attack in which the attacker overloads our serversâ€™ capacity to process requests, resulting in legitimate users being unable to use our app. While some attacks are below the application layer (like on TCP or HTTP), those are usually taken care of by our DNS and/or hosting provider (at least in the case of PaaS and FaaS). In this section, weâ€™ll look at application layer attacks, which can be separated into two buckets: expensive requests and a large number of requests. We want to guard against both. 

First, guarding against expensive requestsâ€”requests that take up significant resources while the server processes them:

- [Safelisting](https://www.apollographql.com/docs/graph-manager/operation-registry/): If our API is privateâ€”only for use by our own client codeâ€”then we can safelist our queries. Weâ€™ll send Apollo Graph Manager our client queries during a build step in the client repo(s), and then our server will check all incoming requests against the registered queries in Graph Manager and reject any unrecognized queries. If our API is public, however, we canâ€™t safelist, because we want third-party devs to be able to construct whatever queries they need.
- Validate arguments: Attackers can alter arguments to take up resources. For instance, if we have a `username` argument in our `signup` mutation, and then we save it to the database without checking the length, an attacker could provide a long string that takes up a gigabyte of hard drive space. Soon, our database would become full, which would prevent us from storing any further data.
- Add a timeout: If a request isnâ€™t done after N milliseconds, terminate it.
- Limit depth: One way to make a query expensive is to make it really deepâ€”continuing to select connection fields (like `query { posts { comments { users { posts { comments { ...etc. }}}}}}`). We can use the [`graphql-depth-limit`](https://github.com/stems/graphql-depth-limit) library for this.
- Limit complexity: This is a more advanced technique than just limiting depth and involves assigning a complexity cost value to each field and limiting the total cost of a query. We can implement this using [`graphql-validation-complexity`](https://github.com/4Catalyzer/graphql-validation-complexity), or, if we want more flexibility, [`graphql-cost-analysis`](https://github.com/pa-bru/graphql-cost-analysis), which allows us to multiply costs by arguments or parent multipliers.

We can guard against a large number of requests by rate limiting. GitHub uses a combination of [rate limiting and cost analysis](https://developer.github.com/v4/guides/resource-limitations/#rate-limit) for its public APIâ€”we canâ€™t make queries with a total cost of more than 5,000 points per hour. Thereâ€™s not yet an open-source library that does this. (If you write one, let us know so that we can link to it! And you may want to use a [leaky bucket algorithm](https://en.wikipedia.org/wiki/Leaky_bucket) instead of a fixed window.) The [`graphql-rate-limit-directive`](https://github.com/ravangen/graphql-rate-limit) library provides a directive that allows us to limit the number of times a particular field or object is selected within a certain time window.
- Hide schema: A common practice for private GraphQL APIs is disabling introspection in production. This is the default behavior of Apollo Server. While it doesnâ€™t guard against expensive operations, it makes it harder for an attacker to construct them, since they canâ€™t just open Playground and read through the schema.

In addition to blocking requests that are too complex or too frequent, we can reduce the amount of resources each request takes. For instance, instead of doing all the work needed during the request, in some cases we can send a response and then queue a job to be executed by a different server, clearing more room for our API server to handle more requests. Another example is cachingâ€”we can reduce the load on our database by using a cache, which weâ€™ll get to in the next section, [Performance > Caching](#caching).

Many of these techniques are implemented for us automatically when we use a backend-as-a-service like [Hasura](#hasura).

## Performance

Background: [HTTP](bg.md#http), [Latency](bg.md#latency), [Databases](bg.md#databases), [CDN](bg.md#cdn)

Performance is mostly about speedâ€”how quickly can the client receive a response. Itâ€™s also about *load* (how much work a server is doing) since high load (caused by many concurrent requests) can result in either slower responses or no responses ðŸ˜…. *Capacity* is defined as either the load a server can handle before it fails to respond or before its response speed decreases.

There are many places in the request-response cycle where we can improve speed or increase capacity. They all have different costs (in terms of development time, maintenance, and money) and different levels of improvement. An essential aspect of performance engineering is measurement. We need to know how long things take or how much load we can handle before we:

1. Decide we want to improve (performance / scalability is a common area of premature optimization).
2. Make improvements (so we can compare measurements before and after to determine how effective the change is).

We can determine our capacity with *load testing*, using [k6](https://k6.io/) with [`easygraphql-load-tester`](https://easygraphql.com/docs/easygraphql-load-tester/usage) to make many simultaneous requests. We can measure server-side performance with Graph Manager like we did in the [Analytics](#analytics) section: request rate and response time, as well as resolver timelines. Resolvers usually spend most of their time making database queries (which weâ€™ll examine in the next section, [Data fetching](#data-fetching)), but if we wanted to look at exactly how long each one takes, we could do that as well (how we do that depends on which database weâ€™re using). 

We also want to measure the response time from the client in order to spot:

- Longer times due to latency or limited bandwidth.
- Shorter times due to CDN or browser caching.

[Caching](#caching) has its own section, but here are a couple of other ways to improve speed measured from the client:

- Use an HTTP/2 server (like Node.js 10+).
- Use automatic persisted queries (APQ).

**HTTP/2**: Browsers limit the number of HTTP/1.1 connections to a single server, so if more than a certain number of requests (usually six) are made, the ones beyond six wait until the first six are completed. This drastically increases the time it takes the ones beyond six to complete. We can fix this by using HTTP/2, which can make multiple requests over a single connection.

**APQ**: When the clientâ€™s requests include large queries and theyâ€™re on a low-bandwidth connection, it can take a long time to send the request. Automatic persisted queries allow the client to send a hash of the query instead of the whole thing. Itâ€™s enabled by default in Apollo Server and [with a link](https://www.apollographql.com/docs/apollo-server/performance/apq/#setup) on the client. The client creates a hash (a relatively small string) of the query and sends that to the server. The first time the server receives a hash, it doesnâ€™t recognize it and returns an error. Then the client replies with the full query and the hash, which the server saves. After that, whenever any client sends that hash, the server will recognize it and know which query to execute.

> Itâ€™s also possible to persist database queries (called *prepared statements* in SQL), in which the query is stored in the database and the API server just sends the query ID and arguments. This is done for us automatically when using [Hasura](#hasura).

Before a requestâ€™s processing reaches our resolvers, the GraphQL server library has to parse and validate the request. Then, during the execution phase, the library calls our resolvers. Different GraphQL servers do this process faster than others. For Node.js, the main improvement available is compiling queries to code, which [`graphql-jit`](https://github.com/zalando-incubator/graphql-jit) does. It integrates with Apollo Server [like this](https://github.com/zalando-incubator/graphql-jit/blob/master/examples/blog-apollo-server/src/server.ts). Another option for Python, Ruby, and Node is [Quiver](https://graphql-quiver.com/).

### Data fetching

The largest server-side factor that contributes to the response time is how long resolvers take to return, and the majority of resolversâ€™ runtime is usually taken up by fetching data. In this section, weâ€™ll cover the performance of data fetching in our resolvers.

> Some of this section will apply to subscriptions. We also discussed scaling subscription servers in [Subscriptions in depth > Server architecture](#server-architecture).

The three general speed factors, in order of importance:

1. How many data requests are made in series
2. How long the data source takes to get the data
3. Latency between our GraphQL server and the data source

> Here *data source* means a source of data, like a database or an APIâ€”not an Apollo Server data source class.

Usually, we locate both our GraphQL server and our data sources in the same location, in which case #3 is very small (~0.2ms when inside the same AWS Availability Zone). However, when theyâ€™re far apartâ€”for instance when the data source is an external API hosted across the countryâ€”#3 can become a larger factor than #2. 

Factor #2 depends on the type of data source and what data is being requested. For databases, usually the largest factor is if there is an index that covers the queryâ€”otherwise, the database has to search through all records in the table/collection, which takes much more time. Another large factor is whether data has to be read from diskâ€”itâ€™s faster when the data is already in RAM. (MongoDB [recommends](https://docs.atlas.mongodb.com/sizing-tier-selection/#memory) having enough RAM to fit the *working set*â€”the indexes and data that are accessed frequently.) 

Since different types of databases work differently, we may get faster results by using another database, in which case we might move or duplicate part or all of our data to the other database. For instance, Elasticsearch handles search queries more efficiently than our main database. We would duplicate all the data we wanted searchable from our main database to Elasticsearch, and then we would resolve all searches by querying Elasticsearch. Another type of query that is slow in many databases is one that skips a large number of results. This issue, which we talk about in the [Pagination section](#pagination), is one reason to use [cursors](#relay-cursor-connections).

Another factor that can improve database speed and reduce load is avoiding overfetchingâ€”instead of fetching all the fields (for instance `SELECT * FROM reviews`), we can fetch only the ones needed for the current queryâ€™s selection set. If we use a library like [Join Monster](#sql-performance) or a platform like [Hasura](#hasura), this is done for us, as well as JOINs. Otherwise we can look at [`info`](https://www.apollographql.com/docs/graphql-tools/resolvers/#resolver-function-signature), the fourth resolver argument, to look up which fields to select.

A large area in which we can reduce load on a data source is sending fewer queries! One issue of basic implementations of GraphQL resolvers and ORMs is the *N+1 problem*. Consider this query:

```gql
query {
  post(id: "abc") {
    comments {
      id
      text
    }
  }
}
```

The N+1 problem is when our server does 1 query for the post document and then N comment queriesâ€”one for each ID in the `post.commentIds` array. There are actually two issues with this:

- The comment queries are done in parallel, but the post and the group of comment queries are done in seriesâ€”the post is fetched before the comments. This is a significant hit to our GraphQL serverâ€™s response time.
- When there are a lot of comments, there are a lot of comment queries, which is a high load on the server.

The second issue is fixed by DataLoader, which batches all the comment queries into a single query. To learn how to use DataLoader, see the [Custom data source](#custom-data-source) section. Also, if our data source is existing REST APIs, we can generate DataLoader code with Yelpâ€™s [`dataloader-codegen`](https://github.com/Yelp/dataloader-codegen) library.

To fix the first issue, we need the `post` resolver to fetch both the post and the comments at the same time. If we use Join Monster or Hasura, this is done for us. If we use MongoDB, we have two options:

- Use a de-normalized structure in the posts collection, storing an array of comment objects inside each post documentâ€”then fetching the post will get the comments as well.
- Use the `info` resolver arg:
  - Store each comment with a `postId` field.
  - Look at `info` to see if `comments` is selected.
  - If it is selected, query for both the post and the comments at the same time.
  
```js
const resolvers = {
  Query: {
    post: async (_, { id }, { dataSources }, info) => {
      const postPromise = dataSources.posts.findOneById(id)

      if (commentsIsSelected(info)) {
        const [post, comments] = await Promise.all([
          postPromise,
          dataSources.comments.findAllByPostId(id)
        ])
        post.comments = comments
        return post
      } else {
        return postPromise
      }
    }
  }
}
```

We can use this `info` technique with other databases as well as beyond the N+1 problemâ€”there may be other queries we can initiate early. Viewing data in the `info` object can be simplified with the [`graphql-parse-resolve-info`](https://github.com/graphile/graphile-engine/tree/master/packages/graphql-parse-resolve-info) library.

### Caching

Wikipediaâ€™s [definition](https://en.wikipedia.org/wiki/Cache_(computing)) of a cache is â€œa hardware or software component that stores data so that future requests for that data can be served faster.â€ In addition to improving speed, caching also reduces load on the part of the system that originally provided the data thatâ€™s being cached. For instance, a CDN caching an HTTP response reduces load on our server, which originally provided the response. And our `MongoDataSource` caching documents reduces load on our MongoDB database.

Here are the possible places for caches, starting in the client code thatâ€™s requesting data, and ending with the database:

- **Client library**: GraphQL client libraries like Apollo Client [cache](5.md#caching) response data from previous requests in memory.
- **Browser / Client OS**: Browsers, iOS, and Android cache HTTP responses based on the `Cache-Control` HTTP header.
- **CDN**: CDNs also cache HTTP responses based on `Cache-Control` (see [Background > CDN](bg.md#cdn)).
- **Application server**: 
  - Our GraphQL server can cache GraphQL responses in a caching database like Redis.
  - Our serverâ€™s data source classes can [cache database responses](#redis-caching) in Redis.
- **Database**: Our database has various levels of cachingâ€”in its software that uses RAM, in the operating system, and in the hard drives.
  
> Itâ€™s caches all the way down.
> â€”Yoav Weiss

Apollo Server will set the `Cache-Control` header for us as well as save the response to the cache. By default, however, it assumes we donâ€™t want data cached and doesnâ€™t do so. We have to tell it which fields and types we want cached and for how long. Then, if a response includes only those fields, it will set the header and save the response in the cache.

We can tell Apollo Server which fields and types we want cached with a *cache hint*. We can provide the hint in two ways:

- The `@cacheControl` schema directive
- Calling [`info.cacheControl.setCacheHint()`](https://www.apollographql.com/docs/apollo-server/performance/caching/#adding-cache-hints-dynamically-in-your-resolvers) in our resolvers

The first method we can use on both types and fields:

```gql
type Query {
  hello: String!
  reviews: [Review!]! @cacheControl(maxAge: 120)
  user(id: ID!): User
}

type Review @cacheControl(maxAge: 60) { 
  id: ID!
  text: String!
  stars: Int
  commentCount: Int! @cacheControl(maxAge: 30)
}

type User @cacheControl(maxAge: 600) {
  id: ID!
  firstName: String!
  reviews: [Review!]!
}
```

`maxAge` is in seconds. The lowest `maxAge` is used. For instance, `Review.commentCount` has a `maxAge` of 30, so the response to the below query would be cached for 30 seconds:

```gql
query {
  user(id: "1") {
    reviews {
      text
      stars
      commentCount
    }
  }
}
```

Whereas this would be cached for 60:

```gql
query {
  user(id: "1") {
    reviews {
      text
      stars
    }
  }
}
```

Similarly, if we didnâ€™t select `User.reviews`, the hint on `User` would be used, and the below query would be cached for 10 minutes:

```gql
query {
  user(id: "1") {
    firstName
  }
}
```

Field cache hints override type hints, so for the below query, `Query.reviews`â€™s `maxAge: 120` would be used instead of `Review`â€™s `maxAge: 60`:

```gql
query {
  reviews {
    text
    stars
  }
}
```

Finally, neither of the below queries would be cached, as `Query.hello` doesnâ€™t have a hint:

```gql
query {
  hello
}
```

```gql
query {
  reviews {
    text
    stars
  }
  hello
}
```

Thereâ€™s one more directive argument: `scope`. Itâ€™s `PUBLIC` by default, and the other value is `PRIVATE`:

```gql
type Query {
  me: User! @cacheControl(maxAge: 300, scope: PRIVATE)
}
```

Apollo would set the response header to `Cache-Control: max-age=300, private`. Including [`private`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control#Directives) means that the response should only be stored in a browserâ€™s cache, not a CDN. Because if a CDN stored `Query.me` (the current userâ€™s account), other clients who made the query would get access to the first userâ€™s account data.

Some advanced CDNs like [Cloudflare](https://blog.cloudflare.com/token-authentication-for-cached-private-content-and-apis/) actually support caching private responses by matching them to a single user with an authentication token. Similarly, Apollo Server supports caching responses through a function that returns a session ID or any unique string associated with a userâ€”in the below code, we use the JWT:

```js
import responseCachePlugin from 'apollo-server-plugin-response-cache';

const server = new ApolloServer({
  ...,
  plugins: [
    responseCachePlugin({
      sessionId: requestContext => 
        requestContext.request.http.headers.get('authorization') || null,
    })
  ]
})
```

If Apollo caches a response with scope `PRIVATE`, it will also save the session ID. If the same request arrives later, and the same session ID is returned from this function, Apollo will use the cached response.

One issue with CDN caching is that many CDNs only cache GET requests, and GraphQL requests are usually made via POSTs. Apollo Server supports GET requests, and clients can switch to using them, but GET requests have the query in the URL, and sometimes queries are too long to fit in a URL. However, we can use automatic persisted queries (discussed [earlier](#performance)), which results in clients using cacheable GET requests with short URLs, regardless of the query length.

## Future

The largest change to GraphQL-land in the coming years will be its size! The [S curve](https://en.wikipedia.org/wiki/Sigmoid_function) of GraphQL adoption is currently in the exponential phase (*seemingly* exponentialâ€”technically, itâ€™s logistic). Hereâ€™s a graph of the [`graphql`](https://www.npmjs.com/package/graphql) packageâ€™s weekly npm downloads over the first 5 years:

![Downloads increasing exponentially over 5 years, ending in 3.5M](img/npm-graphql-downloads.png)

And it doesnâ€™t even include client packages like `apollo-client`, which are also growing. At the time of writing, [`apollo-client`](https://www.npmjs.com/package/apollo-client) has 1.7M weekly downloads. Thereâ€™s also a lot of room left to growâ€”according to Google Trends, REST still has 3x the interest that GraphQL has:

![Line chart with GraphQL increasing and ending at 21:61 GraphQL:REST](img/google-trends.png)

As adoption grows, more resources will be put into GraphQL libraries, tools, and services. The existing ones will improve, and new ones will be created. 

- Apollo Serverâ€™s [roadmap](https://github.com/apollographql/apollo-server/blob/master/ROADMAP.md#future-work) lists near-term future work, including: 
  - Adding subscription support to Apollo Federation.
  - Adding `@defer` and `@stream` directives.
  - Invalidation of whole-query cache through cache tags with CDN integration.
  - Building a â€œgraphâ€ caching layer for the gateway.
- Apollo Client also has a [roadmap](https://github.com/apollographql/apollo-client/blob/master/ROADMAP.md) as well.
- For some futuristic-seeming services and tooling, check out [this video](https://www.youtube.com/watch?v=JilN_PvQOqs) from the creator of [OneGraph](https://www.onegraph.com/docs/) (a GraphQL API that combines many different companiesâ€™ APIs).
- An exciting area in which weâ€™re looking forward to growth is full-stack GraphQL frameworksâ€”the Ruby on Rails of GraphQL, Node, and React. Our current favorites are [RedwoodJS](https://redwoodjs.com/) (a new project based on serverless and Prisma) and [Vulcan.js](http://vulcanjs.org/) (a mature project based on Meteor and MongoDB).

There will also be changes to the language itself. In 2018, Facebook transferred the GraphQL project (which includes the spec, the [`graphql-js`](https://github.com/graphql/graphql-js) reference implementation, GraphiQL, and DataLoader) to a new Linux Foundation called the [GraphQL Foundation](https://foundation.graphql.org/). Anyone can discuss or propose changes to the specification in its GitHub repo, [graphql/graphql-spec](https://github.com/graphql/graphql-spec), or in the [GraphQL Working Group](https://github.com/graphql/graphql-wg), a monthly virtual meeting of maintainers.

Changes to the spec go through an [RFC process](https://github.com/graphql/graphql-spec/blob/master/CONTRIBUTING.md), and the current proposals are [listed here](https://github.com/graphql/graphql-spec/tree/master/rfcs). A few of them are:

- The [`@defer` and `@stream`](https://github.com/graphql/graphql-spec/blob/master/rfcs/DeferStream.md) query directives we mentioned on the Apollo Server roadmap. Adding `@defer` to a field tells the server they can initially return `null` and later fill in the data. Adding the `@stream` directive to a field with a list type means the server can send part of the list initially, and further parts of the list later. These directives address the fact that currently the server only sends a single response, which means it has to wait for all data to arrive from its data sources. And that means the response time is limited by the slowest source. With `@defer` and `@stream`, the client can get some of the data sooner.
- The [`@live`](https://github.com/graphql/graphql-spec/blob/master/rfcs/Subscriptions.md) query directive, which means: â€œsend me the current value of this field, and then send me the updated value whenever it changes.â€
- The Input Unionâ€”creating a union type that can be used for arguments. The [proposal](https://github.com/graphql/graphql-spec/blob/master/rfcs/InputUnion.md) (a.k.a. RFC) is a long document that starts with:

> RFC: GraphQL Input Union
>
> The addition of an Input Union type has been discussed in the GraphQL community for many years now. The value of this feature has largely been agreed upon, but the implementation has not.
> 
> This document attempts to bring together all the various solutions and perspectives that have been discussed with the goal of reaching a shared understanding of the problem space.
> 
> From that shared understanding, the GraphQL Working Group aims to reach a consensus on how to address the proposal.

There are also specifications in GraphQL-land other than the GraphQL spec, including the [Relay Cursor Connections](https://relay.dev/graphql/connections.htm) spec, the [Relay server](https://relay.dev/docs/en/graphql-server-specification.html) spec, and the in-development [GraphQL over HTTP](https://github.com/APIs-guru/graphql-over-http) spec.

---

You can contribute to the future of GraphQL by:

- Building things with it!
- Contributing to GraphQL libraries and tools.
- Getting involved with the spec and foundation.
- Spreading the word. 

Speaking of spreading the word, if youâ€™d like to recommend the Guide to a friend or co-worker, weâ€™d appreciate it ðŸ™ðŸ¤—. [`https://graphql.guide`](https://graphql.guide). And weâ€™d value any feedback you may have on the book via [GitHub issues](https://github.com/GraphQLGuide/book/issues) or PRs.

To learn more about GraphQL, we recommend:

- Books: 
  - [Production Ready GraphQL](https://book.productionreadygraphql.com/): An in-depth discussion of production topics.
  - [Advanced GraphQL with Apollo & React](https://8bit.press/book/advanced-graphql): A large tutorial-style book based on Apollo Federation and React.
- Course: [Fullstack Advanced React & GraphQL](https://advancedreact.com/)
- Reading [the spec](https://spec.graphql.org/draft/)